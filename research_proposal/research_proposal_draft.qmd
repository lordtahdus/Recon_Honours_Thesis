---
title: "Enhancing Forecast Reconciliation: A Study of Alternative Covariance Estimators"
format: 
  pdf:
    include-in-header:
      text: |
        <!-- \usepackage{amsmath} -->
        <!-- \usepackage{mathspec} -->
        <!-- \usepackage{algorithm} -->
        <!-- \usepackage{algpseudocode} -->
        \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
    mathspec: true
    number-sections: true
execute:
  cache: true
bibliography: D:/Github/Recon_Honours_Thesis/references.bib
# bibliographystyle: apa
csl: D:/Github/Recon_Honours_Thesis/apa.csl
---

# Objectives

Globally, Starbucks manages thousands of outlets across dozens of countries, each projecting sales for dozens of beverages. Yet when regional, national, and brand-wide forecasts are generated, discrepancies of millions of dollars emerge simply because individual outlet forecasts fail to respect the hierarchical nature of the data. Forecast reconciliation comes in to solve this huge problem, where the individual forecasts are adjusted to satisfy the aggregation constraints. Among the various reconciliation methods, **MinT** (Minimum Trace) is considered the optimal approach, however, it requires a good estimate of the covariance matrix of the base forecast errors. The current practice is to use the shrinkage estimator (often shrinking toward a diagonal matrix), but it lacks flexibility and might neglect the prominent structure presented. In this project, we aim to assess the forecasting performance of MinT when different covariance estimators are used, namely NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance), POET (Principal Orthogonal complEment Thresholding), and others.

# Background

In time series forecasting, aggregation occurs in a variety of settings. A concrete example of a hierarchy would be electricity demand forecasting, where the national demand is the sum of the demands for each state, and demand for each state comes from many regions within the states. Forecasting national tourism or Gross Domestic Product (GDP) is another example of hierarchical time series. The impact of methods for forecasting hierarchical time series has not been limited to academia, with industry also showing a strong interest. Many companies have adopted these methods in practice, including Amazon, the International Monetary Fund, IBM, SAP, and more [@Athanasopoulos2024-as].

The hierarchical structure can be represented as a tree, as shown in @fig-hierarchy. The top level of the tree represents the total value of all series, while the lower levels represent the series at different levels of disaggregation. When there are attributes of interest that are crossed, such as the electricity demand at any aggregation level (national, state, or regional) is also considered by usage purposes (e.g., residential, commercial), the structure is described as a grouped time series (illustrated in @fig-grouped).

![Diagram of 2-level hierarchical tree structure](D:/Github/Recon_Honours_Thesis/research_proposal/figs/hierarchical_structure.png){#fig-hierarchy width="40%" height="40%"}

![Diagram of 2-level grouped structure, which can be considered as the union of two hierarchical trees with common top and bottom level series](D:/Github/Recon_Honours_Thesis/research_proposal/figs/grouped_structure.png){#fig-grouped width="90%" height="40%"}


For simplicity, we refer to both of these structures as hierarchical time series, we will distinguish between them if and when it is necessary. All hierarchical structures can be represented using matrix algebra:

$$
\boldsymbol{y}_t = \boldsymbol{S} \boldsymbol{b}_t,
$$

where $\boldsymbol{S}$ is a summing matrix of order $n \times n_b$ which aggregates the bottom-level series $\boldsymbol{b}_t$ ($n_b$-vector) to the series at aggregation levels above. The $n$-vector $\boldsymbol{y}_t$ contains all observations at time $t$.

The summing matrix $\boldsymbol{S}$ for the tree structure in @fig-hierarchy is:

$$
\boldsymbol{S} = 
\left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\boldsymbol{I_n}}
\end{array}
\right].
$$

# Methodology

\color{red}

Assume we produce h-step-ahead base forecasts $\hat{\boldsymbol{b}}_h$ for the bottom-level series, obtained by any prediction methods. Then pre-multiplying them by $\boldsymbol{S}$ we get: 

$$
\tilde{\boldsymbol{y}}_h = \boldsymbol{S} \hat{\boldsymbol{b}}_h .
$$ {#eq-sb}

We refer to $\tilde{\boldsymbol{y}}_h$ as coherent forecasts, as they respect the aggregation structure. We also refer to this way of obtaining coherent forecasts by summing the bottom-level forecasts as the bottom-up approach. However, generating forecasts this way is anchored only to prediction models at a single level, and will not be utilising the information from other levels. This drawback applies to the top-down and middle-out approaches. For example, the bottom-level data can be very noisy or even intermittent, and the higher-level data might be smoother due to the aggregation. 

Another issue with expressing reconciled methods as in @eq-sb is that it restricts the reconciliation to only single-level approaches. Thus, @Hyndman2011-jv suggested a generalised expression for all existing methods, which also provides a framework for new methods to be developed:

\color{black}

$$
\tilde{\boldsymbol{y}}_h = \boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_h,
$$ {#eq-sgy}

for a suitable $n_b \times n$ matrix $\boldsymbol{G}$. $\boldsymbol{G}$ maps the base forecasts of all levels $\hat{\boldsymbol{y}}_h$ down into the bottom level, which is then aggregated to the higher levels by $\boldsymbol{S}$.

The choice of $\boldsymbol{G}$ determines the composition of reconciled forecasts $\tilde{\boldsymbol{y}}_h$. Methods are developed to estimate $\boldsymbol{G}$, the first attempts at least squares forecast reconciliation were made by @Hyndman2011-jv.

They proposed the optimal $\boldsymbol{G}$ based on the regression model:

$$
\hat{\boldsymbol{y}}_h = \boldsymbol{S} \boldsymbol{\beta}_h + \boldsymbol{\epsilon}_h,
$$

where $\boldsymbol{\epsilon}_h = \tilde{\boldsymbol{y}}_h - \hat{\boldsymbol{y}}_h$ is the coherency error with variance $\boldsymbol{V}_h$. This led to the GLS solution: $\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{V}_h^{-1} \boldsymbol{S})^{-1} \boldsymbol{S}' \boldsymbol{V}_h^{-1}$.

However, the covariance matrix $\boldsymbol{V}_h$ is unknown (and later shown by @Wickramasuriya2019-xq to be unidentifiable), and replaced by an identity matrix. The method then collapses into an OLS solution: $\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{S})^{-1} \boldsymbol{S}'$.

An obvious drawback of the OLS solution is that it weights all series in any level equally, regardless of its scale or forecast error variance. The issue prompted @Hyndman2016-ic to propose a WLS solution, where the series are weighted by the inverse variances of the base forecast errors. The WLS solution is $\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{\Lambda}_h^{-1} \boldsymbol{S})^{-1} \boldsymbol{S}' \boldsymbol{\Lambda}_h^{-1}$. $\boldsymbol{\Lambda}_h = \text{diag} (\boldsymbol{W}_h)$ and $\boldsymbol{W}_h = Var(\boldsymbol{y}_h - \hat{\boldsymbol{y}}_h)$.

## Minimum Trace (MinT) Reconciliation

@Wickramasuriya2019-xq reframed the problem by taking an optimisation approach. They formulated the problem as minimising the variances of all reconciled forecasts from @eq-sgy. They showed that this is equivalent to minimising the trace of the base forecast error covariance matrix (sum of the diagonal elements - the variances). This is known as the Minimum Trace (MinT) reconciliation method. The MinT solution is given by

$$
\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{W}_h^{-1} \boldsymbol{S})^{-1}
\boldsymbol{S}' \boldsymbol{W}_h^{-1}
$$

and $\boldsymbol{W}_h$ is the covariance matrix of the h-step-ahead base forecast errors.

@Wickramasuriya2019-xq also showed that MinT is an algebraic generalisation of the GLS, and the OLS and WLS methods are special cases of MinT when $\boldsymbol{W}_h$ is an identity and a diagonal matrix, respectively. 

The MinT solution hinges on a reliable, positive-definite estimate of $\boldsymbol{W}_h$, which is challenging to estimate in high-dimensional setting. Therefore, we will be exploring alternative covariance estimators.



## Alternative Covariance Estimators

We reconstruct estimator of $\boldsymbol{W}_h$ as $\hat{\boldsymbol{W}}_h = k_h \, g(\hat{\boldsymbol{W}}_1)$, and $g(.)$ is an estimator function of the unbiased sample covariance of in-sample one-step-ahead base forecast errors $\hat{\boldsymbol{W}}_1 = \frac{1}{T} \sum_{t=1}^{T} \hat{\boldsymbol{e}}_{t|t-1} \hat{\boldsymbol{e}}_{t|t-1}'$


### (a) Shrinkage {-}

The proposed MinT approach by @Wickramasuriya2019-xq uses the shrinkage estimator from @Schafer2005-yw. \textcolor{red}{It guarantees positive definiteness and variance reduction for the covariance matrix, especially when $p>T$}. The shrinkage estimator is given by:

$$
\hat{\boldsymbol{W}}^{shr}_{1, D} = \lambda_D \hat{\boldsymbol{W}}_{1, D} + (1 - \lambda_D) \hat{\boldsymbol{W}}_1
$$

$\hat{\boldsymbol{W}}_{1, D}$ is a diagonal matrix comprising the diagonal entries of $\hat{\boldsymbol{W}}_1$. This approach will shrink the covariance matrix $\hat{\boldsymbol{W}}_1$ towards its diagonal version, meaning the off-diagonal elements are shrunk towards zero while the diagonal ones remain unchanged.

@Schafer2005-yw also proposed an optimal shrinkage intensity parameter $\lambda_D$ for this setting, assuming the variances are constant:

$$
\hat{\lambda}_D = \frac{\sum_{i \neq j} \widehat{Var}(\hat{r}_{ij})} 
{\sum_{i \neq j} \hat{r}_{ij}^2}
$$

where $\hat{r}_{ij}$ is the $ij$th element of $\hat{\boldsymbol{R}}_1$, the 1-step-ahead sample correlation matrix (obtained from $\hat{\boldsymbol{W}}_1$) to shrink it toward an identity matrix

The optimal lambda is obtained by minimising the $MSE(\hat{\boldsymbol{W}}_1) = Bias(\hat{\boldsymbol{W}}_1)^2 + Var(\hat{\boldsymbol{W}}_1)$. More specifically, we trade the unbiasedness of the sample covariance matrix for a lower variance. The objective function itself does not take into account any possible principal components structure in the data, and is not flexible enough since it shrinks all off-diagonal elements equally towards zeros.


### (b) NOVELIST {-}

The NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance) estimator, proposed by @Huang2019-ua, is currently the main focus of this research project. \textcolor{red}{It introduces adaptive sparsity, retaining strong correlations while discarding weak, noisy links. NOVELIST offers more flexibility than the shrinkage estimator, however, it does not guarantee positive definiteness.}

The method is based on the idea of soft-thresholding the sample covariance matrix, then performing shrinkage towards this thresholded version. This introduces an extra parameter, the threshold $\delta$, which is used to control the amount of soft-thresholding. The NOVELIST estimator is given by:

$$
\hat{\boldsymbol{W}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{W}}_{1, \delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{W}}_1
$$

By convenient setting, we rewrite it as in correlation matrix:

$$
\hat{\boldsymbol{R}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{R}}_{1,\delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{R}}_1,
$$

In this setting, $\hat{\boldsymbol{R}}_{1,\delta}$ is the thresholded sample correlation matrix, where each element is regularised by:

$$
\hat{r}_{1,ij}^\delta = \operatorname{sign}(\hat{r}_{1,ij}) \, (|\hat{r}_{1,ij}| - \delta)_+.
$$

For a given threshold $\delta$, @Huang2019-ua derived the optimal shrinkage intensity parameter $\lambda(\delta)$ using Ledoit-Wolf's lemma [@Ledoit2003-qv]. It can be computed as:

$$
\hat{\lambda}(\delta) = \frac{
  \sum_{i \neq j} \widehat{Var}(\hat{r}_{1,ij}) \; I(|\hat{r}_{1,ij}| \leq \delta)
} {
  \sum_{i \neq j} (\hat{r}_{1,ij} - \hat{r}_{1,ij}^\delta)^2
}
$$

On the other hand, the optimal threshold $\delta^*$ does not have a closed-form solution, and is typically obtained by executing a rolling-window cross-validation procedure. The idea is to, for each rolling-window set, loop through a range of threshold values, compute the corresponding $\hat{\lambda}(\delta)$ and $\hat{\boldsymbol{R}}^{N}_{1}$. Then we select the one that minimises the average out-of-sample reconciled forecast errors. Although it is not required to fit forecasting models multiple time, the cross-validation procedure is still computationally expensive as it computes the NOVELIST estimator and perform reconciliation for each threshold value. The formal algorithm is given in the @sec-novelist_cv Appendix.

\textcolor{red}{Note that when $\delta \in \bigl[ \text{max}_{ij}|\hat{r}_{1,ij}|, \; 1 \bigr]$, the NOVELIST estimator collapses to the shrinkage estimator, and when $\delta = 0$, it becomes the sample covariance matrix.}


### (c) POET {-}

The POET (Principal Orthogonal complEment Thresholding) estimator, proposed by @Fan2013-jz, is another "sparse" + "non-sparse" covariance estimator. \textcolor{red}{It takes the latent factors into account, and is appealing when there are common drivers in the time series within the hierarchy.}

The POET method starts by decomposing the correlation matrix $\hat{\boldsymbol{R}}_1$ into a prominent principle components part (low-rank) and a orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$ (the correlation matrix after removing the first $K$ principal components). Then it applies thresholding to $\hat{\boldsymbol{R}}_{1, K}$. The POET estimator is given by:

$$
\hat{\boldsymbol{R}}^{K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + T(\hat{\boldsymbol{R}}_{1, K})
$$

where $\hat{\gamma}_k$ and $\hat{\boldsymbol{\xi}}_k$ are the $k$th eigenvalue and eigenvector of the sample covariance matrix. $T(.)$ is the thresholding function, which can be either soft-thresholding, hard-thresholding, or others.


### (d) PC-adjusted NOVELIST {-}

\textcolor{red}{This approach is best of both worlds, leveraging the strengths of both NOVELIST and POET. The PC-adjusted NOVELIST overcomes the shortcomings of the current shrinkage estimator, taking prominent PCs into account while also offers extra flexibility.} The idea is to apply the NOVELIST estimator to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$, and then add the principal components part back. The PC-adjusted NOVELIST estimator is formulated as:

$$
\hat{\boldsymbol{R}}^{N, K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + \hat{\boldsymbol{R}}^{N}_{1, K}
\; ,
$$

and $\hat{\boldsymbol{R}}^{N}_{1, K}$ is the NOVELIST estimator applied to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$. Similar to the NOVELIST estimator, $\hat{\boldsymbol{R}}^{N, K}_{1}$ is not guaranteed to be positive definite.


# Experimental Design

The experimental design is to simulate a hierarchical time series data set, then apply the MinT reconciliation method with different covariance estimators. The data set will be split into training and test sets. In case of cross-validation, the training set will be further split into training and validation sets. The data generating process is described in @sec-dgp Appendix.

- Put some graphs here

# Timeline & Milestones

```{r timeline}
#| echo: false
#| message: false
#| warning: false
library(tibble)
library(knitr)

tibble(
  Period = c(
    "March - April", 
    "May", 
    "June - July", 
    "August",
    "September - October"
  ),
  Task = c(
    "Literature review, methods implementation (shrinkage, NOVELIST) + simulation framework",
    "Simulation + NOVELIST assessment",
    "Experiments with POET and PC-adjusted NOVELIST",
    "Real-world application + evaluation",
    "Final writing + submission"
  ),
  Deliverable = c(
    "Main codes + paper draft",
    "Results: NOVELIST",
    "Results: POET & PC-adjusted NOVELIST",
    "Application completed",
    "Thesis manuscript"
  )
) %>%
  kable(caption = "Project Timeline and Key Milestones")
```


# Expected Contributions

\pagebreak

# Appendix {#sec-appendix} 

## Algorithm: NOVELIST cross-validation for optimal threshold $\delta^*$ {#sec-novelist_cv}

![NOVELIST cross-validation procedure](D:/Github/Recon_Honours_Thesis/research_proposal/figs/novelist_cv_algo.png){#fig-novelist_cv height=70%}

## Data generating progress design {#sec-dgp}

The designed data generating process for bottom-level series is a stationary VAR(1) process, with the following structure:

$$
\boldsymbol{y}_t = \boldsymbol{A} \boldsymbol{y}_{t-1} + \boldsymbol{e}_t,
$$

where $\boldsymbol{A}$ is a $p \times p$ block diagonal matrix of autoregressive coefficients $\boldsymbol{A} = diag(\boldsymbol{A}_1, \ldots, \boldsymbol{A}_m)$, with each $\boldsymbol{A}_i$ being a $p_i \times p_i$ matrix. The block diagonal structure ensures that the time series are grouped into $m$ groups, with each group having its own autoregressive coefficients. This aim to simulate the interdependencies between the time series within each group, where reconciliation will be better performed than the usual base forecasts.

The model is added with a Gaussian innovation process $\boldsymbol{e}_t$, with covariance matrix $\Sigma$. The covariance matrix $\Sigma$ is generated specifically in the following way:

1. A compound symmetric correlation matrix is used for each block of size $p_i$ in $\boldsymbol{A}_i$, where the coefficients are sampled from a uniform distribution.

2. The correlations between different blocks are imposed using the Algorithm 1 in @Hardin2013-wu.

3. The covariance matrix $\Sigma$ is then constructed by uniform sampling of standard deviations for all $p$ series.

We have an option to randomly flip the signs of the covariance elements, which will create a more realistic structure in the innovation process.



<!-- ```{=latex} -->
<!-- \begin{algorithm} -->
<!-- \caption{Cross-validation procedure to select optimal threshold $\delta^*$ for NOVELIST} -->
<!-- \begin{algorithmic}[1] -->
<!-- \State \textbf{Input:} Residual matrix $E$ of dimension $T \times p$, grid of threshold candidates $\{\delta_j\}$, window size $n$ -->
<!-- \For{each threshold candidate $\delta_j$} -->
<!--     \For{$t = n+1$ to $T$} -->
<!--         \State Define in-sample window: $E_{1:n} = \{e_{t-n}, ..., e_{t-1}\}$ -->
<!--         \State Compute sample covariance $\hat{\Sigma}_t = \frac{1}{n} E_{1:n}^\top E_{1:n}$ -->
<!--         \State Compute thresholded covariance: $\hat{\Sigma}^{\text{thr}}_t = \text{Threshold}(\hat{\Sigma}_t, \delta_j)$ -->
<!--         \State Compute NOVELIST: $\hat{\Sigma}^{\text{nov}}_t = (1 - \lambda) \hat{\Sigma}_t + \lambda \hat{\Sigma}^{\text{thr}}_t$ -->
<!--         \State Compute inverse: $W_t = (\hat{\Sigma}^{\text{nov}}_t)^{-1}$ -->
<!--         \State Compute MinT reconciliation matrix: $P_t = (S^\top W_t S)^{-1} S^\top W_t$ -->
<!--         \State Compute forecast error: $e_t = y_t - S P_t \hat{y}_t$ -->
<!--     \EndFor -->
<!--     \State Compute total CV loss $L_j = \sum_t \|e_t\|^2$ -->
<!-- \EndFor -->
<!-- \State \textbf{Output:} $\delta^* = \arg\min_{\delta_j} L_j$ -->
<!-- \end{algorithmic} -->
<!-- \end{algorithm} -->
<!-- ``` -->


\pagebreak

# References








---
title: "Enhancing Forecast Reconciliation: A Study of Alternative Covariance Estimators"
author: Vincent Su
format: 
  pdf:
    include-in-header:
      text: |
        <!-- \usepackage{amsmath} -->
        <!-- \usepackage{mathspec} -->
        \usepackage{algorithm}
        \usepackage{algpseudocode}
        \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
        \usepackage{array}
    mathspec: true
    number-sections: true
execute:
  cache: true
bibliography: D:/Github/Recon_Honours_Thesis/references.bib
# bibliographystyle: apa
csl: D:/Github/Recon_Honours_Thesis/apa.csl
---

# Objectives

Globally, Starbucks manages thousands of outlets across dozens of countries, each projecting sales for dozens of beverages. Yet when regional, national, and brand-wide forecasts are generated, discrepancies of millions of dollars emerge simply because individual outlet forecasts fail to respect the hierarchical nature of the data. Forecast reconciliation comes in to solve this huge problem, where the individual forecasts are adjusted to satisfy the aggregation constraints. Among the various reconciliation methods, **MinT** (Minimum Trace) is considered the optimal approach, however, it requires a good estimate of the covariance matrix of the base forecast errors. The current practice is to use the shrinkage estimator (often shrinking toward a diagonal matrix), but it lacks flexibility and might neglect the prominent structure presented. In this project, we aim to assess the forecasting performance of MinT when different covariance estimators are used, namely NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance), POET (Principal Orthogonal complEment Thresholding), and others.

# Background

In time series forecasting, aggregation occurs in a variety of settings. A concrete example of a hierarchy would be the Starbucks sales data, where their total sales is the sum of all countries they are operating in, and each national sales is the sum for all cities, and the sales for each city come from many outlets. Data on national tourism, electricity demand, or Gross Domestic Product (GDP) are similar examples of hierarchical time series. The impact of methods for forecasting hierarchical time series has not been limited to academia, with industry also showing a strong interest. Many companies have adopted these methods in practice, including Amazon, the International Monetary Fund, IBM, SAP, and more [@Athanasopoulos2024-as].

The hierarchical structure can be represented as a tree, as shown in @fig-hierarchy. The top level of the tree represents the total value of all series, while the lower levels represent the series at different levels of disaggregation. When there are attributes of interest that are crossed, such as the Starbucks drinks sales at any aggregation level (brand-wise, national, city, or outlet) is also considered by kinds of drinks (e.g., coffees, refreshers), the structure is described as a grouped time series (illustrated in @fig-grouped).

![A 2-level hierarchical tree structure](D:/Github/Recon_Honours_Thesis/research_proposal/figs/hierarchical_structure.png){#fig-hierarchy width="40%" height="40%"}

![A 2-level grouped structure, which can be considered as the union of two hierarchical trees with common top and bottom level series](D:/Github/Recon_Honours_Thesis/research_proposal/figs/grouped_structure.png){#fig-grouped width="90%" height="40%"}


For simplicity, we refer to both of these structures as hierarchical time series, we will distinguish between them if and when it is necessary. All hierarchical structures can be represented using matrix algebra:

$$
\boldsymbol{y}_t = \boldsymbol{S} \boldsymbol{b}_t,
$$

where $\boldsymbol{S}$ is a summing matrix of order $n \times n_b$ which aggregates the bottom-level series $\boldsymbol{b}_t \in \mathbb{R}^{n_b}$ to the series at aggregation levels above. The $n$-vector $\boldsymbol{y}_t \in \mathbb{R}^n$ contains all observations at time $t$.

The summing matrix $\boldsymbol{S}$ for the tree structure in @fig-hierarchy is:

$$
\boldsymbol{S} = 
\left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\boldsymbol{I_4}}
\end{array}
\right].
$$

<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- 1 & 1 & 1 & 1 \\ -->
<!-- 1 & 1 & 0 & 0 \\ -->
<!-- 0 & 0 & 1 & 1 \\ -->
<!-- 1 & 0 & 0 & 0 \\ -->
<!-- 0 & 1 & 0 & 0 \\ -->
<!-- 0 & 0 & 1 & 0 \\ -->
<!-- 0 & 0 & 0 & 1 \\ -->
<!-- \end{array} -->
<!-- \right] -->

# Methodology

Assume we produce $h$-step-ahead base forecasts $\hat{\boldsymbol{b}}_h$ for the bottom-level series, obtained by any prediction methods. Then pre-multiplying them by $\boldsymbol{S}$ we get: 

$$
\tilde{\boldsymbol{y}}_h = \boldsymbol{S} \hat{\boldsymbol{b}}_h .
$$ {#eq-sb}

We refer to $\tilde{\boldsymbol{y}}_h$ as coherent forecasts, as they respect the aggregation structure. We also refer to this way of obtaining coherent forecasts by summing the bottom-level forecasts as the bottom-up approach. However, generating forecasts this way is anchored only to prediction models at a single level, and will not be utilising the information from other levels. This drawback applies to the top-down and middle-out approaches. For example, the bottom-level data can be very noisy or even intermittent, and the higher-level data might be smoother due to the aggregation. 

Another issue with expressing reconciled methods as in @eq-sb is that it restricts the reconciliation to only single-level approaches. Thus, @Hyndman2011-jv suggested a generalised expression for all existing methods, which also provides a framework for new methods to be developed:

$$
\tilde{\boldsymbol{y}}_h = \boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_h,
$$ {#eq-sgy}

for a suitable $n_b \times n$ matrix $\boldsymbol{G}$. $\boldsymbol{G}$ maps the base forecasts of all levels $\hat{\boldsymbol{y}}_h$ down into the bottom level, which is then aggregated to the higher levels by $\boldsymbol{S}$.

The choice of $\boldsymbol{G}$ determines the composition of reconciled forecasts $\tilde{\boldsymbol{y}}_h$. Methods are developed to estimate $\boldsymbol{G}$, including the OLS and WLS developed by @Hyndman2011-jv and @Hyndman2016-ic, respectively. A more detailed explanation of these methods can be found in the @sec-ols-wls Appendix.


## Minimum Trace (MinT) Reconciliation

@Wickramasuriya2019-xq reframed the problem as minimising the variances of all reconciled forecast errors. They showed that this is equivalent to minimising the trace of the reconciled forecast error covariance matrix (sum of the diagonal elements - the variances). This is known as the Minimum Trace (MinT) reconciliation method. The MinT solution is given by

$$
\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{W}_h^{-1} \boldsymbol{S})^{-1}
\boldsymbol{S}' \boldsymbol{W}_h^{-1}
$$

and $\boldsymbol{W}_h$ is the positive definite covariance matrix of the $h$-step-ahead base forecast errors.

@Wickramasuriya2019-xq also showed that MinT is an algebraic generalisation of the GLS, and the OLS and WLS methods are special cases of MinT when $\boldsymbol{W}_h$ is an identity and a diagonal matrix, respectively. 

The MinT solution hinges on a reliable, positive-definite estimate of $\boldsymbol{W}_h$, which is challenging to estimate in high-dimensional setting. Therefore, we will be exploring alternative covariance estimators.



## Alternative Covariance Estimators

We reconstruct the estimator of $\boldsymbol{W}_h$ as $\hat{\boldsymbol{W}}_h = k_h \, g(\hat{\boldsymbol{W}}_1)$, where $k_h > 0$. The function $g(.)$ is an estimator of the unbiased sample covariance matrix of the in-sample one-step-ahead base forecast errors $\hat{\boldsymbol{W}}_1 = \frac{1}{T} \sum_{t=1}^{T} \hat{\boldsymbol{e}}_{t} \hat{\boldsymbol{e}}_{t}'$, and $T$ is the length of series' time dimension.

<!-- $\boldsymbol{e}}_{t|t-1}$ we keep things simplified without the conditional subscript -->

### (a) Shrinkage {-}

The proposed MinT approach by @Wickramasuriya2019-xq uses the shrinkage estimator from @Schafer2005-yw. It guarantees positive definiteness and variance reduction for the covariance matrix, especially when the total number of series $n>T$. The shrinkage estimator is given by:

$$
\hat{\boldsymbol{W}}^{shr}_{1} = \lambda_D \hat{\boldsymbol{W}}_{1, D} + (1 - \lambda_D) \hat{\boldsymbol{W}}_1,
$$

where $\hat{\boldsymbol{W}}_{1, D}$ is a diagonal matrix comprising the diagonal entries of $\hat{\boldsymbol{W}}_1$. We refer to any $\lambda \in [0,1]$ as the shrinkage intensity parameter, the subscript specifies which estimator it belongs to. This approach shrinks the covariance matrix $\hat{\boldsymbol{W}}_1$ towards its diagonal matrix, meaning the off-diagonal elements are shrunk towards zero while the diagonal ones remain unchanged.

@Schafer2005-yw also proposed an estimate of the optimal shrinkage intensity parameter $\lambda_D$:

$$
\hat{\lambda}_D = \frac{\sum_{i \neq j} \widehat{Var}(\hat{r}_{ij})} 
{\sum_{i \neq j} \hat{r}_{ij}^2}
$$

where $\hat{r}_{ij}$ is the $ij$th element of $\hat{\boldsymbol{R}}_1$, the 1-step-ahead sample correlation matrix (obtained from $\hat{\boldsymbol{W}}_1$). The optimal estimate is obtained by minimising $MSE(\hat{\boldsymbol{W}}_1) = Bias(\hat{\boldsymbol{W}}_1)^2 + Var(\hat{\boldsymbol{W}}_1)$. More specifically, we trade the unbiasedness of the sample covariance matrix for a lower variance. 

However, the hierarchical time series data often exhibit a prominent principal components structure, which is not taken into account by the shrinkage estimator. Taking an example of the Australian domestic overnight trips data set [@tourismau-jk], where the national trips are disaggregated into states and territories, and further into regions. We then fit ETS models to all series, using the algorithm from Fabletools R package [@fabletools-fa], and compute the one-step-ahead in-sample base forecast error covariance matrix $\hat{\boldsymbol{W}}_1$. The twenty largest eigenvalues of the covariance matrix are plotted in @fig-eigen. We can see that the point of inflexion occurs at the component with 5th largest eigenvalue, indicating a prominent principal components structure.

![Twenty largest eigenvalues of one-step-ahead in-sample base forecast error covariance, Australian domestic overnight trips](D:/Github/Recon_Honours_Thesis/research_proposal/figs/eigen_tourism.png){#fig-eigen width="70%"}

Additionally, the shrinkage estimator shrinks all off-diagonal elements towards zeros with equal weights. We might prefer to better preserve strong signals, and largely discard the effects small, noisy correlations. In the next sections, we will explore several options that can take the prominent structure into account, and/or also largely discard the effects of small correlations.

### (b) NOVELIST {-}

The NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance) estimator, proposed by @Huang2019-ua, is currently the main focus of this research project. It introduces a way to control the target matrix's sparsity, retaining strong correlations while discarding weak, noisy effects. NOVELIST offers more flexibility than the shrinkage estimator, which is useful when we believe that only a few variables are truly correlated. However, it does not guarantee to be positive definite.

The method is based on the idea of soft-thresholding the sample covariance matrix, then performing shrinkage towards this thresholded version. This introduces an extra parameter, the threshold $\delta$, which is used to control the amount of soft-thresholding. The NOVELIST estimator is given by:

$$
\hat{\boldsymbol{W}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{W}}_{1, \delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{W}}_1,
$$ {#eq-novelist-cov}

where $\hat{\boldsymbol{W}}_{1, \delta}$ is the thresholded version of $\hat{\boldsymbol{W}}_{1}$. By convenient setting, we can rewrite it in terms of correlation:

$$
\hat{\boldsymbol{R}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{R}}_{1,\delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{R}}_1,
$$ {#eq-novelist-cor}

In this setting, $\hat{\boldsymbol{R}}_{1,\delta}$ is the thresholded correlation matrix, where each element is regularised by:

$$
\hat{r}_{1,ij}^\delta = \text{sign}(\hat{r}_{1,ij}) \, 
\text{max}(|\hat{r}_{1,ij}| - \delta, \; 0),
$$ {#eq-soft-thr}

where $\delta \in [0,1]$ is the threshold parameter. For a given threshold $\delta$, @Huang2019-ua derived an analytical expression for the optimal shrinkage intensity parameter $\lambda(\delta)$ using Ledoit-Wolf's lemma [@Ledoit2003-qv], following similar logic to @Schafer2005-yw. It can be computed as:

<!-- - \textcolor{red}{@Huang2019-ua mentioned Ledoit-Wolf's lemma} -->

$$
\hat{\lambda}(\delta) = \frac{
  \sum_{i \neq j} \widehat{Var}(\hat{r}_{1,ij}) \; \boldsymbol{1}(|\hat{r}_{1,ij}| \leq \delta)
} {
  \sum_{i \neq j} (\hat{r}_{1,ij} - \hat{r}_{1,ij}^\delta)^2
},
$$ {#eq-lambda-thr}

where $\boldsymbol{1}(.)$ is the indicator function.

On the other hand, the optimal threshold $\delta^*$ does not have a closed-form solution, and is typically obtained by executing a rolling-window cross-validation procedure. The idea is to find the threshold $\hat{\delta^*}$, with the corresponding $\hat{\lambda}^*$ and $\hat{\boldsymbol{R}}^{N}_{1}(\hat{\delta^*}, \hat{\lambda}^*)$, that minimises the average out-of-sample reconciled forecast errors. The formal algorithm is given in the @sec-novelist_cv Appendix. Although it is not required to fit forecasting models multiple times, the cross-validation procedure is still computationally expensive as it computes the NOVELIST estimator and perform reconciliation for each threshold value.

Note that when $\delta \in \bigl[ \text{max}_{i \neq j}|\hat{r}_{1,ij}|, \; 1 \bigr]$, the NOVELIST estimator collapses to the shrinkage estimator, and when $\delta = 0$, it becomes the sample covariance matrix.


### (c) POET {-}

The POET (Principal Orthogonal complEment Thresholding) estimator, proposed by @Fan2013-jz, is another "sparse" + "non-sparse" covariance estimator. It takes the latent factors directly into its construction, and is appealing when there are common drivers in the time series within the hierarchy, as we saw in the Australian tourism example.

The POET method starts by decomposing the correlation matrix $\hat{\boldsymbol{R}}_1$ into a prominent principle components part (low-rank) and a orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$ (the correlation matrix after removing the first $K$ principal components). Then it applies thresholding to $\hat{\boldsymbol{R}}_{1, K}$. The POET estimator is given by:

$$
\hat{\boldsymbol{R}}^{K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + T(\hat{\boldsymbol{R}}_{1, K})
$$

where $\hat{\gamma}_k$ and $\hat{\boldsymbol{\xi}}_k$ are the $k$th largest eigenvalue and the corresponding eigenvector of the sample covariance matrix, respectively, and $T(.)$ is the thresholding function, which can be either soft-thresholding, hard-thresholding, or others.


### (d) PC-adjusted NOVELIST {-}

This approach is best of both worlds, leveraging the strengths of both NOVELIST and POET. The PC-adjusted (Principal-Component-adjusted) NOVELIST overcomes the shortcomings of the current shrinkage estimator, taking prominent PCs into account while also offers extra flexibility. The idea is to apply the NOVELIST estimator to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$, and then add the principal components part back. The PC-adjusted NOVELIST estimator is formulated as:

$$
\hat{\boldsymbol{R}}^{N, K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + \hat{\boldsymbol{R}}^{N}_{1, K}
\; ,
$$

where $\hat{\boldsymbol{R}}^{N}_{1, K}$ is the NOVELIST estimator applied to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$. Similar to the NOVELIST estimator, $\hat{\boldsymbol{R}}^{N, K}_{1}$ is not guaranteed to be positive definite.

Methods to ensure positive definiteness of the NOVELIST estimator (and its PC-adjusted variant) will be explored and studied in the project. @Huang2019-ua proposed to diagonalise the NOVELIST
estimator and replace any eigenvalues that fall under a certain small positive threshold by the value of that threshold. Alternatively, we can implement the algorithm of @Higham2002-te that computes the nearest positive definite matrix to a given matrix.


# Experimental Design

The experiment starts from simulating a hierarchical time series data set, then applying the MinT reconciliation method with different covariance estimators. The data set will be split into training and test sets. In the case of performing NOVELIST cross-validation, the training set will be further split into training and validation sets equally.

In this preliminary experiment, we consider a slightly large hierarchical structure. The structure consists of 3 levels with a total of 45 series. The bottom level has 36 series and are aggregated in groups of size six to form six level-1 series. The level-1 series are then aggregated in groups of size three to form two level-2 series, which are finally aggregated to form the top level. The observations at bottom level is generated from a VAR(1) process. A more detailed data generating process is described in @sec-dgp Appendix.

For each series, $T = 116$ or $316$ observations are generated. The first 100 or 300 observations are used for training, and the last 16 observations are used for testing. The remaining data is used to compute the best fitted ARIMA models by minimising the AICc criterion, in which we use the automatic algorithm from Fabletools R package [@fabletools-fa]. We refer to them as base models, and their base forecasts are then reconciled using the MinT with different covariance estimators. These include using the unbiased sample covariance matrix - MinT(Sample), the shrinkage estimator - MinT(Shrink), and the NOVELIST estimator - MinT(N). The Monte Carlo simulation is repeated 500 times, in which the parameters for data generating process is also randomly generated. The results are presented in @tbl-sim-summary.

```{r tbl-sim-summary}
#| echo: false
#| message: false
#| warning: false

# Code --------------------------------------
library(tidyr)
library(ggplot2)
library(tsibble)
library(dplyr)
library(purrr)
library(kableExtra)
library(knitr)

sim1 <- readRDS("D:/Github/Recon_Honours_Thesis/research_proposal/sim_results/S36-6-2-1_T100_M500_par.rds")
sim2 <- readRDS("D:/Github/Recon_Honours_Thesis/research_proposal/sim_results/S36-6-2-1_T300_M500_par.rds")

transfrom_sim_MSE <- function(MSE, to_ts = T) {
  df <- imap_dfr(MSE, function(mat, model_name) {
    as_tibble(mat) %>%
      mutate(h = row_number()) %>%
      pivot_longer(cols = -h, names_to = "series", values_to = "MSE") %>%
      mutate(
        .model = model_name,
      ) %>%
      select(.model, series, h, MSE)
  })
  if (to_ts) {
    return(
      as_tsibble(key = c(.model, series), index = h)
    )
  }
  return(df)
}

MSE1 <- transfrom_sim_MSE(sim1$MSE, F)
MSE2 <- transfrom_sim_MSE(sim2$MSE, F)

hier_level <- MSE1$series |>
  substring(1, 1) |>
  unique() |> sort(decreasing = TRUE)

MSE_full <- bind_rows(MSE1, MSE2, .id = "id") |>
  mutate(
    level = match(substring(series,1,1), hier_level)
  )

group_defs <- tibble(
  h_group = c("h=1",  "h=1-8",   "h=1-16"),
  h_max   = c(1,      8,         16)
)

MSE_tidy <- MSE_full %>%
  # filter(h<=16) %>%
  crossing(group_defs) %>%    # gives every row all 3 candidate groups
  filter(h <= h_max) %>%      # keep only the overlapping ones
  # filter(h>15)
  group_by(id, .model, level, h_group) %>%
  summarise(mean_MSE = mean(MSE), .groups = "drop")

MSE_tidy <- MSE_tidy %>%
  group_by(level, id, h_group) %>%
  mutate(
    base_MSE = mean_MSE[.model == "base"],
    change = if_else(.model == "base",
                     formatC(mean_MSE, digits = 1, format = "f"),
                     formatC(100 * (mean_MSE - base_MSE) / base_MSE, digits = 1, format = "f"))
  ) %>%
  ungroup()

MSE_tidy <- MSE_tidy %>%
  pivot_wider(
    id_cols = c(level, .model),
    names_from = c(id, h_group),
    values_from = change
  ) %>%
  mutate(
    .model = recode(.model,
                    "mint_sample" = "MinT(Sample)",
                    "mint_shr"    = "MinT(Shrink)",
                    "mint_n"      = "MinT(N)",
                    "base"        = "Base"
    ),
    .model = factor(.model, levels = c("MinT(Sample)", "MinT(Shrink)", "MinT(N)", "Base"))
  ) %>%
  mutate(
    level = recode(level,
                   `1` = "Top level",
                   `2` = "Level 1",
                   `3` = "Level 2",
                   `4` = "Bottom level"
    ),
    level = factor(level, levels = c("Top level", "Level 1", "Level 2", "Bottom level"))
  ) %>%
  arrange(level, .model)

row_levels <- MSE_tidy$level
MSE_display <- MSE_tidy %>% select(-level)


# Kable --------------------------------------
my_kable <- MSE_display %>%
  mutate(across(where(is.numeric), ~formatC(.x, digits = 1, format = "f"))) %>%
  kable(
    booktabs = TRUE,
    longtable = TRUE,
    align = c("l", rep("r", 6)),  # now only 7 cols
    caption = "Out-of-sample forecast results from simulating 45 series with 3 levels",
    col.names = c(" ", rep(c("h=1", "h=1-8", "h=1-16"), 2)),
    escape = FALSE
  ) %>%
  add_header_above(c(" " = 1, "T = 100" = 3, "T = 300" = 3)) %>%
  {
    tbl <- .
    start <- 1
    for (lvl in unique(row_levels)) {
      end <- start + sum(row_levels == lvl) - 1
      tbl <- tbl %>% pack_rows(lvl, start, end)
      start <- end + 1
    }
    tbl
  }

my_kable
```

```{=latex}
{\footnotesize
\begin{center}
  \textbf{NOTES:} Base shows the average MSE of the base forecasts. A negative (positive) entry above this row shows a percentage decrease (increase) in average MSE relative to the base forecasts.
\end{center}
}
```

In this experiment, where the series are contemporaneously correlated, the Minimum Trace approach improves the out-of-sample forecast accuracy tremendously, especially for the levels closer to the aggregated top level. Both MinT(Shrink) and MinT(N) consistently outperform the MinT(Sample) at any level and forecasting horizon in terms of accuracy. 

The results also show that the MinT(Shrink) and MinT(N) methods are comparable, and almost identical in terms of accuracy. The MinT(Shrink) forecasts are slightly more accurate, but again, the difference is negligible. This suggests that the NOVELIST estimator might be shrinking the same way as the shrinkage estimator, and a more thorough investigation is needed to understand the differences between them.

In this preliminary experiment, the observations are generated from a different data generating process for each simulation, attempting to assess the predictive accuracy between methods. In future experiments, we will consider a fixed data generating process, in order to assess the behaviors of NOVELIST's shrinkage intensity parameter $\lambda_{\delta}$ and threshold $\delta$. We will also explore the the effect of correlations and larger hierarchical structures, where the in-sample one-step-ahead base forecast error variance $\hat{\boldsymbol{W}}_1$ is not positive definite.

Regarding other covariance estimators, we will implement the POET and PC-adjusted NOVELIST estimators, and experiment whether the prominent principal component structure can be exploited to improve the out-of-sample forecast performance. The project will continue to explore other high-dimensional covariance estimators, and their compatibility with the MinT reconciliation method.


\pagebreak

# Timeline & Milestones

```{r timeline}
#| echo: false
#| message: false
#| warning: false
library(tibble)
library(knitr)
library(kableExtra)

tibble(
  Period = c(
    "March",
    "April", 
    "May", 
    "June",
    "July-August", 
    "September",
    "October"
  ),
  Task = c(
    "Initial setup and understanding the research scope. Preliminary knowledge and Literature review",
    "Implementing the Min Trace approach with the shrinkage and NOVELIST estimator. Implementing rolling-window cross-validation for choosing NOVELIST threshold parameter. Exploring simulation framework.",
    "Executing simulation on different settings, in order to assess the performance and behaviours of NOVELIST under different conditions. Writing research proposal and presenting research project.",
    "Implementing POET and PC-adjusted NOVELIST estimators. Assessing whether prominent principal components structure can be exploited to improve the out-of-sample forecast performance.",
    "Exploring other high-dimensional covariance estimators",
    "Real-world application + evaluation (e.g. company sales data or Australia tourism data)",
    "Final writing + submission"
  ),
  Deliverable = c(
    "Literature review + paper draft",
    "Main codes for estimators + simulation framework + cross-validation",
    "NOVELIST simulation results + research proposal + presentation",
    "POET & PC-adjusted NOVELIST simulation results",
    "Other estimators simulation results",
    "Application",
    "Thesis manuscript"
  )
) %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    longtable = TRUE,
    caption = "Project Timeline and Key Milestones",
    escape = TRUE
  ) %>%
  column_spec(1, width = "1.9cm") %>%
  column_spec(2, width = "8cm") %>%
  column_spec(3, width = "4.8cm") %>%
  kable_styling(latex_options = "hold_position", font_size = 10)
```


<!-- # Expected Contributions -->

\pagebreak

# Appendix {#sec-appendix} 

## OLS and WLS Reconciliation {#sec-ols-wls}

The first attempts at least squares forecast reconciliation were made by @Hyndman2011-jv. They proposed the optimal $\boldsymbol{G}$ based on the regression model:

$$
\hat{\boldsymbol{y}}_h = \boldsymbol{S} \boldsymbol{\beta}_h + \boldsymbol{\epsilon}_h,
$$

where $\boldsymbol{\epsilon}_h = \tilde{\boldsymbol{y}}_h - \hat{\boldsymbol{y}}_h$ is the coherency error with variance $\boldsymbol{V}_h$. This led to the GLS solution: $\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{V}_h^{-1} \boldsymbol{S})^{-1} \boldsymbol{S}' \boldsymbol{V}_h^{-1}$.

However, the covariance matrix $\boldsymbol{V}_h$ is unknown (and later shown by @Wickramasuriya2019-xq to be unidentifiable), and replaced by an identity matrix. The method then collapses into an OLS solution: $\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{S})^{-1} \boldsymbol{S}'$.

An obvious drawback of the OLS solution is that it assumes an identity matrix for the covariance, ignoring information on the errors' scale and variance. The issue prompted @Hyndman2016-ic to propose a WLS solution, where the series are weighted by the inverse variances of the base forecast errors. The WLS solution is $\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{\Lambda}_h^{-1} \boldsymbol{S})^{-1} \boldsymbol{S}' \boldsymbol{\Lambda}_h^{-1}$. $\boldsymbol{\Lambda}_h = \text{diag} (\boldsymbol{W}_h)$ and $\boldsymbol{W}_h = Var(\boldsymbol{y}_h - \hat{\boldsymbol{y}}_h)$.

\pagebreak

## Algorithm: NOVELIST cross-validation for optimal threshold $\delta^*$ {#sec-novelist_cv}

```{=latex}
\begin{algorithm}
\caption{Cross-validation procedure}
\begin{algorithmic}[1]

\State \textbf{Input:} Observations and fitted values $\boldsymbol{y}_t, \hat{\boldsymbol{y}}_t \in \mathbb{R}^n$ for $t = 1,\dots,T$, set of threshold candidates $\Delta$, window size $v$.

\State $\hat{\boldsymbol{e}}_t = \boldsymbol{y}_t - \hat{\boldsymbol{y}}_t$ for $t = 1,\dots,T$

\For{$i = v:T-1$}
    \State $j = i - v +1$
    \State $\hat{\boldsymbol{W}}_j = \frac{1}{v} \sum_{t=j}^{i} \hat{\boldsymbol{e}}_{t} \hat{\boldsymbol{e}}_{t}'$
    \State $\hat{\boldsymbol{D}}_j = \text{diag}(\hat{\boldsymbol{W}}_j)$
    \State $\hat{\boldsymbol{R}}_j = \hat{\boldsymbol{D}}_j^{-1/2} \hat{\boldsymbol{W}}_j \hat{\boldsymbol{D}}_j^{-1/2}$
    \For{$\delta \in \Delta$}
        \State Compute thresholded correlation $\hat{\boldsymbol{R}}_{j,\delta}$ using Equation 5
        \State Compute $\hat{\lambda}_{j,\delta}$ using Equation 6
        \State Compute $\hat{\boldsymbol{R}}^{N}_{j,\delta}$ using Equation 4
        \State $\hat{\boldsymbol{W}}^{N}_{j,\delta} = \hat{\boldsymbol{D}}_j^{1/2} \hat{\boldsymbol{R}}^{N}_{j,\delta} \hat{\boldsymbol{D}}_j^{1/2}$
        \State $\boldsymbol{G} = (\boldsymbol{S}' \hat{\boldsymbol{W}}^{N^{-1}}_{j,\delta} \boldsymbol{S})^{-1} \boldsymbol{S}' \hat{\boldsymbol{W}}^{N^{-1}}_{j,\delta}$
        \State Reconciled forecasts $\tilde{\boldsymbol{y}}_{i+1 | \delta} = \boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_{i+1}$
        \State $\tilde{\boldsymbol{e}}_{i+1 | \delta} = \boldsymbol{y}_{i+1} - \tilde{\boldsymbol{y}}_{i+1 | \delta}$
    \EndFor
\EndFor

\State $\text{MSE}_{\delta} = \frac{1}{T-v} \sum_{i=v}^{T-1} (\tilde{\boldsymbol{e}}_{i+1 | \delta})^2$ for each $\delta \in \Delta$
\State $\hat{\delta}^* = \arg\min_{\delta \in \Delta} \text{MSE}_{\delta}$
\State Compute $\hat{\lambda}^*$ on all training data using $\hat{\delta}^*$
\State Compute $\hat{\boldsymbol{R}}_1^*$ using $\hat{\delta}^*$ and $\hat{\lambda}^*$ on all training data, using Equation 3

\State \textbf{Output:} Estimate of optimal $\hat{\delta}^*$

\end{algorithmic}
\end{algorithm}
```

<!-- ![NOVELIST cross-validation procedure](D:/Github/Recon_Honours_Thesis/research_proposal/figs/novelist_cv_algo.png){#fig-novelist_cv height=70%} -->


## Data generating progress design {#sec-dgp}

The designed data generating process for bottom-level series is a stationary VAR(1) process, with the following structure:

$$
\boldsymbol{b}_t = \boldsymbol{A} \boldsymbol{b}_{t-1} + \boldsymbol{\epsilon}_t,
$$

where $\boldsymbol{A}$ is a $n_b \times n_b$ block diagonal matrix of autoregressive coefficients $\boldsymbol{A} = diag(\boldsymbol{A}_1, \ldots, \boldsymbol{A}_m)$, with each $\boldsymbol{A}_i$ being a $n_{b,i} \times n_{b,i}$ matrix. The block diagonal structure ensures that the time series are grouped into $m$ groups, with each group having its own autoregressive coefficients. This aim to simulate the interdependencies between the time series within each group, where reconciliation will be better performed than the usual base forecasts.

The model is added with a Gaussian innovation process $\boldsymbol{\epsilon}_t$, with covariance matrix $\Sigma$. The covariance matrix $\Sigma$ is generated specifically in the following way:

1. A compound symmetric correlation matrix is used for each block of size $n_{b,i}$ in $\boldsymbol{A}_i$, where the coefficients are sampled from a uniform distribution between 0 and 1.

2. The correlations between different blocks are imposed using the Algorithm 1 in @Hardin2013-wu.

3. The covariance matrix $\Sigma$ is then constructed by uniform sampling of standard deviations, in a range of $[\sqrt{2}, \sqrt{6}]$, for all $n_b$ series.

We have an option to randomly flip the signs of the covariance elements, which will create a more realistic structure in the innovation process. This can be done by pre- and post-multiplying $\Sigma$ by a random diagonal matrix $V$ with entries sampled from $\{-1, 1\}$, yielding $\Sigma' = V \Sigma V$.




\pagebreak

# References








---
title: "Enhancing Forecast Reconciliation: A Study of Alternative Covariance Estimators"
author: 
  - Vincent Su
  - Shanika Wickramasuriya (supv.)
  - George Athanasopoulos (supv.)
format: 
  pdf:
    include-in-header:
      text: |
        <!-- \usepackage{amsmath} -->
        <!-- \usepackage{mathspec} -->
        \usepackage{algorithm}
        \usepackage{algpseudocode}
        \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
        \usepackage{array}
        \usepackage{float}
        \usepackage{xcolor}
    mathspec: true
    number-sections: true
# crossref:
#   custom:
#     - kind: float
#       key: alg
#       reference-prefix: Algorithm
#       caption-prefix: Algorithm
#       # for PDF output only, tell LaTeX to use the `algorithm` env:
#       latex-env: algorithm
link-citations: true
execute:
  cache: true
  echo: false
  warning: false
bibliography: D:/Github/Recon_Honours_Thesis/references.bib
# bibliographystyle: apa
csl: D:/Github/Recon_Honours_Thesis/apa.csl
editor_options: 
  chunk_output_type: console
---



```{r library}
library(ggplot2)
library(patchwork)
library(dplyr)
library(tidyr)
library(fable)
library(fabletools)
library(feasts)
library(tsibble)
library(lubridate)
library(knitr)
library(kableExtra)
library(stringr)
library(forcats)
library(tibble)
library(fmsb)

library(ReconCov)

# source all R scripts in thesis/R folder
R_files <- list.files("D:/Github/Recon_Honours_Thesis/R", pattern = "*.R", full.names = TRUE)
invisible(lapply(R_files, source))


path <- "D:/Github/Recon_Honours_Thesis"
knitr::opts_knit$set(root.dir = path)
# setwd(path)
```

# Abstract {.unnumbered}

<!-- [This is pasted from the Project Description]{style="color:red;"} -->

A collection of time series connected via a set of linear constraints is known as hierarchical time series. Forecasting these series without respecting the hierarchical nature of the data can lead to incoherent forecasts across aggregation levels and, in practice, reduced accuracy. Forecast reconciliation corrects this by adjusting base forecasts to satisfy such constraints. Among modern reconciliation methods, Minimum Trace (MinT) is widely used, however, it requires a good estimate of the forecast error covariance matrix. The current practice is to use the linear shrinkage toward a diagonal target to estimate 1-step-ahead covariance and proportionally scale it to approximate the h-step-ahead covariance matrix. This leaves a question of whether this method is appropriate for all real-world applications. We study alternative estimators that address the shortcomings of the current practice, including the NOVELIST estimator (shrinkage toward a soft-thresholded target), the PC-adjusted shrinkage (which utilises the latent factors structure), and horizon-specific estimators that relax proportional scaling. We evaluate MinT under these covariance estimates for both point and probabilistic reconciliation, and demonstrate their effectiveness and improvements over the shrinkage estimator in a complex, large-hierarchy dataset.


# Introduction

<!-- 

############  Paragraphs  ############ 

1. Introduce hierarchical time series and its importance in various applications.
2. Discuss the challenges in forecasting hierarchical time series, and current methods used. Introduce MinT.
3. Highlight the reliance of MinT on a good estimate of the covariance matrix of base forecast errors, and the problems with current estimation methods.
4. Note the Double Shrinkage estimator paper, but still early stage.
5. Mention advances in high-dimensional covariance estimation that can be potentially useful.
6. State the purpose and contribution of this paper.

-->

In time series forecasting, aggregation occurs in a variety of settings. For example, Starbucks operates in many countries, and each country has multiple cities where they have outlets. The sales data is structured *hierarchically*: the top level is the total sales across all countries, which are the sum of sales in each country, which in turn are the sum of sales in each city, and finally the sum of sales from each outlet in the city. As a result, there are over 50,000 sales series across all *aggregation levels*, and decision makers need forecasts at each level to manage inventory and plan marketing strategies effectively. The hierarchy can be even more complex if we consider the sales of different kinds of drinks (e.g., coffees, teas, refreshers) at each aggregation level. Such hierarchical structures also arises in many other decision-making contexts, from supply chains [@Seaman2022-bb; @Angam2025-od] and energy planning [@Di-Modica2021-ad], to macroeconomics [@El-Gemayel2022-hx; @Li2019-vv] and tourism analysis [@Athanasopoulos2009-lp]. Stakeholders in these settings need forecasts at several aggregation levels to allocate resources and manage risk. 

In practice, when forecasts are produced for all series (often called *base forecasts*), they typically violate the aggregation constraints observed in the data (e.g. the sum of all countries' sales does not equal the total sales). Such forecasts are called *incoherent*. Incoherence undermines downstream decisions that require internal consistency and can degrade forecasting performance. To tackle this problem, forecast reconciliation was introduced. Forecast reconciliation, a post-processing step, utilises the information from the hierarchical structure and data to adjust the initially produced base forecasts, so that the resulting forecasts are *coherent* (i.e. respecting the aggregation constraints). It first introduced by @Hyndman2011-jv, and later developed by @Van_Erven2015-nx, @Hyndman2016-ic, @Ben-Taieb2019-yx, @Wickramasuriya2019-xq, @Wickramasuriya2020-uk, and others to enhance point forecast performance. @Athanasopoulos2024-as provided a comprehensive review of the literature on forecast reconciliation. Among the modern methods, the Min Trace (MinT) approach developed by @Wickramasuriya2019-xq is is widely used due to its strong theoretical properties for minimising total reconciled error variance, computational efficiency, and robust empirical performance. MinT is implemented in popular R and Python software ecosystems [@O-Hara-Wild2024-we; @Nixtla].

A central difficulty for MinT is estimating the covariance matrix of base-forecast errors, particularly beyond one-step horizons. This is a high-dimensional estimation problem in which the number of series often exceeds the effective time dimension. A common practice, following @Wickramasuriya2019-xq, is to estimate the one-step covariance using linear shrinkage toward a diagonal target [@Schafer2005-yw], and to obtain multi-step covariances by proportional scaling of the one-step estimate. While convenient and guaranteed to produce a positive-definite estimator, this practice has three important shortcomings. First, the shrinkage is uniform across off-diagonals, applying a single penalty that may over-shrink genuine dependence and under-shrink noise. Second, many hierarchical data sets exhibit strong latent low-rank structure that is not explicitly exploited. Third, the proportionality relationship between the h-step-ahead and 1-step-ahead forecast error covariance matrices might not hold when error dynamics change with horizon.

These limitations matter because the theoretical advantages of MinT depend on the quality of the covariance estimate available in finite samples. Despite its central role, there has been limited work on tailored covariance estimation for reconciliation. An exception is the recent double-shrinkage proposal in @Carrara2025-rz, which introduces an additional target designed to encode conditional dependence suggested by the hierarchy. This line of work still remains at an early stage and does not fully address the aforementioned limitations.

Meanwhile, there has been substantial progress in high-dimensional covariance estimation that can be leveraged for MinT. Building on shrinkage, @Huang2019-ua proposed NOVELIST, which shrinks toward a soft-thresholded target rather than a diagonal one, improving flexibility to sparse targets. Extending beyond linear shrinkage, @Ledoit2012-ga (and further developed by @Ledoit2020-xt) introduced nonlinear shrinkage that replaces sample eigenvalues with data-driven nonlinear transformation of themselves. In a complementary direction, @Fan2013-jz introduce factor-based estimators that explicitly preserve latent low-rank structure while thresholding the idiosyncratic component. Related thresholding approaches, including hard thresholding [@Bickel2008-nh], generalised thresholding [@Rothman2009-yf], and adaptive thresholding [@Cai2011-jf], have also been developed. This growing toolkit provides multiple pathways to maximise the performance of MinT.

This paper focuses on covariance estimation for MinT. We examine the shortcomings of the standard shrinkage plus scaling practice and introduce alternative estimators that address these issues individually and in combination. Specifically, we study NOVELIST as a more adaptive target-based shrinkage; principal-component-adjusted (PC-adjusted) estimators that exploit latent factor structure; and horizon-specific estimators that relax proportional scaling, including direct estimation from multi-step residuals. We evaluate MinT under these alternatives for both point and probabilistic reconciliation. In a large, complex real-world hierarchy, we find: first, NOVELIST improves probabilistic performance relative to linear shrinkage, including higher empirical coverage; second, PC-adjusted estimators consistently outperform other estimators when common components are strong, for both point and probabilistic metrics; third, proportional scaling of the one-step covariance remains a competitive baseline, indicating that the assumption is not universally invalid.

The remainder of the paper is organised as follows. @sec-pre-theory sets out the framework for hierarchical time series, forecast reconciliation, and MinT, and motivates the need for improved covariance estimation. @sec-cov-est presents the covariance estimators considered, outlining their strengths and limitations in the reconciliation context. @sec-simulation details the simulation design and examines the performance of NOVELIST compared to shrinkage. @sec-empirical dives into a real-world application with all proposed methods and highlights behaviours not observed in the simplified simulations.

# Theoretical Framework {#sec-pre-theory}

## Hierarchical Time Series {#sec-hierarchy}

*Hierarchical time series* are multivariate time series $\boldsymbol{y}_t \in \mathbb{R}^n$ organised in a structure where the series adheres to some constraints. @fig-hierarchy illustrates a simple two-level hierarchical structure with one top-level series $y_{Tot,t}$, disaggregating down to two level-1 series $(y_{A,t}, y_{B,t})'$, and to four bottom-level series $(y_{A1,t}, y_{A2,t}, y_{B1,t}, y_{B2,t})'$. Here, the *aggregation constraints* imply that $y_{Tot,t} = y_{A,t} + y_{B,t}$, $y_{A,t} = y_{A1,t} + y_{A2,t}$, and $y_{B,t} = y_{B1,t} + y_{B2,t}$.

![A 2-level hierarchical tree structure](../figs/hierarchical_structure.png){#fig-hierarchy width="40%"}

The bottom-level (or most disaggregated) series are denoted as $\boldsymbol{b}_t \in \mathbb{R}^{n_b}$. Thus, the full vector of all series in the hierarchy can be represented as:

$$
\boldsymbol{y}_t = \boldsymbol{S} \boldsymbol{b}_t,
$$

where $\boldsymbol{S} \in \mathbb{R}^{n \times n_b}$ is a summing matrix that aggregates the bottom-level to all-level series. The summing matrix $\boldsymbol{S}$ for the tree structure in @fig-hierarchy is:

$$
\boldsymbol{S} = 
\left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\boldsymbol{I_4}}
\end{array}
\right].
$$

The matrix $\boldsymbol{S}$ encodes the aggregation constraints implied by the structure. Hence, the columns of $\boldsymbol{S}$ span a linear subspace. Any observation $\boldsymbol{y}_t$ that lies inside this subspace is called *coherent*, while those outside are *incoherent*. We refer to the subspace spanned by $\boldsymbol{S}$ as the *coherent subspace* $\mathfrak{s} \in \mathbb{R}^{n_b}$.

![A 2-level grouped structure, which can be considered as the union of two hierarchical trees with common top and bottom level series](../figs/grouped_structure.png){#fig-grouped width="90%"}

This representation extends beyond hierarchical (nested) structures. When attributes of interest are crossed, such as the company sales at any aggregation level (company-wise, city-wise, or outlet-wise) is also considered by kinds of products, the structure is described as a *grouped structure*. In grouped systems, as illustrated in @fig-grouped, aggregation and disaggregation paths are not unique, but the linear constraints can still be written compactly through a summing matrix $\boldsymbol{S}$. For simplicity, we refer to both structures as hierarchical structure and distinguish between them when needed.

<!-- There are other forms of structures, such as temporal hierarchies or non-linear constraints, but they are beyond the scope of this paper.  -->

In practice, when we produce forecasts for each individual series, referred to as *base forecasts* $\hat{\boldsymbol{y}}_{t+h|t}$, they generally violate the aggregation constraints, and thus are incoherent. Coherency can be restored by linearly projecting the base forecasts onto the coherent subspace $\mathfrak{s}$ using a projection matrix $\boldsymbol{P}$: $\tilde{\boldsymbol{y}}_{t+h|t} = \boldsymbol{P} \hat{\boldsymbol{y}}_{t+h|t}$, where $\tilde{\boldsymbol{y}}_{t+h|t}$ are the *reconciled forecasts*.

![Geometry of probabilistic forecast reconciliation. The base forecast distribution is projected orthogonally onto the coherent subspace (purple line), resulting in the reconciled forecast distribution (red). The projection is defined by the projection matrix $\boldsymbol{P}$. Note that this figure is schematic since most applications are high-dimensional.](../figs/geometry_orthogonal.png){#fig-geometry width="60%"}

Many existing reconciliation methods including the OLS [@Hyndman2011-jv], WLS [@Hyndman2016-ic], and MinT [@Wickramasuriya2019-xq] express the projection matrix as $\boldsymbol{P} = \boldsymbol{S} \boldsymbol{G}$, for a suitable $n_b \times n$ mapping matrix $\boldsymbol{G}$. The idea is to map the base forecasts of all levels $\hat{\boldsymbol{y}}_{t+h|t}$ down into the bottom level, which is then aggregated to the higher levels by $\boldsymbol{S}$. Since the projection matrix $\boldsymbol{P}$ is idempotent, $\boldsymbol{G}$ must satisfy the condition $\boldsymbol{S} \boldsymbol{G} \boldsymbol{S} = \boldsymbol{S}$. Within this class, a broad family of mapping matrix is given by: $\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{M}^{-1} \boldsymbol{S})^{-1} \boldsymbol{S}' \boldsymbol{M}^{-1}$, for some positive definite matrix $\boldsymbol{M} \in \mathbb{R}^{n \times n}$ [@Gamakumara2020-ml].

When setting $\boldsymbol{M} = \boldsymbol{I}_n$, the identity matrix, this reduces to the OLS reconciliation, which corresponds to an orthogonal projection onto the coherent subspace (similar to @fig-geometry). A schematic illustration of this projection is depicted in @fig-geometry. Reconciliation takes the (possibly elliptical) base forecast distribution and projects it onto the coherent subspace, resulting in the reconciled forecast distribution. The choice of $\boldsymbol{M}$ determines the direction of the projection, which can yield oblique projection to deliver better-performing forecasts.


## The Minimum Trace (MinT) Reconciliation

@Wickramasuriya2019-xq showed that by setting $\boldsymbol{M} = \boldsymbol{W}_h = \mathbb{E}( \hat{\boldsymbol{e}}_{t+h|t} \; \hat{\boldsymbol{e}}_{t+h|t}' )$, the covariance matrix of the $h$-step-ahead base forecast errors $\hat{\boldsymbol{e}}_{t+h|t} = \boldsymbol{y}_{t+h} - \hat{\boldsymbol{y}}_{t+h|t}$, we essentially minimise the total variance of the reconciled forecast errors across all series. Equivalently, MinT is the unique linear-unbiased reconciler that minimises the trace of $\text{Var}[y_{t+h} - \tilde{y}_{t+h|t}] = \boldsymbol{S} \boldsymbol{G}_h \boldsymbol{W}_h \boldsymbol{G}_h' \boldsymbol{S}'$. This method is thus called Minimum Trace (MinT) reconciliation. The matrix $\boldsymbol{G}_h$ is thus given by:

$$
\boldsymbol{G}_h = (\boldsymbol{S}' \boldsymbol{W}_h^{-1} \boldsymbol{S})^{-1}
\boldsymbol{S}' \boldsymbol{W}_h^{-1},
$$

provided that $\boldsymbol{W}_h$ is positive definite. Intuitively, $\boldsymbol{W}_h^{-1}$ down-weights the base forecasts of series with high uncertainty, and up-weights those with low uncertainty, when mapping to the bottom level.

Although originally developed for point reconciliation, @Wickramasuriya2024-mb showed that MinT extends naturally to the probabilistic setting when base predictive distributions are Gaussian. If $h$-step-ahead base forecast distribution be $\hat{\boldsymbol{y}}_{t+h|t} \sim \mathcal{N}(\hat{\boldsymbol{y}}_{t+h|t}, \boldsymbol{W}_h)$, then reconciliation gives $\tilde{\boldsymbol{y}}_{t+h|t} = \boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_{t+h|t} \sim \mathcal{N}(\tilde{\boldsymbol{y}}_{t+h|t}, \boldsymbol{S} \boldsymbol{G}_h \boldsymbol{W}_h \boldsymbol{G}_h' \boldsymbol{S}')$. Within the class of linear-unbiased reconcilers, MinT also minimises the negative log score (equivalently, the Gaussian log predictive score) of the reconciled distribution.

In this paper we adopt this Gaussian framework to compare covariance estimators for MinT in both point and probabilistic reconciliation. It is also worth to mention that the methods can be extended to non-Gaussian settings by bootstrapping [@Gamakumara2020-ml; @Panagiotelis2023-fs], which is beyond the scope of this paper.


## Shrinkage Estimator for MinT

The performance of MinT hinges on a reliable, positive-definite estimate of $\boldsymbol{W}_h$, which comes in both the mapping matrix $\boldsymbol{G}_h$ and the reconciled forecast variance $\boldsymbol{S} \boldsymbol{G}_h \boldsymbol{W}_h \boldsymbol{G}_h' \boldsymbol{S}'$.

However, the covariance matrix $\boldsymbol{W}_h$ is often not available in closed-form, and is challenging to estimate in high-dimensional setting where the number of series $n$ is larger than the time dimension $T$. To tackle this issue, the original paper @Wickramasuriya2019-xq assumed a proportionality relationship between $\hat{\boldsymbol{W}}^g_h = k_h g( \hat{\boldsymbol{W}}_1 )$, where $\hat{\boldsymbol{W}}_1$ is the covariance matrix of the in-sample 1-step-ahead base residuals (to approximate $\boldsymbol{W}_1$) and $k_h > 0$ is a scaling constant (which will be algebraically cancelled out in point-forecast reconciliation). The function $g(.)$ is a covariance estimator that produces a positive-definite matrix, the main focus of this paper.

The recommended choice for $g(.)$ in the original work is the shrinkage estimator with diagonal target from @Schafer2005-yw:

$$
\hat{\boldsymbol{W}}^{S}_{1} = \lambda_S \, \text{diag}( \hat{\boldsymbol{W}}_1 ) + (1 - \lambda_S) \hat{\boldsymbol{W}}_1 \, ,
$$

where $\text{diag}( \hat{\boldsymbol{W}}_1 )$ comprises only the diagonal elements of $\hat{\boldsymbol{W}}_1$. We refer to any $\lambda_S \in [0,1]$ as the shrinkage intensity of the shrinkage estimator. This approach shrinks the covariance matrix $\hat{\boldsymbol{W}}_1$ towards its diagonal matrix, meaning the off-diagonal elements are shrunk towards zero while the diagonal ones remain unchanged.

@Schafer2005-yw also provided an closed-form estimate of the optimal shrinkage intensity parameter $\lambda_S$:

$$
\hat{\lambda}_S = \frac{\sum_{i \neq j} \widehat{Var}(\hat{r}_{ij})} 
{\sum_{i \neq j} \hat{r}_{ij}^2} \, ,
$$

where $\hat{r}_{ij}$ is the $i,j$-th element of $\hat{\boldsymbol{R}}_1$, the 1-step-ahead sample correlation matrix (obtained from $\hat{\boldsymbol{W}}_1$). The optimal estimate is obtained by minimising $MSE(\hat{\boldsymbol{W}}_1) = Bias(\hat{\boldsymbol{W}}_1)^2 + Var(\hat{\boldsymbol{W}}_1)$. More specifically, we trade a small bias for a substantial variance reduction, which is especially valuable in high dimension.

Despite its simplicity and guaranteed positive-definiteness, MinT coupled with diagonal-target shrinkage presents three important limitations.

### Problem 1: Uniform shrinkage {.unnumbered}

Linear shrinkage shrinks all off-diagonal elements towards zeros with equal weights $\lambda_S$. The resulting penalty is global and non-adaptive: strong, genuinely systematic correlations are shrunk at the same rate as weak, noisy ones. In hierarchical and grouped systems, this can be problematic. Aggregation naturally induces stronger dependence among related nodes (for example, a region and a neighbouring region, or a region and its parent state), while many unrelated pairs exhibit near-zero correlations. A uniform penalty may over-shrink informative co-movements and under-shrink idiosyncratic noise, reducing reconciliation efficiency.

### Problem 2: Latent factors {.unnumbered}

Many real-world hierarchical data sets exhibit a prominent low-rank (factor) structure.

In Australian domestic tourism, for example, national and state-level movements often load on a small number of common components, with residual dependence concentrated in local idiosyncrasies. A scree plot of the one-step residual covariance typically shows a marked elbow after a handful of principal components, indicating strong latent structure. Diagonal-target shrinkage is factor-unaware: it neither preserves the low-rank common subspace nor differentially shrinks the idiosyncratic remainder, and so can be inefficient when common factors dominate.

Taking an example of the Australian domestic overnight trips data set [@tourism-au], where the national trips are disaggregated into states and territories, and further into regions. The tourism activities might be driven by a few common factors, such as economic conditions, fuel prices, or major events, which might be left in the residuals after fitting the base models. From @fig-eigen, a scree plot of the one-step residual covariance $\hat{\boldsymbol{W}}_1$ shows a marked elbow after a handful of principal components (largest eigenvalues), indicating strong latent structure.

![Twenty largest eigenvalues of one-step-ahead in-sample base forecast error covariance, Australian domestic overnight trips. The point of inflection occurs around the fifth largest eigenvalue.](D:/Github/Recon_Honours_Thesis/figs/eigen_tourism.png){#fig-eigen width="70%"}

### Problem 3: Proportional scaling for h-step-ahead {.unnumbered}

The proportionality relationship $\hat{\boldsymbol{W}}^g_h = k_h g( \hat{\boldsymbol{W}}_1 )$ enforces horizon-invariant cross-series dependence of the forecast errors up to a scalar factor. In many applications, the shape of the error covariance might change with horizon due to evolving error dynamics or horizon-specific interactions among series. Proportional scaling may therefore misrepresent multi-step dependence.

Additionally, while the scalar factor cancels in point reconciliation, it directly controls dispersion in probabilistic reconciliation and can lead to miscalibrated predictive distributions. However, we will not explore this issue in this paper, and leave it for future research.

In the next sections, we investigate covariance estimators that address these issues. We consider a estimator that mitigate uniform shrinkage, factor-aware adjustments that preserve low-rank structure while shrinking idiosyncratic noise, and horizon-specific estimators that relax proportional scaling.

# Covariance Estimation Approaches {#sec-cov-est}

## NOVELIST Estimator

The NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance) estimator, proposed by @Huang2019-ua, introduces a way to control the target matrix's sparsity, retaining strong correlations while discarding weak, noisy effects. NOVELIST offers more flexibility than the shrinkage estimator, which is useful when we believe that only a few variables are truly correlated.

The method is based on the idea of soft-thresholding the sample covariance matrix, then performing shrinkage towards this thresholded version. This introduces an extra parameter, the threshold $\delta$, which is used to control the amount of soft-thresholding. The NOVELIST estimator is given by:

$$
\hat{\boldsymbol{W}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{W}}_{1, \delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{W}}_1,
$$ {#eq-novelist-cov}

where $\hat{\boldsymbol{W}}_{1, \delta}$ is the thresholded version of $\hat{\boldsymbol{W}}_{1}$. By convenient setting, we can rewrite it in terms of correlation:

$$
\hat{\boldsymbol{R}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{R}}_{1,\delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{R}}_1,
$$ {#eq-novelist-cor}

where, $\hat{\boldsymbol{R}}_{1,\delta}$ is the thresholded correlation matrix, where each element is regularised by:

$$
\hat{r}_{1,ij}^\delta = \text{sign}(\hat{r}_{1,ij}) \, 
\text{max}(|\hat{r}_{1,ij}| - \delta, \; 0),
$$ {#eq-soft-thr}

where $\delta \in [0,1]$ is the threshold parameter. For a given threshold $\delta$, @Huang2019-ua derived an analytical expression for the optimal shrinkage intensity parameter $\lambda(\delta)$ using Ledoit-Wolf's lemma \[@Ledoit2003-qv\], following similar logic to @Schafer2005-yw. It can be computed as:

<!-- - \textcolor{red}{@Huang2019-ua mentioned Ledoit-Wolf's lemma} -->

$$
\hat{\lambda}(\delta) = \frac{
  \sum_{i \neq j} \widehat{Var}(\hat{r}_{1,ij}) \; \boldsymbol{1}(|\hat{r}_{1,ij}| \leq \delta)
} {
  \sum_{i \neq j} (\hat{r}_{1,ij} - \hat{r}_{1,ij}^\delta)^2
},
$$ {#eq-lambda-thr}

where $\boldsymbol{1}(.)$ is the indicator function.

On the other hand, the optimal threshold $\hat \delta$ does not have a closed-form solution, and is typically obtained by executing a rolling-window cross-validation procedure. The idea is to find the threshold $\delta^*$, with the corresponding $\lambda^*$ and $\hat{\boldsymbol{R}}^{N}_{1}(\delta^*, \lambda^*)$, that minimises the average out-of-sample 1-step-ahead reconciled forecast mean squared error over all windows. The formal algorithm is given in @sec-novelist_cv.

We also tested out minimising 1- to h-step-ahead overall MSE in the cross-validation procedure. Surprisingly, it returns almost the same best threshold parameter is in the 1-step-ahead case above. Note that when $\delta \in \bigl[ \text{max}_{i \neq j}|\hat{r}_{1,ij}|, \; 1 \bigr]$, the NOVELIST estimator collapses to the shrinkage estimator, and when $\delta = 0$, it becomes the sample covariance matrix. An additional concern is that the estimator does not guarantee to be positive definite, but we can use @Higham2002-te algorithm to compute the nearest positive definite matrix if needed.

### NOVELIST cross-validation algorithm {#sec-novelist_cv}

It is only required to fit the base models once on the whole training data $\{\boldsymbol{y}_t\}_{t=1}^T$, and obtain the in-sample fitted values $\{\hat{\boldsymbol{y}}_t\}_{t=1}^T$.

```{=latex}
\begin{algorithm}[H]
\caption{Cross-validation procedure}
\begin{algorithmic}[1]

\State \textbf{Input:} Observations and fitted values $\boldsymbol{y}_t, \hat{\boldsymbol{y}}_t \in \mathbb{R}^n$ for $t = 1,\dots,T$, set of threshold candidates $\Delta$, window size $v$.

\State $\hat{\boldsymbol{e}}_t = \boldsymbol{y}_t - \hat{\boldsymbol{y}}_t$ for $t = 1,\dots,T$

\For{$i = v:T-1$}
    \State $j = i - v +1$
    \State $\hat{\boldsymbol{W}}_j = \frac{1}{v} \sum_{t=j}^{i} \hat{\boldsymbol{e}}_{t} \hat{\boldsymbol{e}}_{t}'$
    \State $\hat{\boldsymbol{D}}_j = \text{diag}(\hat{\boldsymbol{W}}_j)$
    \State $\hat{\boldsymbol{R}}_j = \hat{\boldsymbol{D}}_j^{-1/2} \hat{\boldsymbol{W}}_j \hat{\boldsymbol{D}}_j^{-1/2}$
    \For{$\delta \in \Delta$}
        \State Compute thresholded correlation $\hat{\boldsymbol{R}}_{j,\delta}$ using Equation 5
        \State Compute $\hat{\lambda}_{j,\delta}$ using Equation 6
        \State Compute $\hat{\boldsymbol{R}}^{N}_{j,\delta}$ using Equation 4
        \State $\hat{\boldsymbol{W}}^{N}_{j,\delta} = \hat{\boldsymbol{D}}_j^{1/2} \hat{\boldsymbol{R}}^{N}_{j,\delta} \hat{\boldsymbol{D}}_j^{1/2}$
        \State $\boldsymbol{G} = (\boldsymbol{S}' \hat{\boldsymbol{W}}^{N^{-1}}_{j,\delta} \boldsymbol{S})^{-1} \boldsymbol{S}' \hat{\boldsymbol{W}}^{N^{-1}}_{j,\delta}$
        \State Reconciled forecasts $\tilde{\boldsymbol{y}}_{i+1 | \delta} = \boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_{i+1}$
        \State $\tilde{\boldsymbol{e}}_{i+1 | \delta} = \boldsymbol{y}_{i+1} - \tilde{\boldsymbol{y}}_{i+1 | \delta}$
    \EndFor
\EndFor

\State $\text{MSE}_{\delta} = \frac{1}{T-v} \sum_{i=v}^{T-1} (\tilde{\boldsymbol{e}}_{i+1 | \delta})^2$ for each $\delta \in \Delta$
\State $\hat{\delta}^* = \arg\min_{\delta \in \Delta} \text{MSE}_{\delta}$
\State Compute $\hat{\lambda}^*$ on all training data using $\hat{\delta}^*$
\State Compute $\hat{\boldsymbol{R}}_1^*$ using $\hat{\delta}^*$ and $\hat{\lambda}^*$ on all training data, using Equation 3

\State \textbf{Output:} Estimate of optimal $\hat{\delta}^*$

\end{algorithmic}
\end{algorithm}
```

<!-- The cross-validation algorithm for NOVELIST is available in the ReconCov package [@ReconCov]. -->

## PC-adjusted Estimator

To utilise the latent factors structure for better shrinkage, the PC-adjusted method takes the latent factors directly into its construction, and is appealing when there are common drivers in the time series within the hierarchy, as we saw in the Australian tourism example. It starts by decomposing the covariance matrix $\hat{\boldsymbol{W}}_1$ into a prominent principle components part (low-rank) and a orthogonal complement part $\hat{\boldsymbol{W}}^K_{1}$ (the correlation matrix after removing the first $K$ principal components). Then we can apply either shrinkage or NOVELIST estimator to $\hat{\boldsymbol{W}}^K_{1}$:

$$
\hat{\boldsymbol{W}}^{g, K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + g(\hat{\boldsymbol{W}}^K_{1})
$$

where $g(.)$ is either the shrinkage or NOVELIST estimator, $\hat{\gamma}_k$ and $\hat{\boldsymbol{\xi}}_k$ are the $k$-th largest eigenvalue and the corresponding eigenvector of the sample covariance matrix, respectively. Similar to the NOVELIST estimator, its PC-adjusted variant $\hat{\boldsymbol{W}}^{N, K}_{1}$ requires a cross-validation procedure and adjustment to obtain positive definiteness.
## Scaled Variance

To address the potential issue of the proportionality relationship $\hat{\boldsymbol{W}}^g_h = k_h g( \hat{\boldsymbol{W}}_1 )$ not holding in practice, we can relax the assumption by allowing the variances to scale differently. This is done by scaling the $1$-step-ahead correlation matrix back to $h$-step-ahead covariance matrix using the h-step-ahead standard deviations. The scaled variance estimator is given by:

$$
\hat{\boldsymbol{W}}^{g, sv}_{h} = \boldsymbol{D}^{1/2}_h g(\hat{\boldsymbol{R}}_1) \boldsymbol{D}^{1/2}_h,
$$

where $\boldsymbol{D}_h = \text{diag}(\hat{\sigma}^2_{1,h}, \ldots, \hat{\sigma}^2_{n,h})$, and $\hat{\sigma}^2_{i,h}$ is the variance of the $i$-th series' h-step-ahead base forecast errors. Similarly, $g(.)$ is either the shrinkage or NOVELIST estimator.

If NOVELIST is used to produce $h$-step-ahead reconciled forecasts, the cross-validation procedure is slightly modified to evaluate the out-of-sample reconciled forecast MSE using $\hat{\boldsymbol{W}}^{N, sv}_{h}$ instead of $\hat{\boldsymbol{W}}^{N}_{1}$.

## Constructing from h-step-ahead Residuals

Another alternative is not to rely on the assumption of proportionality at all, and directly estimate the covariance matrix from the h-step-ahead base forecast errors:

$$
\hat{\boldsymbol{W}}^{g}_{h} = g(\hat{\boldsymbol{W}}_h)
$$

where $\hat{\boldsymbol{W}}_h$ is the covariance matrix of in-sample h-step-ahead base forecast residuals, and $g(.)$ is either the shrinkage or NOVELIST estimator. Similar to the scaled variance approach, if NOVELIST is used, the cross-validation procedure is modified to compute $\hat{\boldsymbol{W}}^{N}_{h}$ and evaluate the out-of-sample reconciled forecast MSE using it.

## Summary of MinT with Covariance Estimators {-}

The covariance estimators explored in this paper are summarised in the table below. The abbreviations will be used in the following sections.

| Covariance estimators used              | Abbreviation       | 
|-----------------------------------------|--------------------|
| Shrinkage                               | MinT-S             |
| NOVELIST                                | MinT-N             |
| PC-adjusted Shrinkage with *K* PCs      | MinT-S(PC*K*)      |
| PC-adjusted NOVELIST with *K* PCs       | MinT-N(PC*K*)      |
| Scaled Variance Shrinkage               | MinT-S(sv)         |
| Scaled Variance NOVELIST                | MinT-N(sv)         |
| Constructed from h-step-ahead Shrinkage | MinT-S(hcov)       |
| Constructed from h-step-ahead NOVELIST  | MinT-N(hcov)       |

# Evaluation of Point and Probabilistic Forecasts {#sec-scores}

This section briefly introduces the scoring rules used to evaluate the point and probabilistic forecast accuracy in @sec-simulation and @sec-empirical.

For point forecasts, we use the mean squared error (MSE) to evaluate the accuracy of different reconciliation methods: $MSE = \frac{1}{n} \sum_{i=1}^n (y_{i, t+h} - \tilde{y}_{i,t+h|t})^2$, where $y_{i, t+h}$ is the realised value of series $i$ at time $t+h$, and $\tilde{y}_{i,t+h|t}$ is the reconciled point forecast.

Meanwhile, to assess the quality of the probabilistic forecasts, it is common to use proper scoring rules. A scoring rule is a function $S(., .)$ taking a predictive distribution as its first argument and a realisation as its second argument, then returns a numerical score. We follow a convention that lower scores are better. A scoring rule is said to be *proper* if $\mathbb{E}_Q[S(Q, y)] \leq \mathbb{E}_Q[S(F, y)]$ for all $F$, where $F$ is a predictive distribution produced by forecasting model, $Q$ is the true distribution of the realisation $y$, and $\mathbb{E}_Q$ is the expectation with respect to $Q$. Hence, the expected score is minimised when the forecast distribution matches the true distribution.

We employ Winkler score and continuous ranked probability score as our univariate scoring rules, and energy score as the multivariate scoring rule. All three are proper scoring rules. In this paper, we only evaluate $1$-step-ahead probabilistic forecasts, and thus we drop the subscript $t$ and $h$ for simplicity.

\textcolor{blue}{\textit{Winkler score (WS).}} If the 100$(1-\alpha)$\% prediction interval of $i$-th series is $[l_i, u_i]$ (the $\alpha / 2$ and $1 - \alpha / 2$ quantiles), then the Winkler score is defined as:

$$
WS_{\alpha}(l_i, u_i; y_i) = (u_i - l_i) + \frac{2}{\alpha} (l_i - y_i) \boldsymbol{1}(y_i < l_i) + \frac{2}{\alpha} (y_i - u_i) \boldsymbol{1}(y_i > u_i),
$$

where $y_i$ is the observed value of $i$-th series, and $\boldsymbol{1}(.)$ is the indicator function. The Winkler score rewards narrow intervals that contain the observation, and penalises intervals that do not contain the observation.

\textcolor{blue}{\textit{Continuous ranked probability score (CRPS).}} The CRPS is defined as the squared difference between the predictive cumulative distribution function (CDF) $F_i$ and the empirical CDF of the observation $y_i$ of series $i$:

$$
CRPS(F_i, y_i) = \int_{-\infty}^{\infty} (F_i(x) - \boldsymbol{1}(x \geq y_i))^2 dx.
$$

When the predictive distribution is Gaussian with mean $\mu_i$ and standard deviation $\sigma_i$, the CRPS has a closed-form expression:

$$
CRPS(F_i, y_i) = \sigma_i \left[ z_i \left( 2 \Phi(z_i) -1 \right) + 2 \phi(z_i) - \frac{1}{\sqrt{\pi}} \right],
$$

where $z_i = \frac{y_i - \mu_i}{\sigma_i}$, and $\Phi(.)$ and $\phi(.)$ are the CDF and probability density function (PDF) of a standard normal distribution, respectively.

\textcolor{blue}{\textit{Energy score (ES).}} The energy score is a multivariate generalisation of the CRPS. It is defined as:

$$
ES(F, \boldsymbol{y}) = \mathbb{E}_F ||\boldsymbol{X} - \boldsymbol{y}||^{\beta} - \frac{1}{2} \mathbb{E}_F ||\boldsymbol{X} - \boldsymbol{X}'||^{\beta},
$$

where $\boldsymbol{X}$ and $\boldsymbol{X}'$ are independent random vectors with multivariate distribution $F$, $\boldsymbol{y}$ is the observed vector, $||.||$ is the Euclidean norm, and $\beta \in (0, 2]$. We set $\beta = 1$ following common convention.

Since the closed-form expression of the ES may not available, we approximate it using Monte Carlo samples $\{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_M\}$ drawn from $P$:

$$
\widehat{ES}(F, \boldsymbol{y}) = \frac{1}{M} \sum_{m=1}^M ||\boldsymbol{x}_m - \boldsymbol{y}|| - \frac{1}{2M(M-1)} \sum_{m=1}^M ||\boldsymbol{x}_m - \boldsymbol{x}^*_m||,
$$

where $\boldsymbol{x}^*_m$ is a randomly selected sample from $\{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_M\} \setminus \{\boldsymbol{x}_m\}$. In our experiments, we use $M=10000$ samples to approximate the ES.



# Simulation {#sec-simulation}

## General Design

The general design of data generating process for bottom-level series is a stationary VAR(1) process, with the following structure:

$$
\boldsymbol{b}_t = \boldsymbol{A} \boldsymbol{b}_{t-1} + \boldsymbol{\epsilon}_t,
$$

where $\boldsymbol{A}$ is a $n_b \times n_b$ block diagonal matrix of autoregressive coefficients $\boldsymbol{A} = diag(\boldsymbol{A}_1, \ldots, \boldsymbol{A}_m)$, with each $\boldsymbol{A}_i$ being a $n_{b,i} \times n_{b,i}$ matrix. The block diagonal structure ensures that the time series are grouped into $m$ groups, with each group having its own autoregressive coefficients. This aim to simulate the interdependencies between the time series within each group, where reconciliation will be expected to better performed than the usual base forecasts.

The model is added with a Gaussian innovation process $\boldsymbol{\epsilon}_t$, with covariance matrix $\boldsymbol{\Sigma}$. The covariance matrix $\boldsymbol{\Sigma}$ is generated specifically using the Algorithm 1 in @Hardin2013-wu:

1.  A compound symmetric correlation matrix is used for each block of size $n_{b,i}$ in $\boldsymbol{A}_i$, where the entries $\rho_i$ for each block $i$ are sampled from a uniform distribution between 0 and 1. They are baseline correlations within group.

2.  A constant correlation, which is smaller than $\text{min} \{\rho_1, \rho_2, \dots, \rho_m \}$, is imposed on the entries between different blocks. It serves as baseline correlations between group.

3.  The entry-wise random noise is added on top of the entire correlation matrix.

4.  The covariance matrix $\boldsymbol{\Sigma}$ is then constructed by uniform sampling of standard deviations, in a range of $[\sqrt{2}, \sqrt{6}]$, for all $n_b$ series.

We will randomly flip the signs of the covariance elements, which will create a more realistic structure in the innovation process. This can be done by pre- and post-multiplying $\boldsymbol{\Sigma}$ by a random diagonal matrix $\boldsymbol{V}$ with diagonal entries sampled from $\{-1, 1\}$, yielding $\boldsymbol{\Sigma}^* = \boldsymbol{V} \boldsymbol{\Sigma} \boldsymbol{V}$.

For all hierarchies in our experiments, we simulate two panel lengths, $T=54$ and $T=304$, reserving the final four observations as an out-of-sample test set. In each Monte Carlo replication ($M=500$), we fit univariate ARIMA models (base models) to the training observations using an automatic AICc minimization algorithm from @Hyndman2008-rf, implemented in the *fabletools* package [@O-Hara-Wild2024-we], generating incoherent 1–4-step base forecasts. We then reconcile these forecasts under different methods, and evaluate their point and probabilistic forecast accuracy on the test set.


```{r load results}
path_sim <- "D:/Github/Recon_Honours_Analysis/sim/thesis_sim/"

sim1_1 <- readRDS("./thesis/lib/S4-2-1_T50_M500.rds")
sim1_2 <- readRDS("./thesis/lib/S4-2-1_T300_M500.rds")

sim2_1 <- readRDS("./thesis/lib/S36-6-1_T50_M500.rds")
sim2_2 <- readRDS("./thesis/lib/S36-6-1_T300_M500.rds")

sim_dense_50 <- readRDS("./thesis/lib/S100-10-3-1_T50_M200_dense.rds")
sim_dense_300 <- readRDS("./thesis/lib/S100-10-3-1_T300_M200_dense.rds")

sim_sparse_50 <- readRDS("./thesis/lib/S100-10-3-1_T50_M500_sparsebwgrp.rds")
sim_sparse_300 <- readRDS("./thesis/lib/S100-10-3-1_T300_M200_sparsebwgrp.rds")

sim3_1 <- sim_dense_50 # using same with next section simulation
sim3_2 <- sim_dense_300
```


## Exploring Effects of Hierarchy's Size

In our first set of experiments, we examine how MinT combined with the different estimators perform as the hierarchy expands. We generate synthetic data from the same VAR(1) framework described earlier, but vary the number of bottom‐level series, $n_b$, across two structures: a small structure with six groups of six bottom series ($n_b = 6$x$6 = 36$), and a much larger configuration with two groups of fifty ($n_b = 2$x$50 = 100$).

In the 36‐series case, each block of six forms a level‐1 aggregate, and those six aggregates form the national total. The 100‐series design employs a deliberately intricate aggregation path to stress‐test reconciliation methods. We first sum the one hundred bottom series into ten intermediate series by grouping them in contiguous blocks of ten. These ten series are then organised into three level-2 aggregates—four, three, and four series, respectively—before finally summing to a single top node. This asymmetric hierarchy creates overlapping correlation patterns: some level‐2 series share bottom‐level groups, while others draw from both, emulating practical scenarios such as regional sales aggregations that span multiple product categories or overlapping territories. The aggregation paths for both structures are illustrated in @fig-sim-structures.

![Aggregation structures used in the simulation experiments: 6 groups of 6 (left) and 2 groups of 50 (right)](D:/Github/Recon_Honours_Thesis/figs/sim_structures.png){#fig-sim-structures}

The VAR(1) and correlation configurations for the 6 by 6 case and 2 by 50 case are illustrated in @fig-settings-6x6 and @fig-settings-2x50, respectively. The block diagonal structure of the VAR(1) coefficient matrices $\boldsymbol{A}$ reflects the grouping of series, and the correlation matrices- show higher correlations among series within the same group.

```{r generate plots for A and Sigma}
MSE1_1 <- transform_sim_MSE(sim1_1$MSE, F) |> 
  filter(.model != "ols")
MSE1_2 <- transform_sim_MSE(sim1_2$MSE, F) |> 
  filter(.model != "ols")

MSE2_1 <- transform_sim_MSE(sim2_1$MSE, F) |> 
  filter(! .model %in% c("mint_n_hstep", "ols"))
MSE2_2 <- transform_sim_MSE(sim2_2$MSE, F) |> 
  filter(! .model %in% c("mint_n_hstep", "ols"))

MSE3_1 <- transform_sim_MSE(sim3_1$MSE, F)
MSE3_2 <- transform_sim_MSE(sim3_2$MSE, F) |> 
  filter(.model != "ols")

p1_A <- plot_heatmap(sim1_1$A, T) + 
  ggtitle("2x2 VAR(1) Coefficients Matrix")

p1_Sigma <- plot_heatmap(cov2cor(sim1_1$Sigma), T) + 
  ggtitle("2x2 Correlation Matrix of Innovation process")

p2_A <- plot_heatmap(sim2_1$A) +
  ggtitle("VAR(1) Coefficients Matrix") + theme_void()

p2_Sigma <- plot_heatmap(cov2cor(sim2_1$Sigma), T) +
  ggtitle("Correlation Matrix of Innovation process") + theme_void()

p3_A <- plot_heatmap(sim3_1$A) +
  ggtitle("VAR(1) Coefficients Matrix")

p3_Sigma <- plot_heatmap(cov2cor(sim3_1$Sigma), T) +
  ggtitle("Correlation Matrix of Innovation process")
```

```{r settings 6x6}
#| label: fig-settings-6x6
#| fig-cap: "The VAR(1) coefficients matrix (left) and correlation matrix of the innovation process (right) for the 6 groups of 6 structure."
#| fig.height: 5
#| fig.width: 7
#| out.width: "75%"
#| fig.align: "center"

p2_A + p2_Sigma + plot_layout(ncol = 2) & theme(legend.position = "bottom")
```

```{r settings 2x50}
#| label: fig-settings-2x50
#| fig-cap: "The VAR(1) coefficients matrix (top) and correlation matrix of the innovation process (bottom) for the 2 groups of 50 structure."
#| fig.height: 12
#| fig.width: 8
#| out.width: "100%"
#| fig.align: "center"

p3_A + p3_Sigma + plot_layout(ncol = 1) & theme(legend.position = "right")
```


![Percentage relative improvement in MSE of reconciled forecasts over the base forecasts in the 6 by 6 case (top row) and the 2 by 50 case (bottom row), T=50 (left column) and T=300 (right column), for 1- to 4-step-ahead forecasts. The positive (negative) entries indicate a decrease (increase) in MSE relative to base.](D:/Github/Recon_Honours_Thesis/figs/sim_MSE.png){#fig-sim-results-1}

Figure @fig-sim-results-1 illustrates the relative improvements in mean squared error (MSE) of reconciled forecasts over the incoherent base forecasts, across two structures and time series lengths. The MinT with shrinkage (*MinT-S*) and its variants are colored in mint green, while MinT with NOVELIST (*MinT-N*) are in purple. The concrete lines represent the vanilla *MinT-S* and *MinT-N*; the dashed lines with dot points denote the PC-adjusted variants (e.g. *MinT-S(PC1)*, *MinT-N(PC2)*); and the dotted or dashed-dotted lines indicate the scaled variance and h-step-ahead residuals versions (e.g. *MinT-S(SV)*, *MinT-N(hcov)*).

The first key observation is that methods with shrinkage slightly outperform those with NOVELIST across all scenarios, despite the differences being small. Second, the PC-adjusted variants (using one and two principal components) do not yield improvements over the vanilla versions. This is expected since the synthetic data generating process does not simulate from strong latent factors. Lastly, the scaled variance and h-step-ahead residuals approaches do not enhance performance as the forecast horizon increases, suggesting that the proportionality assumption may not be severely violated in this VAR(1) setup. 

<!-- Lastly, as the hierarchy expands from 36 to 100 series, forecasts from NOVELIST methods become worse relative to base forecasts when we move to $2$-step-ahead and beyond.  -->

When looking into each MCMC replication, we find that there are instances where the NOVELIST estimator collapses to the shrinkage estimator due to a large optimal threshold $\hat{\delta}$ being selected in the cross-validation step, resulting in a diagonal shrinkage target.

![Percentage relative improvement in Energy score in both hierarchies and time dimensions, for 1-step-ahead forecasts. The positive entries indicate a decrease in Energy score relative to base.](D:/Github/Recon_Honours_Thesis/figs/sim_energy.png){#fig-sim-results-2}

Moving on to probabilistic forecasts, we evaluate the performance of reconciliation methods using the energy score, a proper scoring rule for multivariate predictive distributions. Figure @fig-sim-results-2 presents the percentage relative improvement in energy score for 1-step-ahead forecasts. Across both hierarchies and time dimensions, the MinT methods consistently outperform the base forecasts. Meanwhile, the differences among *MinT-S* and *MinT-N* variants are insignificant, except for the 6 by 6 case with 50 observations, where shrinkage has a slight edge. The PC-adjusted variants again degrade performance as we add more principal components. The scaled variance and h-step-ahead residuals approaches are not available since we only evaluate 1-step-ahead covariance estimates.

Other scoring rules, such as the Winkler score and CRPS, yield similar conclusions.


## Other Data Generating Processes

In attempts to differentiate the performance of NOVELIST from the shrinkage estimator, we also simulate from a sparse covariance matrix for a 2 groups of 50 scenario, as illustrated in @fig-Sigma-2x50. The sparse covariance matrix is obtained by randomly choosing 40% of the bottom series and setting their correlations with all other series to zero, resulting in a grid-like sparse structure. The VAR(1) coefficient matrix remains the same as in the dense case. The idea is to allow NOVELIST to exploit the sparsity in the covariance structure, since it can control the sparsity of the shrinkage target via the thresholding parameter $\delta$. However, no profound insights can be drawn from the results.

```{r}
#| label: fig-Sigma-2x50
#| fig-cap: "Sparse correlation matrix of the innovation process for 2 groups of 50 structure."
#| fig.height: 6
#| fig.width: 8
#| out.width: "80%"
#| fig.align: "center"

sim_sparse_50 <- readRDS(paste0(path, "/thesis/lib/params_S100_10_3_1_sparse46.rds"))
sim_sparse_50$Sigma |>
  cov2cor() |> 
  plot_heatmap(T)
```

Additional designs (varying block sizes, grouped structure, aggregation paths, correlation configurations) also failed to separate NOVELIST from Shrinkage. Their nearly identical performance under these synthetic scenarios suggests that our current simulation may not unveil the full advantages of the thresholding estimators. Nevertheless, we have not explored settings where PC variants or using h-step-ahead residuals approaches would have an edge.

These findings motivates our turn to empirical data in the next section, where latent structural features, regime shifts, and noisy, intermittent series will reveal performance differences. 

# Forecasting Australian Domestics Tourism {#sec-empirical}

```{r}
tourism_plots <- readRDS("D:/Github/Recon_Honours_Thesis/thesis/tourism_plots.rds")
```

Domestic tourism flows in Australia exhibit a natural hierarchical and grouped structure, driven both by geography and by purpose of travel. At the top of this hierarchy lies the national total, which splits into the seven states and territories. Each state is further sub-divided into tourism zones, which in turn break down into 77 regions. A complete illustration of this geographic hierarchy appears in Appendix @sec-tourism-supplement. Intersecting this geographic hierarchy is a second dimension--travel motive--partitioning tourism flows into four categories: holiday, business, visiting friends and relatives, and other. Altogether, this yields a grouped system of 560 series, from the most disaggregated regional-purpose cells up to the full national aggregate. @tbl-tourism-structure depicts this structure.

```{r summary structure tourism}
#| tbl-cap: "Hierarchical and grouped structure of Australian domestic tourism flows"
#| label: tbl-tourism-structure

num_series_geo <- c(1, 7, 27, 77)
table_structure <- tibble(
  `Geographical division` = c("Australia", "States", "Zones", "Regions"),
  `Number of series per geographical division` = num_series_geo,
  `Number of series per purpose` = 4*num_series_geo,
  `Total number of series` = 5*num_series_geo
)
# add total per row
table_structure <- rbind(
  table_structure,
  c("Total", colSums(table_structure[,-1]))
)

# width margin smaller (90%)
table_structure |> 
  knitr::kable(
    align = "lccc",
    booktabs = TRUE
  )
```

We quantify tourism demand via "visitor nights", the total number of nights spent by Australians away from home. The data is collected via the National Visitor Survey, managed by Tourism Research Australia, using computer assisted telephone interviews from nearly 120,000 Australian residents aged 15 years and over [@tourism-au].

The data are monthly time series spanning from January 1998 to December 2016, resulting in 228 observations per series, producing a canonical "$n \ll p$" setting which is ideal for evaluating reconciliation approaches that rely on high-dimensional covariance estimation. The extreme dimensionality over sample size mirrors many contemporary business problems, for instance, Starbucks drink sales. Tourism demand is also economically vital yet highly volatile, with geographical and purpose‑specific patterns create a realistic stress‑test for reconciliation algorithms.

<!-- Consequently, this panel offers both a rich policy case study and a stringent statistical laboratory for comparing reconciliation strategies that exploit cross-series information to improve forecasts when historical data are scarce. -->

<!-- This dataset is particularly well‑suited to evaluating forecast‑reconciliation methods for three reasons. First, tourism demand is economically important and notoriously volatile; the heterogeneous regional and purpose‑specific patterns create a realistic stress‑test for reconciliation algorithms. Second, the clear structural shift towards stronger growth after 2016—visible in most aggregates—necessitates techniques capable of handling possible breaks or time‑varying error structures, making the exercise practically relevant. Third, the extreme dimensionality relative to sample size mirrors many contemporary business problems (e.g., SKU‑level retail sales), allowing insights to generalise beyond tourism. Using this panel therefore provides both a compelling policy context and a stringent statistical laboratory for comparing alternative reconciliation strategies, especially those that rely on regularised covariance estimation or exploit cross‑sectional information to stabilise forecasts when historical data are scant. -->

@Wickramasuriya2019-xq also argued that modelling spatial autocorrelations directly from the start would be challenging as in this case of a large collection of time series. Post-processing reconciliation approaches have the advantage to implicitly model this spatial autocorrelation structure, especially true for MinT.

![Rolling-window cross-validation scheme for evaluating forecasting performance in Australia tourism data](D:/Github/Recon_Honours_Thesis/figs/tourism_cv.png){#fig-tourism-cv width="50%"}

To assess forecasting performance between models, we adopt a rolling-window cross-validation scheme. Beginning with the first 120 monthly observations (January 1998-December 2005) as the initial training set, we obtain the best-fitted ARIMA model for each of the 560 series via the automatic algorithm by minimising AICc from @Hyndman2008-rf, implemented in the *fabletools* package [@O-Hara-Wild2024-we]. The 1- to 12-step-ahead base forecasts are then generated by these ARIMA models, and then reconciled using multiple approaches. To estimate the NOVELIST and its variants, we would have an extra cross-validation prodecure within this training window, as described in @sec-novelist_cv. We then roll the training window forward by one month and refit all models, rebuild reconciliations, and produce another batch of 1- to 12-step-ahead forecasts, repeating until the training set reaches December 2015. In total, this results in 97 out-of-sample windows. The entire procedure is illustrated in @fig-tourism-cv.


<!-- 
Should MinT-sample be included in the paper?

Note that the number of series is larger than the number of observations (560 compared to 96), hence the sample covariance matrix is not positive definite and will not be considered.
-->


```{r tourism-MSE-line}
#| label: fig-tourism-MSE-line
#| fig.height: 6
#| fig.width: 8
#| fig-cap: "Percentage relative improvement in the mean squared error (MSE) of different reconciled forecasts over the base forecasts for the Australian domestic tourism data, for 1 to 12 steps ahead forecasts. The positive entries indicate an decrease in MSE."

tourism_MSE_results <- readRDS(paste0(path, "/thesis/lib/tourism_MSE_results.rds"))

tourism_MSE_results |> 
  mutate(shape_lab = ifelse(shape_lab %in% c("PC1","PC2"), shape_lab, "")) |>
  ggplot(aes(x = h, y = pct_change, color = family, linetype = variant_type, shape = shape_lab)) +
  geom_line(linewidth = 0.8) +
  geom_point(alpha = 0.85, size = 2.6, na.rm = TRUE) +
  recon_scales +
  labs(x = "Horizon", y = "% improvements") +
  # scale_y_continuous(limits = c(-10, 0.2)) +
  theme_minimal() + 
  theme(
    legend.position = "bottom", 
    legend.direction = "vertical", 
    legend.title.position = "left"
  ) + 
  scale_x_continuous(breaks = c(1:12), minor_breaks = NULL)
```

```{r}
#| label: fig-tourism-MSE-heat
#| fig.height: 5
#| fig.width: 7
#| out.width: "90%"
#| fig-cap: "Heatmap of relative improvement in the mean squared error (MSE) of different reconciled forecasts over the base forecasts for the Australian domestic tourism data, for 1 to 12 steps ahead forecasts. The values are scaled to the range of 0 to 100 for better visualisation, with darker colors indicating greater improvement and best performance is noted by a star."

tourism_MSE_results |> 
  # best models per h
  filter(.model != "base") |> 
  group_by(h) |>
  mutate(
    is_best = pct_change == max(pct_change),
    # rescale to (0 to -1)
    scaled_pct_change = scales::rescale(pct_change, to = c(0, 100))
  ) |>
  filter(.model != "base") |> 
  ggplot(aes(
    x = h, y = fct_rev(model_label), 
    # fill = pct_change
    fill = scaled_pct_change
  )) +
    geom_tile() +
    geom_text(aes(label = ifelse(is_best, "*", "")), color = "black", size = 8) +
    scale_fill_gradient2(high = "#ca2b33ff", mid = "#f7ffadff", midpoint = 0) +
    labs(x = "Horizon", y = "Model") +
    theme_minimal() + 
    theme(legend.title = element_blank(), legend.position = "bottom") + 
    scale_x_continuous(breaks = c(1:12), minor_breaks = NULL)
```

<!-- @fig-tourism-MSE-line and @fig-tourism-MSE-heat show that reconciliation is beneficial for point forecasts: across horizons $h=1,2,\dots,12$, all reconciled methods improve MSE over incoherent base ARIMA on average. Among vanilla MinT variants, the shrinkage estimator (*MinT-S*) marginally outperforms NOVELIST (*MinT-N*) at most horizons, though the gap is small. Adjusting for a single dominant factor via PC decomposition tightens performance further: both *MinT-S(PC1)* and *MinT-N(PC1)* beat their unadjusted counterparts, with *MinT-N(PC1)* narrowly ahead. Adding more than one PC brings no additional benefit, likely due to injecting estimation noise from weaker components. Variants that modify the multi-step covariance, either via scaled-variance or direct h-step residual covariances, underperform standard MinT, suggesting that extra estimation at horizon $h>1$ is not rewarded in this setting. -->


@fig-tourism-MSE-line show a main difference compared to the one of simulation results. Adjusting for a single dominant factor via PC decomposition (dashed line with dot points) tightens performance further: both *MinT-S(PC1)* and *MinT-N(PC1)* beat their unadjusted counterparts. Other than that, similar patterns are observed: among vanilla MinT variants, the shrinkage estimator (*MinT-S*) slightly outperforms NOVELIST (*MinT-N*) at most horizons; adding more than one PC brings no additional benefit, likely due to injecting estimation noise from weaker components; variants that modify the multi-step covariance, either via scaled-variance or direct h-step residual covariances, underperform standard MinT, suggesting that extra estimation at horizon $h>1$ is not rewarded in this empirical analysis. 

@fig-tourism-MSE-heat provides a complementary heatmap view, scaling each method's MSE improvement to a 0-100 range for better visual discrimination. Here, *MinT-N(PC1)* and *MinT-S(PC1)* emerge as the top performers across most horizons. The heatmap underscores the consistent gains from PC adjustment and highlights the diminishing returns from more complex covariance treatments.


```{r tourism-probscores}
#| label: fig-tourism-probscores
#| fig.height: 10
#| fig.width: 10
#| fig-cap: "Percentage relative improvement in the Winkler score at 80% and 95% nominal coverage, CRPS, and Energy score of multiple reconciled forecasts over the base forecasts for the Australian domestic tourism data, for 1-step-ahead forecasts. The positive (negative) entries indicate a decrease (increase) in the probabilistic scores relative to base."

# a 2x2 showing 4 plots showing the 1-step-ahead % improv in Winkler score 80, 95, CRPS, and Energy

tourism_uniscores <- readRDS(paste0(path, "./thesis/lib/tourism_uniscore_results.rds"))
tourism_energy <- readRDS(paste0(path, "./thesis/lib/tourism_energyscore_results.rds"))

plot_uniscore <- function(score_label, title) {
  tourism_uniscores %>%
    filter(rule == score_label) %>%
    group_by(family, model_label, pc_k, iter) %>%
    summarise(
      score = mean(score),
      base_score = mean(base_score),
      pct_change = (base_score - score) / base_score * 100, # positive indicate decrease in score
      .groups = "drop_last"
    ) %>%
    ungroup() %>%
    group_by(family, model_label, pc_k) %>%
    summarise(
      pct_change_avg = pmin(mean(pct_change), 10),   # cap at 10
      capped = as.integer(pct_change_avg >= 10),
      .groups = "drop"
    ) %>%
    filter(model_label != "Base", model_label != "OLS-S") %>%
    mutate(pc_k = ifelse(is.na(pc_k), "0", as.character(pc_k))) %>%
    # ggplot
    ggplot(aes(x = model_label, y = pct_change_avg,
              fill = family, alpha = pc_k)) +
      geom_col(color = "black", size = 0.5) +
      # geom_text(aes(label = ifelse(capped == 1, "> 10%", "")), vjust = -0.5) +
      recon_fill_scale +
      scale_alpha_manual(values = c("0" = 1, "1" = 0.6, "2" = 0.3), guide = "none") +
      labs(
        x = " ", y = " ",
        title = title
      ) +
      theme_minimal(base_size = 12) +
      theme(axis.text.x = element_text(angle = 25, hjust = 0.6)) +
      guides(fill = "none")
}

plot_W80 <- plot_uniscore("W80", "Winkler score 80%")
plot_W95 <- plot_uniscore("W95", "Winkler score 95%")
plot_CRPS <- plot_uniscore("CRPS", "CRPS")

plot_energy <- tourism_energy |> 
  group_by(family, model_label, pc_k) |> 
  summarise(
    pct_change_avg = mean(pct_change_per_series)
  ) |>
  mutate(pc_k = ifelse(is.na(pc_k), "0", as.character(pc_k))) |> 
  filter(! grepl("Ba|OLS-S", model_label)) |>
  ggplot(aes(x = model_label, y = pct_change_avg, fill = family, alpha = pc_k)) +
    geom_col(color = "black", size = 0.5) +
    recon_fill_scale +
    scale_alpha_manual(values = c("0" = 1, "1" = 0.6, "2" = 0.3), guide = "none") +
    labs(x = " ", y = " ", title = "Energy Score") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 25, hjust = 0.6))

p <- plot_W80 + plot_W95 + plot_CRPS + plot_energy +
  plot_layout(ncol = 2, guides = "collect", axis_titles = "collect") & 
  theme(legend.position = "bottom", legend.title = element_blank())

cowplot::ggdraw(p) +
  cowplot::draw_label("% improvements", angle = 90, x = 0.02, y = 0.5, size = 16)

# tourism_plots$W80 + labs(title = "Winkler score 80%", subtitle = NULL) + 
#   tourism_plots$W95 + labs(title = "Winkler score 95%", subtitle = NULL) +
#   tourism_plots$CRPS + labs(title = "CRPS", subtitle = NULL) +
#   tourism_plots$Energy + labs(title = "Energy score", subtitle = NULL) +
#   plot_layout(ncol = 2, guides = "collect", axis_titles = "collect") & 
#   theme(legend.position = "bottom", legend.title = element_blank())
```

<!-- ![Percentage relative improvement in the Winkler score at 80% and 95% nominal coverage, CRPS, and Energy score of multiple reconciled forecasts over the base forecasts for the Australian domestic tourism data, for 1-step-ahead forecasts. The positive (negative) entries indicate a decrease (increase) in the probabilistic scores relative to base.](D:/Github/Recon_Honours_Thesis/figs/tourism_probscores.png){#fig-tourism-probscores} -->

<!-- Turning to probabilistic forecasts (1-step-ahead forecasts), @fig-tourism-probscores shows that *MinT-N* consistently outperforms *MinT-S* across CRPS and both Winkler scores, and PC1-adjustment again yields the best improvements. Results are aligned across probabilistic scoring rules, but the 95% Winkler reveals a notable pattern, as all MinT variants lose to the base while *OLS* performs best. In the multivariate evaluation, the Energy score places *OLS* close to the PC1-adjusted MinT methods, indicating that simple coherent projection might be able to capture a substantial share of cross-series dependence benefits even without sophisticated covariance regularisation. -->

Turning to probabilistic forecasts (1-step-ahead forecasts), @fig-tourism-probscores shows that *MinT-N* (purple bars) consistently outperforms *MinT-S* (green bars) across univariate and multivariate scores. The PC1-adjusted variants again yield improvements over their vanilla counterparts, with *MinT-N(PC1)* leading overall. In the multivariate evaluation, the Energy score places *OLS* close to the PC1-adjusted MinT methods. One surprising finding is that all MinT variants underperform the base forecasts when moving to the 95% Winkler score, while *OLS* performs best.

![Percentage relative improvement in Winkler score at 95% nominal coverage by level (left), and empirical coverage of 95% prediction intervals by level (right)](D:/Github/Recon_Honours_Thesis/figs/tourism_coverage_95.png){#fig-tourism-coverage-95-level}

To dissect the 95% Winkler score results, @fig-tourism-coverage-95-level breaks down performance by hierarchical level, and examines empirical coverage of the 95% prediction intervals. The left panel shows that *MinT-S* underperforms the base at all levels, especially in higher aggregated levels. From our inspection, this is due to the overly shrunk variances from the shrinkage estimator, leading to narrow prediction intervals and thus high Winkler penalties when observations fall outside. The *MinT-N* method has relatively good coverage and improve the Winkler score at bottom levels, but still underperforms at higher levels. The PC-adjusted variants seem to strike a better balance, improving overall coverage and relative Winkler scores over the base.

```{r}
#| label: fig-tourism-radar
#| fig.height: 7
#| fig.width: 7
#| out.width: "70%"
#| fig-cap: "Radar plot of relative improvements in probabilistic scores (Winkler score at 80% and 95% intervals, CRPS, and Energy) over the base forecasts. The scores are scaled to a range of 0 to 100, with larger values indicating better performance. The outermost polygon represents the best possible score (100) and the innermost polygon represents the worst possible score (0). Only the top 4 MinT approaches are shown (with the base model)."

# a radar plot of 5 top models based on probabilistic scores (Winkler 80, 95, CRPS, Energy) scaled to 0-100 
par(mfrow=c(1,1), mar=c(1,2,1,2))
plot_radar(tourism_uniscores, tourism_energy)
```

The summary radar graph in @fig-tourism-radar consolidates these findings: among the selected MinT models by probabilistic criteria, *MinT-N(PC1)* (the yellow polygon) clearly leads across CRPS, Winkler score at 80% and 95% intervals, and Energy, extending the improvements seen in the single-metric panels. Except for the Winkler score at 95% interval, all MinT variants outperform the base forecasts.

Taken together, the evidence supports the use of reconciliation for both point and probabilistic forecasts in this high-dimensional setting. For point forecasts, MinT with shrinkage is a solid default; for probabilistic forecasts, NOVELIST performs more reliably. PC adjustment with a single dominant factor consistently enhances performance in both point and probabilistic forecast and should be considered when a dominant latent factor is evident. More complex adjustments, such as multiple PCs or horizon-specific covariances, add estimation noise without clear benefits and can be omitted for parsimony.

<!-- # Supplementary -->

# Conclusions and Future Work {#sec-conclusion}

<!-- 
- Aim to study the shortcomings of current MinT, a method attracting lots of attention in academia and practice.
- Propose a NOVELIST, PC-adjusted variants, and multi-step covariance variants.
- Empirical results on tourism data show that:
  - MinT with shrinkage is a solid default for point forecasts. NOVELIST performs more reliably for probabilistic forecasts.
  - PC adjustment with a single dominant factor consistently enhances performance in both point and probabilistic forecast and should be considered when a dominant latent factor is evident.
  - Proportionality assumption still gives reasonably good results.
  - More complex adjustments, such as multiple PCs or horizon-specific covariances, add estimation noise without clear benefits and can be omitted for parsimony.
- Limitations & Future works:
  - More complex simulation studies needed to tease out the differences between shrinkage and NOVELIST. No simulating from factor models.
  - Not standardising the data for PC-variants methods, higher-level series can dominate the PCs.
  - Explore other methods utilising cross-series information, eigenvalue/eigenvector structures,..
  - Explore why MinT shrinkage fails for Winkler 95% score, especially at higher levels.
  - h-step-ahead probabilistic forecast evaluation.
-->

This paper aims to address the limitations of the current Minimum Trace reconciliation method, a method that has garnered significant attention in both academic and practical forecasting contexts. We propose the NOVELIST estimator to address the lack of flexibility in the uniform shrinkage target of the traditional shrinkage estimator. Additionally, we introduce PC-adjusted variants of shrinkage and NOVELIST to take into account dominant latent factors, as well as multi-step covariance variants that relax the proportionality assumption.

Empirical results on Australian domestic tourism data reveal several insights. For point forecasts, shrinkage (and its PC-adjusted version) remains a robust default choice, while NOVELIST demonstrates improved performance for probabilistic forecasts. Adjusting for a single dominant principal component consistently enhances performance across both point and probabilistic forecasts, suggesting its utility when such latent structures are present. More complex adjustments, such as incorporating multiple principal components or horizon-specific covariances, tend to introduce additional estimation noise without notable benefits.

However, several limitations and avenues for future research remain. More intricate simulation studies are necessary to further highlight the differences between shrinkage and NOVELIST, particularly in scenarios involving factor models. The current implementation does not standardise data for PC-variant methods, which may lead to higher-level series largely influencing the principal components. Future work could also explore alternative methods that leverage cross-series information or eigenvalue/eigenvector structures. Additionally, understanding why MinT with shrinkage underperforms in the Winkler 95% score, especially at higher aggregation levels, needs further investigation. Finally, evaluating h-step-ahead probabilistic forecasts remains an open area for further research.

All data generation, covariance estimation, and reconciliation routines were implemented in the ReconCov R package and is available under an open‐source license on GitHub [@ReconCov].

\pagebreak

# Appendix {#sec-appendix}

## Appendix: Simulation Supplementary {#sec-sim-supplement}


## Appendix: Australian Domestic Tourism Geographical Hierarchy {#sec-tourism-supplement}

```{r}
#| tbl-cap: "Geographical divisions of Australia."
#| label: tbl-tourism-geo
#| warning: false
#| message: false

table_geo_labeled <- create_table_geo()
table_geo_labeled[1,1] <- "Total"

df <- table_geo_labeled %>% 
  select(Series, Name, Label)

n  <- ceiling(nrow(df) / 2)
L  <- df[1:n, ]
R  <- df[(n+1):nrow(df), ]

# pad the shorter side
if (nrow(R) < nrow(L)) {
  R <- bind_rows(R, tibble(Series = rep(NA_integer_, n - nrow(R)),
                           Name = "", Label = ""))
}

wide <- bind_cols(
  setNames(L, c("Series","Name","Label")),
  setNames(R, c("Series","Name","Label"))
)

kable(
  wide,
  align     = c("r","l","l","r","l","l"),
  col.names = c("Series","Name","Label","Series","Name","Label")
) |> 
  kable_styling(latex_options = "hold_position", font_size = 8)
```

\pagebreak

# References
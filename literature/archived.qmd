# Old literature review (archived)

<!--
-   **Problem Statement:**

    The sample covariance matrix, although natural, suffers in high-dimensional settings. Especially when the number of series $p$ is huge and larger than the time dimension $T$, the sample covariance matrix is non-positive definite (rank T if p\>T).

    The shrinkage estimators come in to tackle this issue. The shrinkage estimator with diagonal target (often shrinking toward a diagonal matrix) is proven to produce a guaranteed PD matrix (Schäfer & Strimmer, 2005). However, as it shrinks the covariance matrix toward a diagonal one, it does not have flexibility and might neglect the prominent structure presented in the covariance matrix.

    An alternative approach is to perform shrinkage of the sample covariance towards its thresholded version, instead of a diagonal matrix. This is the NOVELIST (NOVEL Integration of the Sample and Thresholded covariance estimators) method proposed by Huang & Fryzlewicz (2019). They introduced thresholding functions applied only to off-diagonal elements, allowing for more flexibility in the estimation.

    [... can include more estimators ...]{style="color:red;"}

-   **Research Aim:**

    This paper assesses the reconciled forecasting performance of MinT approach using various covariance estimators, with a focus on the NOVELIST estimator.

-   **Paper Outline:**

    The paper is structured as follows:

    -   A literature review of forecast reconciliation and covariance estimation.
    -   A description of the methodology, including the NOVELIST estimator and its principal-component-adjusted variant.
    -   An experimental design using both synthetic and real hierarchical time series.
    -   Empirical results and discussion.
    -   Conclusions and suggestions for future work.
-->

<!--
# Literature Review

## Forecast Reconciliation in Hierarchical and Grouped Time Series

Forecast reconciliation converts a collection of independent base forecasts into a set of coherent forecasts that respect the linear constraints defining a hierarchical or grouped time-series system. Early work focused on heuristic single-level strategies, including bottom-up, top-down, and middle-out (...), each of which exploits only part of the information in the hierarchy and can induce bias or high variance.

-   Cite the single level approach
-   Athanasopoulos et al., 2024

Hyndman et al. (2011) first showed that all single-level methods can be written as $\tilde{y} = SG\hat{y}$, where $S$ is the summing matrix and $G$ is a matrix that maps base forecasts $\hat{y}$ to into the bottom level. Treating reconciliation as a GLS regression problem, Hyndman et al. (2011) found that it yields a solution for $G$, but the required covariance matrix of reconcilation error is not identifiable in practice (Wickramasuriya et al., 2019).

-   (Talk more about how others transform it to OLS, WLS,..)
-   Di Fonzo and Marini (2011)
-   Athanasopoulos et al. (2009)

Wickramasuriya, Athanasopoulos & Hyndman (2019) reframed the problem by taking an optimisation approach rather than the regression. They formulated the problem as minimising the variances of all reconciled forecasts, which happens to be equivalent to minimising the trace of the covariance matrix (sum of the diagonal elements). This is known as the Minimum Trace (MinT) reconciliation method. The MinT solution is given by $G_h = (S'W_h^{-1}S)^{-1}S'W_h^{-1}$, and $W_h$ is the covariance matrix of the h-step-ahead base forecast errors.

The MinT approach is an algebraical generalisation of the GLS, and the OLS and WLS methods are special cases of MinT when $W_h$ is a diagonal or identity matrix, respectively. However, the MinT solution hinges on a reliable estimate of the h-step-ahead base forecast error covariance $W_h$. In high-dimensional setting, the usual sample covariance matrix is unstable, thus we need alternative covariance estimators.

-   Structural Scaling, based only on the struc- ture of the hierarchy (Athanasopoulos et al., 2017)
-   Shrinkage estimators (Schäfer & Strimmer, 2005; Ledoit & Wolf, 2004)

Empirical evaluations have demonstrated that MinT with an appropriate covariance estimate often outperforms earlier methods in both simulation and real data studies.

Wickramasuriya et al. (2019) adopt the diagonal-target Ledoit–Wolf approach and show analytically that any scalar multiple $k_h$ from $W_h = k_hW_1$ suffices for point-forecast reconciliation.  However, simulation and empirical evidence suggest that ignoring cross-series error correlation can leave accuracy untapped, especially in highly-coupled systems (energy, retail SKUs).  Their subsequent work imposes non-negativity constraints via quadratic programming citeturn0file2 and studies bias-robust variants (MinT-U).

Empirical findings to date

Athanasopoulos et al. (2024) synthesise the accumulated evidence: MinT dominates single-level baselines in most studies; WLS or diagonal-shrinkage MinT is the current industry default; but gains plateau when off-diagonals are weak citeturn0file9.  Only a handful of papers explore richer covariance structures (e.g., block or factor shrinkage), and none benchmark them systematically against NOVELIST-type estimators.

## Covariance Estimation in High Dimensions

-   Limitations of the sample covariance matrix.
-   Estimators used by Wickramasuriya et al. (2019).
-   Shrinkage estimators:
    -   Diagonal shrinkage (e.g., Schäfer & Strimmer, Ledoit & Wolf).
    -   NOVELIST estimator and its Cross-validation & PC-adjusted variant.
    -   ...

## Relevance to Forecast Reconciliation

-   Discuss how covariance estimation affects MinT performance.
-   Identify research gaps.
-->

# Old preliminary section

<!-- 
The *hierarchical structure* can be represented as a tree, as shown in @fig-hierarchy. The top level of the tree represents the total value of all series, while the lower levels represent the series at different levels of disaggregation. These hierarchical structures naturally form *aggregation constraints* (child levels must sum up to parents). When there are attributes of interest that are crossed, such as the Starbucks drinks sales at any aggregation level (brand-wise, national, or outlet) is also considered by kinds of drinks (e.g., coffees, refreshers), the structure is described as a grouped time series. As illustrated in @fig-grouped, the aggregation or disaggregation paths are not unique.

![A 2-level hierarchical tree structure](D:/Github/Recon_Honours_Thesis/figs/hierarchical_structure.png){#fig-hierarchy width="40%"}

![A 2-level grouped structure, which can be considered as the union of two hierarchical trees with common top and bottom level series](D:/Github/Recon_Honours_Thesis/figs/grouped_structure.png){#fig-grouped width="90%"}

For simplicity, we refer to both of these structures as hierarchical structure, we will distinguish between them if and when it is necessary. Taken together, a collection of time series organised in a hierarchy and subject to aggregation constraints is refered as *hierarchical time series*. All hierarchical time series can be represented using matrix algebra:

$$
\boldsymbol{y}_t = \boldsymbol{S} \boldsymbol{b}_t,
$$

where $\boldsymbol{S}$ is a summing matrix of order $n \times n_b$ which aggregates the bottom-level series $\boldsymbol{b}_t \in \mathbb{R}^{n_b}$ to the series at aggregation levels above. The vector $\boldsymbol{y}_t \in \mathbb{R}^n$ contains all observations at time $t$. The summing matrix $\boldsymbol{S}$ for the tree structure in @fig-hierarchy is:

$$
\boldsymbol{S} = 
\left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\boldsymbol{I_4}}
\end{array}
\right].
$$

Assume we produce $h$-step-ahead base forecasts $\hat{\boldsymbol{b}}_{t+h|t}$ for the bottom-level series, obtained by any prediction methods, pre-multiplying them by $\boldsymbol{S}$ we get:

$$
\tilde{\boldsymbol{y}}_{t+h|t} = \boldsymbol{S} \hat{\boldsymbol{b}}_{t+h|t} \, .
$$ {#eq-sb}

We refer to $\tilde{\boldsymbol{y}}_{t+h|t}$ as *coherent forecasts*, as they respect the aggregation constraints. We also refer to this way of obtaining coherent forecasts by summing the bottom-level forecasts as the bottom-up approach. However, generating forecasts this way is anchored only to prediction models at a single level, and will not be utilising the inherent information from other levels. This drawback applies to the top-down and middle-out approaches. For example, the bottom-level data can be very noisy or even intermittent, and the higher-level data might be smoother due to the aggregation.

Another issue with expressing reconciled methods as in @eq-sb is that it restricts the reconciliation to only single-level approaches. Thus, @Hyndman2011-jv suggested a generalised expression for all existing methods, which also provides a framework for new methods to be developed:

$$
\tilde{\boldsymbol{y}}_{t+h|t} = \boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_{t+h|t} \; ,
$$ {#eq-sgy}

for a suitable $n_b \times n$ matrix $\boldsymbol{G}$. $\boldsymbol{G}$ maps the base forecasts of all levels $\hat{\boldsymbol{y}}_{t+h|t}$ down into the bottom level, which is then aggregated to the higher levels by $\boldsymbol{S}$. The choice of $\boldsymbol{G}$ determines the composition of reconciled forecasts $\tilde{\boldsymbol{y}}_{t+h|t}$, and modern reconciliation methods are developed to estimate $\boldsymbol{G}$.

including the OLS and WLS developed by @Hyndman2011-jv and @Hyndman2016-ic, respectively. A more detailed explanation of these methods can be found in the @sec-ols-wls Appendix. 

-->

# Evaluating ex- and include 2016 onwards on Tourism

```{r}
library(stringr)
desired_order <- c(
  "Australia", "Australia_by_Purpose",
  "State",     "State_by_Purpose",
  "Zone",      "Zone_by_Purpose",
  "Region",    "Region_by_Purpose"
)
```


```{r fig tourism results}
#| label: fig-tourism-results
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows"
#| fig.width: 8
#| fig.height: 10
#| out.width: "100%"

tourism_MSE <- readRDS("./literature/lib/tourism_MSE.rds")

# % improv plot by level
tourism_MSE |>
  group_by(series, h) |>
  mutate(
    base_MSE = MSE[.model == "base"]
  ) |>
  ungroup() |>
  group_by(.model, level, h) |>
  summarise(
    MSE = mean(MSE),
    base_MSE = mean(base_MSE),
    pct_change = (MSE - base_MSE) / base_MSE * 100
  ) |>
  filter(.model != "mint_sample") |>
  # Reorder the levels according to the desired order
  mutate(
    level = factor(
      level,
      levels = desired_order,
      labels = str_replace_all(desired_order, "_", " ")  # replace "_" with " "
    )
  ) |> 
  ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(x = "Horizon", y = "% improvements") +
    facet_wrap(~ level, ncol=2, scales = "free_y") +
    theme_bw() +
    scale_x_continuous(limits = c(1, 12), expand = c(0, 0)) +
    theme(legend.position = "bottom", legend.title = element_blank())
```


The results are presented in @fig-tourism-results, which shows the relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows. The results are grouped by levels of aggregation. MinT variants show improvement over the base ARIMA forecasts for middle to lower aggregation levels. We hardly differentiate the point accuracy performance between mint_shr and mint_n in these level, except at the most disaggregated, where mint_shr slightly edges out mint_n.

```{r fig STL tourism}
#| label: fig-STL-tourism
#| fig-cap: "Monthly domestic overnight tourism nights in Australia by state, with STL decomposition to extract trend component"
#| fig.width: 8
#| fig.height: 6
#| out.width: "100%"
#| fig.align: "center"

visnights_full <- readRDS("literature/lib/visnights_full.rds")

visnights_full |> 
  filter(
    is_aggregated(Purpose) & is_aggregated(Zone) & is_aggregated(Region),
    !is_aggregated(State)
  ) |> 
  model(
    STL(Nights)
  ) |> 
  components() |> 
  pivot_longer(
    cols = c(Nights, trend, season_year, remainder, season_adjust),
    names_to = "component",
    values_to = "value"
  ) |>
  filter(
    component == c("Nights", "trend"),
    year(Month) >= 2000
  ) |>
  as_tibble() |>
  mutate(component = recode(component,                     # give them nicer names
                            Nights = "Observed Nights",
                            trend  = "STL Decomposed Trend")) |> 
  ggplot(aes(Month, value, color = as.character(State))) +
    geom_line() +
    facet_wrap(~ component, ncol=1, scales = "free_y") +
    labs(
      x = "Month",
      y = "Visitor Nights (in thousands)",
      color = "State"
    ) +
    theme_bw()
```

Surprisingly, however, MinT underperforms OLS at higher aggregation level and even the base forecasts at country-wise. This contradicts with the canonical results from @Wickramasuriya2019-xq, prompting further investigation. Applying STL decomposition by @Bandara2022-og to state-aggregated observations reveals a structural change around 2016: four large states (New South Wales, Victoria, Queensland, Western Australia) develop a modest upward trend in visitor nights, as shown in @fig-STL-tourism. The trend is likely due to the increasing popularity of tourism in these places, as well as the impact of various factors such as economic growth, population growth, and changes in consumer preferences. 

```{r}
#| label: fig-tourism-results-ex2016
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows, excluding the period after 2016"
#| fig.width: 8
#| fig.height: 10
#| out.width: "100%"

tourism_MSE_ex2016 <- readRDS("literature/lib/tourism_MSE_ex2016.rds")

# % improv plot by level
tourism_MSE_ex2016 |>
  group_by(series, h) |>
  mutate(
    base_MSE = MSE[.model == "base"]
  ) |>
  ungroup() |>
  group_by(.model, level, h) |>
  summarise(
    MSE = mean(MSE),
    base_MSE = mean(base_MSE),
    pct_change = (MSE - base_MSE) / base_MSE * 100
  ) |>
  filter(.model != "mint_sample") |>
  # Reorder the levels according to the desired order
  mutate(
    level = factor(
      level,
      levels = desired_order,
      labels = str_replace_all(desired_order, "_", " ")  # replace "_" with " "
    )
  ) |>
  ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(x = "Horizon", y = "% improvements") +
    facet_wrap(~ level, ncol=2, scales = "free_y") +
    theme_bw() +
    scale_x_continuous(limits = c(1, 12), expand = c(0, 0)) +
    theme(legend.position = "bottom", legend.title = element_blank())
```

Neither ARIMA nor reconciliation strategies capture this shift, degrading accuracy at the most aggregated level. From the results in @fig-tourism-results-ex2016, excluding the post-2016 period (i.e., restricting the final training window to end on December 2015) restores reconciliation’s performance: both MinT methods now clearly outperform OLS and the base forecasts at all levels. Additionally, we notice that mint_n increasingly outperforms mint_shr at higher levels (Australia, State, and Australia by Purposes levels). This is an advantage of NOVELIST estimator. At higher levels most series are combinations of many bottom cells, their cross-series base forecast error correlations are strong and genuinely useful. NOVELIST keeps these strong links at those higher levels, while its thresholding feature minimises the effects of weak, noisy ones.

The empirical analysis on real, hierarchical Australian domestic tourism shreds lights on the strengths of the NOVELIST estimator--advantages that were not fully apparent in previous simulation studies. Going forward, we plan to embed this structure into our data-generating processes and to pinpoint contexts where NOVELIST’s thresholding yields gains beyond those of Shrinkage. Moreover, our findings underscore the need to accommodate structural changes--the post-2016 Australian domestic tourism demand shifts we encountered.

<!-- perhaps via robust trend adjustment or state-space extensions that jointly model coherence and temporal dynamics. -->










<!-- ## POET

The POET (Principal Orthogonal complEment Thresholding) estimator, proposed by @Fan2013-jz, is another "sparse" + "non-sparse" covariance estimator. It takes the latent factors directly into its construction, and is appealing when there are common drivers in the time series within the hierarchy, as we saw in the Australian tourism example.

The POET method starts by decomposing the correlation matrix $\hat{\boldsymbol{R}}_1$ into a prominent principle components part (low-rank) and a orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$ (the correlation matrix after removing the first $K$ principal components). Then it applies thresholding to $\hat{\boldsymbol{R}}_{1, K}$. The POET estimator is given by:

$$
\hat{\boldsymbol{R}}^{K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + T(\hat{\boldsymbol{R}}_{1, K})
$$

where $\hat{\gamma}_k$ and $\hat{\boldsymbol{\xi}}_k$ are the $k$th largest eigenvalue and the corresponding eigenvector of the sample covariance matrix, respectively, and $T(.)$ is the thresholding function, which can be either soft-thresholding, hard-thresholding, or others. -->



<!-- ## PC-adjusted NOVELIST

This approach is best of both worlds, leveraging the strengths of both NOVELIST and POET. The PC-adjusted (Principal-Component-adjusted) NOVELIST overcomes the shortcomings of the current shrinkage estimator, taking prominent PCs into account while also offers extra flexibility. The idea is to apply the NOVELIST estimator to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$, and then add the principal components part back. The PC-adjusted NOVELIST estimator is formulated as:

$$
\hat{\boldsymbol{R}}^{N, K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + \hat{\boldsymbol{R}}^{N}_{1, K}
\; ,
$$

where $\hat{\boldsymbol{R}}^{N}_{1, K}$ is the NOVELIST estimator applied to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$. Similar to the NOVELIST estimator, $\hat{\boldsymbol{R}}^{N, K}_{1}$ is not guaranteed to be positive definite.

Methods to ensure positive definiteness of the NOVELIST estimator (and its PC-adjusted variant) will be explored and studied in the project. @Huang2019-ua proposed to diagonalise the NOVELIST estimator and replace any eigenvalues that fall under a certain small positive threshold by the value of that threshold. Alternatively, we can implement the algorithm of @Higham2002-te that computes the nearest positive definite matrix to a given matrix. -->







<!--  
```{r}
plot_relative_improvement <- function(MSE_data) {
  MSE_data |>
    filter(
      .model != "mint_true",
      h <= 4
    ) |>
    group_by(series, h) |>
    mutate(
      base_MSE = MSE[.model == "base"]
    ) |>
    ungroup() |>
    group_by(.model, h) |>
    summarise(
      MSE = mean(MSE),
      base_MSE = mean(base_MSE),
      pct_change = (MSE - base_MSE) / base_MSE * 100
    ) |>
    ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(
      x = "Forecast Horizon (h)",
      y = "Percentage Change (%)"
    ) +
    # labs(x = element_blank(), y = element_blank()) +
    theme_bw()
}
```

```{r fig-sim-results-1}
#| label: fig-sim-results-1
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the 2x2, 6x6, and 2x50 hierarchical structures, for 1 to 4 steps ahead forecasts, with 2 time series lengths (T = 54 and T = 304)."
#| fig.width: 8
#| fig.height: 6
#| out.width: "100%"
#| fig.align: "center"

p_MSE1_1 <- MSE1_1 |> 
  plot_relative_improvement() +
    labs(title = "2x2 Hierarchy", subtitle = "T = 54")
p_MSE2_1 <- MSE2_1 |> 
  plot_relative_improvement() +
    labs(title = "6x6 Hierarchy")
p_MSE3_1 <- MSE3_1 |> 
  plot_relative_improvement() +
    labs(title = "2x50 Hierarchy")

p_MSE1_2 <- MSE1_2 |> 
  plot_relative_improvement() +
    labs(subtitle = "T = 304")
p_MSE2_2 <- MSE2_2 |> 
  plot_relative_improvement()
p_MSE3_2 <- MSE3_2 |> 
  plot_relative_improvement()

p_MSE1_1 + p_MSE2_1 + p_MSE3_1 + p_MSE1_2 + p_MSE2_2 + p_MSE3_2 +
  plot_layout(ncol = 3, guides = "collect", axis_titles = "collect") &
  theme(legend.position = "right")
```

-->

Figure @fig-sim-results-1 illustrates that as hierarchy size and complexity increase, the relative improvements in MSE over the base forecasts from reconciliation amplify, across all 1- to 4-step horizons. While extending the training window (from 50 to 300 in‐sample observations) uniformly improves the accuracy of base ARIMA forecasts, it does not alter the relative improvements of mint_shr or mint_n. This is evidence that both high-dimensional estimators robustly handle the "large $p \gg T$” regime. By contrast, mint_sample achieves significant accuracy when more data make the sample covariance more reliable.

Despite their theoretical differences, mint_shr and mint_n exhibit nearly identical performance in these canonical settings. Yet NOVELIST’s thresholding ability to minimise effects of weak correlations and retain strong ones suggests it may excel in contexts where correlation matrix has small, noisy entries. This motivates our subsequent exploration of sparse covariance structures, in which many off-diagonals are truly zero.








<!-- 
\pagebreak

# Timeline & Milestones {#sec-timeline}

```{r timeline}
#| echo: false
#| message: false
#| warning: false
library(tibble)
library(knitr)
library(kableExtra)

tibble(
  Period = c(
    "March",
    "April", 
    "May", 
    "June",
    "July",
    "August", 
    "September",
    "October"
  ),
  Task = c(
    "Initial setup and understanding the research scope. Preliminary knowledge and Literature review",

    "Develop ReconCov, an R package implementing the Min Trace approach with the shrinkage and NOVELIST estimators, and rolling-window cross-validation process for choosing NOVELIST threshold parameter. Exploring simulation framework.",

    "Code functions for producing simulation settings in ReconCov package. Initial simulation for MinT in hierarchical time series. Writing research proposal and presenting research project.",

    "Intensive simulation on multiple different settings, in order to assess the performance and behaviours of NOVELIST under different conditions, and to explore cases where NOVELIST performs different than Shrinkage.",

    "Empirical analysis on forecasting Australian Domestic Tourism data. Explore the real-world data generating process which can be used to refine the current simulation design",

    "Implement the Tourism DGP configuration into the current simulation settings. Implementing POET and PC-adjusted NOVELIST estimators. Assessing whether prominent principal components structure can be exploited to improve the out-of-sample forecast performance.",

    "Exploring other high-dimensional covariance estimators. More simulations on all explored covariance estimators. Final,  full-fledged assessment on all implemented estimators",

    "Final writing, presenation, submission"
  ),
  Deliverable = c(
    "Initial literature review + paper draft",
    "ReconCov package: Main codes for estimators + simulation framework + cross-validation",
    "NOVELIST initial simulation results + research proposal + presentation",
    "More simulation results on NOVELIST",
    "Empirical results on forecasting Tourism data",
    "New simulation design, POET & PC-adjusted NOVELIST simulation results",
    "Other estimators simulation results and full-fledged covariance estimators assessment",
    "Presentation and thesis manuscript"
  )
) |>
  kable(
    format = "latex",
    booktabs = TRUE,
    longtable = TRUE,
    caption = "Project Timeline and Key Milestones",
    escape = TRUE
  ) |>
  column_spec(1, width = "1.9cm") |>
  column_spec(2, width = "8cm") |>
  column_spec(3, width = "4.8cm") |>
  kable_styling(latex_options = "hold_position", font_size = 10)
```
-->


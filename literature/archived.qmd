

<!--
-   **Problem Statement:**

    The sample covariance matrix, although natural, suffers in high-dimensional settings. Especially when the number of series $p$ is huge and larger than the time dimension $T$, the sample covariance matrix is non-positive definite (rank T if p\>T).

    The shrinkage estimators come in to tackle this issue. The shrinkage estimator with diagonal target (often shrinking toward a diagonal matrix) is proven to produce a guaranteed PD matrix (Schäfer & Strimmer, 2005). However, as it shrinks the covariance matrix toward a diagonal one, it does not have flexibility and might neglect the prominent structure presented in the covariance matrix.

    An alternative approach is to perform shrinkage of the sample covariance towards its thresholded version, instead of a diagonal matrix. This is the NOVELIST (NOVEL Integration of the Sample and Thresholded covariance estimators) method proposed by Huang & Fryzlewicz (2019). They introduced thresholding functions applied only to off-diagonal elements, allowing for more flexibility in the estimation.

    [... can include more estimators ...]{style="color:red;"}

-   **Research Aim:**

    This paper assesses the reconciled forecasting performance of MinT approach using various covariance estimators, with a focus on the NOVELIST estimator.

-   **Paper Outline:**

    The paper is structured as follows:

    -   A literature review of forecast reconciliation and covariance estimation.
    -   A description of the methodology, including the NOVELIST estimator and its principal-component-adjusted variant.
    -   An experimental design using both synthetic and real hierarchical time series.
    -   Empirical results and discussion.
    -   Conclusions and suggestions for future work.
-->

<!--
# Literature Review

## Forecast Reconciliation in Hierarchical and Grouped Time Series

Forecast reconciliation converts a collection of independent base forecasts into a set of coherent forecasts that respect the linear constraints defining a hierarchical or grouped time-series system. Early work focused on heuristic single-level strategies, including bottom-up, top-down, and middle-out (...), each of which exploits only part of the information in the hierarchy and can induce bias or high variance.

-   Cite the single level approach
-   Athanasopoulos et al., 2024

Hyndman et al. (2011) first showed that all single-level methods can be written as $\tilde{y} = SG\hat{y}$, where $S$ is the summing matrix and $G$ is a matrix that maps base forecasts $\hat{y}$ to into the bottom level. Treating reconciliation as a GLS regression problem, Hyndman et al. (2011) found that it yields a solution for $G$, but the required covariance matrix of reconcilation error is not identifiable in practice (Wickramasuriya et al., 2019).

-   (Talk more about how others transform it to OLS, WLS,..)
-   Di Fonzo and Marini (2011)
-   Athanasopoulos et al. (2009)

Wickramasuriya, Athanasopoulos & Hyndman (2019) reframed the problem by taking an optimisation approach rather than the regression. They formulated the problem as minimising the variances of all reconciled forecasts, which happens to be equivalent to minimising the trace of the covariance matrix (sum of the diagonal elements). This is known as the Minimum Trace (MinT) reconciliation method. The MinT solution is given by $G_h = (S'W_h^{-1}S)^{-1}S'W_h^{-1}$, and $W_h$ is the covariance matrix of the h-step-ahead base forecast errors.

The MinT approach is an algebraical generalisation of the GLS, and the OLS and WLS methods are special cases of MinT when $W_h$ is a diagonal or identity matrix, respectively. However, the MinT solution hinges on a reliable estimate of the h-step-ahead base forecast error covariance $W_h$. In high-dimensional setting, the usual sample covariance matrix is unstable, thus we need alternative covariance estimators.

-   Structural Scaling, based only on the struc- ture of the hierarchy (Athanasopoulos et al., 2017)
-   Shrinkage estimators (Schäfer & Strimmer, 2005; Ledoit & Wolf, 2004)

Empirical evaluations have demonstrated that MinT with an appropriate covariance estimate often outperforms earlier methods in both simulation and real data studies.

Wickramasuriya et al. (2019) adopt the diagonal-target Ledoit–Wolf approach and show analytically that any scalar multiple $k_h$ from $W_h = k_hW_1$ suffices for point-forecast reconciliation.  However, simulation and empirical evidence suggest that ignoring cross-series error correlation can leave accuracy untapped, especially in highly-coupled systems (energy, retail SKUs).  Their subsequent work imposes non-negativity constraints via quadratic programming citeturn0file2 and studies bias-robust variants (MinT-U).

Empirical findings to date

Athanasopoulos et al. (2024) synthesise the accumulated evidence: MinT dominates single-level baselines in most studies; WLS or diagonal-shrinkage MinT is the current industry default; but gains plateau when off-diagonals are weak citeturn0file9.  Only a handful of papers explore richer covariance structures (e.g., block or factor shrinkage), and none benchmark them systematically against NOVELIST-type estimators.

## Covariance Estimation in High Dimensions

-   Limitations of the sample covariance matrix.
-   Estimators used by Wickramasuriya et al. (2019).
-   Shrinkage estimators:
    -   Diagonal shrinkage (e.g., Schäfer & Strimmer, Ledoit & Wolf).
    -   NOVELIST estimator and its Cross-validation & PC-adjusted variant.
    -   ...

## Relevance to Forecast Reconciliation

-   Discuss how covariance estimation affects MinT performance.
-   Identify research gaps.
-->



# Evaluating ex- and include 2016 onwards on Tourism

```{r}
library(stringr)
desired_order <- c(
  "Australia", "Australia_by_Purpose",
  "State",     "State_by_Purpose",
  "Zone",      "Zone_by_Purpose",
  "Region",    "Region_by_Purpose"
)
```


```{r fig tourism results}
#| label: fig-tourism-results
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows"
#| fig.width: 8
#| fig.height: 10
#| out.width: "100%"

tourism_MSE <- readRDS("./literature/lib/tourism_MSE.rds")

# % improv plot by level
tourism_MSE |>
  group_by(series, h) |>
  mutate(
    base_MSE = MSE[.model == "base"]
  ) |>
  ungroup() |>
  group_by(.model, level, h) |>
  summarise(
    MSE = mean(MSE),
    base_MSE = mean(base_MSE),
    pct_change = (MSE - base_MSE) / base_MSE * 100
  ) |>
  filter(.model != "mint_sample") |>
  # Reorder the levels according to the desired order
  mutate(
    level = factor(
      level,
      levels = desired_order,
      labels = str_replace_all(desired_order, "_", " ")  # replace "_" with " "
    )
  ) |> 
  ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(x = "Horizon", y = "% improvements") +
    facet_wrap(~ level, ncol=2, scales = "free_y") +
    theme_bw() +
    scale_x_continuous(limits = c(1, 12), expand = c(0, 0)) +
    theme(legend.position = "bottom", legend.title = element_blank())
```


The results are presented in @fig-tourism-results, which shows the relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows. The results are grouped by levels of aggregation. MinT variants show improvement over the base ARIMA forecasts for middle to lower aggregation levels. We hardly differentiate the point accuracy performance between mint_shr and mint_n in these level, except at the most disaggregated, where mint_shr slightly edges out mint_n.

```{r fig STL tourism}
#| label: fig-STL-tourism
#| fig-cap: "Monthly domestic overnight tourism nights in Australia by state, with STL decomposition to extract trend component"
#| fig.width: 8
#| fig.height: 6
#| out.width: "100%"
#| fig.align: "center"

visnights_full <- readRDS("literature/lib/visnights_full.rds")

visnights_full |> 
  filter(
    is_aggregated(Purpose) & is_aggregated(Zone) & is_aggregated(Region),
    !is_aggregated(State)
  ) |> 
  model(
    STL(Nights)
  ) |> 
  components() |> 
  pivot_longer(
    cols = c(Nights, trend, season_year, remainder, season_adjust),
    names_to = "component",
    values_to = "value"
  ) |>
  filter(
    component == c("Nights", "trend"),
    year(Month) >= 2000
  ) |>
  as_tibble() |>
  mutate(component = recode(component,                     # give them nicer names
                            Nights = "Observed Nights",
                            trend  = "STL Decomposed Trend")) |> 
  ggplot(aes(Month, value, color = as.character(State))) +
    geom_line() +
    facet_wrap(~ component, ncol=1, scales = "free_y") +
    labs(
      x = "Month",
      y = "Visitor Nights (in thousands)",
      color = "State"
    ) +
    theme_bw()
```

Surprisingly, however, MinT underperforms OLS at higher aggregation level and even the base forecasts at country-wise. This contradicts with the canonical results from @Wickramasuriya2019-xq, prompting further investigation. Applying STL decomposition by @Bandara2022-og to state-aggregated observations reveals a structural change around 2016: four large states (New South Wales, Victoria, Queensland, Western Australia) develop a modest upward trend in visitor nights, as shown in @fig-STL-tourism. The trend is likely due to the increasing popularity of tourism in these places, as well as the impact of various factors such as economic growth, population growth, and changes in consumer preferences. 

```{r}
#| label: fig-tourism-results-ex2016
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows, excluding the period after 2016"
#| fig.width: 8
#| fig.height: 10
#| out.width: "100%"

tourism_MSE_ex2016 <- readRDS("literature/lib/tourism_MSE_ex2016.rds")

# % improv plot by level
tourism_MSE_ex2016 |>
  group_by(series, h) |>
  mutate(
    base_MSE = MSE[.model == "base"]
  ) |>
  ungroup() |>
  group_by(.model, level, h) |>
  summarise(
    MSE = mean(MSE),
    base_MSE = mean(base_MSE),
    pct_change = (MSE - base_MSE) / base_MSE * 100
  ) |>
  filter(.model != "mint_sample") |>
  # Reorder the levels according to the desired order
  mutate(
    level = factor(
      level,
      levels = desired_order,
      labels = str_replace_all(desired_order, "_", " ")  # replace "_" with " "
    )
  ) |>
  ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(x = "Horizon", y = "% improvements") +
    facet_wrap(~ level, ncol=2, scales = "free_y") +
    theme_bw() +
    scale_x_continuous(limits = c(1, 12), expand = c(0, 0)) +
    theme(legend.position = "bottom", legend.title = element_blank())
```

Neither ARIMA nor reconciliation strategies capture this shift, degrading accuracy at the most aggregated level. From the results in @fig-tourism-results-ex2016, excluding the post-2016 period (i.e., restricting the final training window to end on December 2015) restores reconciliation’s performance: both MinT methods now clearly outperform OLS and the base forecasts at all levels. Additionally, we notice that mint_n increasingly outperforms mint_shr at higher levels (Australia, State, and Australia by Purposes levels). This is an advantage of NOVELIST estimator. At higher levels most series are combinations of many bottom cells, their cross-series base forecast error correlations are strong and genuinely useful. NOVELIST keeps these strong links at those higher levels, while its thresholding feature minimises the effects of weak, noisy ones.

The empirical analysis on real, hierarchical Australian domestic tourism shreds lights on the strengths of the NOVELIST estimator--advantages that were not fully apparent in previous simulation studies. Going forward, we plan to embed this structure into our data-generating processes and to pinpoint contexts where NOVELIST’s thresholding yields gains beyond those of Shrinkage. Moreover, our findings underscore the need to accommodate structural changes--the post-2016 Australian domestic tourism demand shifts we encountered.

<!-- perhaps via robust trend adjustment or state-space extensions that jointly model coherence and temporal dynamics. -->










<!-- ## POET

The POET (Principal Orthogonal complEment Thresholding) estimator, proposed by @Fan2013-jz, is another "sparse" + "non-sparse" covariance estimator. It takes the latent factors directly into its construction, and is appealing when there are common drivers in the time series within the hierarchy, as we saw in the Australian tourism example.

The POET method starts by decomposing the correlation matrix $\hat{\boldsymbol{R}}_1$ into a prominent principle components part (low-rank) and a orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$ (the correlation matrix after removing the first $K$ principal components). Then it applies thresholding to $\hat{\boldsymbol{R}}_{1, K}$. The POET estimator is given by:

$$
\hat{\boldsymbol{R}}^{K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + T(\hat{\boldsymbol{R}}_{1, K})
$$

where $\hat{\gamma}_k$ and $\hat{\boldsymbol{\xi}}_k$ are the $k$th largest eigenvalue and the corresponding eigenvector of the sample covariance matrix, respectively, and $T(.)$ is the thresholding function, which can be either soft-thresholding, hard-thresholding, or others. -->



<!-- ## PC-adjusted NOVELIST

This approach is best of both worlds, leveraging the strengths of both NOVELIST and POET. The PC-adjusted (Principal-Component-adjusted) NOVELIST overcomes the shortcomings of the current shrinkage estimator, taking prominent PCs into account while also offers extra flexibility. The idea is to apply the NOVELIST estimator to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$, and then add the principal components part back. The PC-adjusted NOVELIST estimator is formulated as:

$$
\hat{\boldsymbol{R}}^{N, K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + \hat{\boldsymbol{R}}^{N}_{1, K}
\; ,
$$

where $\hat{\boldsymbol{R}}^{N}_{1, K}$ is the NOVELIST estimator applied to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$. Similar to the NOVELIST estimator, $\hat{\boldsymbol{R}}^{N, K}_{1}$ is not guaranteed to be positive definite.

Methods to ensure positive definiteness of the NOVELIST estimator (and its PC-adjusted variant) will be explored and studied in the project. @Huang2019-ua proposed to diagonalise the NOVELIST estimator and replace any eigenvalues that fall under a certain small positive threshold by the value of that threshold. Alternatively, we can implement the algorithm of @Higham2002-te that computes the nearest positive definite matrix to a given matrix. -->







<!--  
```{r}
plot_relative_improvement <- function(MSE_data) {
  MSE_data |>
    filter(
      .model != "mint_true",
      h <= 4
    ) |>
    group_by(series, h) |>
    mutate(
      base_MSE = MSE[.model == "base"]
    ) |>
    ungroup() |>
    group_by(.model, h) |>
    summarise(
      MSE = mean(MSE),
      base_MSE = mean(base_MSE),
      pct_change = (MSE - base_MSE) / base_MSE * 100
    ) |>
    ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(
      x = "Forecast Horizon (h)",
      y = "Percentage Change (%)"
    ) +
    # labs(x = element_blank(), y = element_blank()) +
    theme_bw()
}
```

```{r fig-sim-results-1}
#| label: fig-sim-results-1
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the 2x2, 6x6, and 2x50 hierarchical structures, for 1 to 4 steps ahead forecasts, with 2 time series lengths (T = 54 and T = 304)."
#| fig.width: 8
#| fig.height: 6
#| out.width: "100%"
#| fig.align: "center"

p_MSE1_1 <- MSE1_1 |> 
  plot_relative_improvement() +
    labs(title = "2x2 Hierarchy", subtitle = "T = 54")
p_MSE2_1 <- MSE2_1 |> 
  plot_relative_improvement() +
    labs(title = "6x6 Hierarchy")
p_MSE3_1 <- MSE3_1 |> 
  plot_relative_improvement() +
    labs(title = "2x50 Hierarchy")

p_MSE1_2 <- MSE1_2 |> 
  plot_relative_improvement() +
    labs(subtitle = "T = 304")
p_MSE2_2 <- MSE2_2 |> 
  plot_relative_improvement()
p_MSE3_2 <- MSE3_2 |> 
  plot_relative_improvement()

p_MSE1_1 + p_MSE2_1 + p_MSE3_1 + p_MSE1_2 + p_MSE2_2 + p_MSE3_2 +
  plot_layout(ncol = 3, guides = "collect", axis_titles = "collect") &
  theme(legend.position = "right")
```

-->

Figure @fig-sim-results-1 illustrates that as hierarchy size and complexity increase, the relative improvements in MSE over the base forecasts from reconciliation amplify, across all 1- to 4-step horizons. While extending the training window (from 50 to 300 in‐sample observations) uniformly improves the accuracy of base ARIMA forecasts, it does not alter the relative improvements of mint_shr or mint_n. This is evidence that both high-dimensional estimators robustly handle the "large $p \gg T$” regime. By contrast, mint_sample achieves significant accuracy when more data make the sample covariance more reliable.

Despite their theoretical differences, mint_shr and mint_n exhibit nearly identical performance in these canonical settings. Yet NOVELIST’s thresholding ability to minimise effects of weak correlations and retain strong ones suggests it may excel in contexts where correlation matrix has small, noisy entries. This motivates our subsequent exploration of sparse covariance structures, in which many off-diagonals are truly zero.
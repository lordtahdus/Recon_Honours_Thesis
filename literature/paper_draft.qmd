---
title: "Enhancing Forecast Reconciliation: A Study of Alternative Covariance Estimators"
author: 
  - Vincent Su
  - Shanika Wickramasuriya (supv.)
  - George Athanasopoulos (supv.)
format: 
  pdf:
    include-in-header:
      text: |
        <!-- \usepackage{amsmath} -->
        <!-- \usepackage{mathspec} -->
        \usepackage{algorithm}
        \usepackage{algpseudocode}
        \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
        \usepackage{array}
        \usepackage{float}
        \usepackage{xcolor}
    mathspec: true
    number-sections: true
# crossref:
#   custom:
#     - kind: float
#       key: alg
#       reference-prefix: Algorithm
#       caption-prefix: Algorithm
#       # for PDF output only, tell LaTeX to use the `algorithm` env:
#       latex-env: algorithm
link-citations: true
execute:
  cache: true
  echo: false
  warning: false
bibliography: D:/Github/Recon_Honours_Thesis/references.bib
# bibliographystyle: apa
csl: D:/Github/Recon_Honours_Thesis/apa.csl
editor_options: 
  chunk_output_type: console
---

<!-- # Notation {-} -->

<!-- -   Scalar $y_t$ -->

<!-- -   Vector $\boldsymbol{y}_t$ -->

<!-- -   Matrix $\boldsymbol{S}$ -->

<!-- -   Covariance matrix of in-sample 1-step-ahead base forecast errors $\hat{\boldsymbol{W}}_1$ -->

<!-- -   Its shrinkage estimator with diagonal target $\hat{\boldsymbol{W}}^{shr}_{1, D}$ -->

<!-- -   Its NOVELIST estimator $\hat{\boldsymbol{W}}^{shr}_{1, thr}$ -->

```{r library}
library(ggplot2)
library(patchwork)
library(dplyr)
library(tidyr)
library(fable)
library(fabletools)
library(feasts)
library(tsibble)
library(lubridate)
library(knitr)
library(kableExtra)
library(stringr)
library(forcats)

library(ReconCov)

path <- "D:/Github/Recon_Honours_Thesis"
knitr::opts_knit$set(root.dir = path)
# setwd(path)
```

# Abstract {.unnumbered}

<!-- [This is pasted from the Project Description]{style="color:red;"} -->

A collection of time series connected via a set of linear constraints is known as hierarchical time series. Forecasting these series without respecting the hierarchical nature of the data can lead to incoherent forecasts across aggregation levels and lower accuracy. To mitigate this issue, various forecast reconciliation approaches have been proposed in the literature, where the individual forecasts are adjusted to satisfy the aggregation constraints. Among these, **MinT** (Minimum Trace) is widely used, however, it requires a good estimate of the covariance matrix of the base forecast errors. The current practice is to use the shrinkage estimator (often shrinking toward a diagonal matrix), but it lacks flexibility and might not fully utilise the prominent latent structure presented. In this project, we aim to assess the forecasting performance of MinT when different covariance estimators are used, namely NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance), PC-adjusted estimators (taking the latent factors directly into its construction), and others.

# Introduction

<!-- ############  Intro  ############ -->

<!--
Para 1
- Introduce hierarchical time series and its importance in various applications.
-->

In time series forecasting, aggregation occurs in a variety of settings. While a formal definition of hierarchical time series can be found in @sec-hierarchy, we can think of Starbucks sales data as an illustrative example. Starbucks operates in many countries, and each country has multiple cities where they have outlets. The sales data is *structured hierarchically*: the top level is the total sales across all countries, followed by national sales for each country, and then individual sales for each outlet in a city. As a result, there are over 40,000 individual outlet sales to forecast, plus additional series at higher levels of aggregation such as city and country. The hierarchy can be even more complex if we consider the sales of different kinds of drinks (e.g., coffees, teas, refreshers) at each aggregation level.

Forecasting data from such hierarchical structures also arises in many other decision-making contexts, from supply chains [@Seaman2022-bb; @Angam2025-od] and energy planning [@Di-Modica2021-ad], to macroeconomics [@El-Gemayel2022-hx; @Li2019-vv] and tourism analysis [@Athanasopoulos2009-lp]. Stakeholders in these settings need forecasts at several aggregation levels to allocate resources and manage risk. The impact of methods for forecasting hierarchical time series has not been limited to academia, with industry also showing a strong interest. Many companies and organisations have adopted these methods in practice, including Amazon, the International Monetary Fund, IBM, SAP, and more [@Athanasopoulos2024-as].

<!-- ##########  Literature Review  ########## -->

<!-- - Talk about the history and evolution of forecasting hierarchical time series, starting from the early heuristic methods to the modern statistical approaches.

  - Single level methods
  - Optimal combination methods (OLS, WLS)
  - MinT
  - Bayesian, Machine learning
  - Probabilistic methods -->


In practice, when forecasts are produced for all series (often called *base forecasts*), they typically violate the aggregation constraints observed in the data; such forecasts are *incoherent*. This can undermine downstream decisions that require internal consistency.

Traditionally, forecasting these hierarchical time series has been done using single-level methods, such as bottom-up, top-down, and middle-out approaches. Bottom-up methods involve generating forecasts for the bottom-level series and aggregating them to higher levels. Top-down methods start with forecasts for the only top-level series and disaggregate them down. Middle-out methods combine both approaches by forecasting middle-level series and then aggregating or disaggregating as needed. Despite their simplicity, these methods only anchor forecasts to a single level, implying a large loss of information on the hierarchy's inherent correlation structure. Additionally, the most disaggregated series often are very noisy or even intermittent, and the higher-level data might be smoother due to the aggregation. Furthermore, as we saw from the Starbucks example considering the sales of different kinds of drinks at each aggregation level -- formally defined as grouped structure in @sec-hierarchy -- the disaggregation becomes more complex since the disaggregation paths are not unique. Consequently, these single-level methods tend to give poor results across other levels of the hierarchy.

To overcome these issues, forecast reconciliation was introduced by @Hyndman2011-jv, and later developed by @Van_Erven2015-nx, @Hyndman2016-ic, @Ben-Taieb2019-yx, @Wickramasuriya2019-xq, and others to achieve coherency in point forecasts and enhance accuracy. Forecast reconciliation projects a collection of independent base forecasts into a set of coherent forecasts that respect the linear constraints defining a hierarchical or grouped time-series system.

Among the modern reconciliation strategies, the Min Trace (MinT) approach developed by @Wickramasuriya2019-xq is widely used and perform significantly well under right conditions. Despite its properties, MinT relies on a good high-dimensional covariance estimator, which is positive-definite. However, not many researchers have explored this issue in the current literature, except @Carrara2025-rz, who introduced a new estimator for MinT--Double Shrinkage estimator. The current stage of this research piece is fairly early, leaving many potential gaps still not yet to explore. As a result, a study of alternative high-dimensional covariance estimators for MinT is timely and desirable.

The remainder of the paper is organised as follows. @sec-pre-theory provides the basic theoretical framework for hierarchical time series and forecasting, and Min Trace approach, introducing notations, terminologies, and motivations for alternative estimators. @sec-cov-est walks through the main covariance estimators this paper explores, and argues their strengths and weaknesses. @sec-simulation covers the simulation design and currently explores the performance of NOVELIST on MinT. @sec-empirical shows a real-world application of MinT with NOVELIST, which produces results that did not occured in the simulation settings, suggesting further inspection and analysis.

<!-- Include more details on the gaps for each of these methods, such as bias, high variance? -->

<!-- - Discuss the interests in MinT and how it has become a standard method for forecast reconciliation.

- Discuss the MinT's reliance on a good estimate of the covariance matrix of base forecast errors and other gaps

  - Empirical evidence of MinT under perform
  - Comparison with other methods

- Explain why this paper focus on exploring alternative covariance estimators for MinT. And is there any paper talk about this.

  - Is better estimate of W_h really lead to better performance?

- Talk about the paper outline and what will be covered in the following sections. -->

# Theoretical Framework {#sec-pre-theory}

## Hierarchical Time Series {#sec-hierarchy}

\color{red}
to be revised with probabilistic framework

- start with hierarchical time series
  - mention hier and grouped example (but stick to the general notation)
- talk about S, containing A
  - example of S
- coherent subspace
- coherent point forecasts
  - projection
- coherent probabilistic forecasts
  - definition
  - image

\color{black}

*Hierarchical time series* are multivariate time series $\boldsymbol{y}_t \in \mathbb{R}^n$ organised in a structure where the series adheres to some *constraints*. For example, @fig-hierarchy illustrates a simple 2-level hierarchical structure with one top-level series $y_{Tot,t}$, two middle-level series $(y_{A,t}, y_{B,t})'$, and four bottom-level series $(y_{A1,t}, y_{A2,t}, y_{B1,t}, y_{B2,t})'$. Here, the *aggregation constraints* imply that $y_{Tot,t} = y_{A,t} + y_{B,t}$, $y_{A,t} = y_{A1,t} + y_{A2,t}$, and $y_{B,t} = y_{B1,t} + y_{B2,t}$.

![A 2-level hierarchical tree structure](../figs/hierarchical_structure.png){#fig-hierarchy width="40%"}

The bottom-level (or most disaggregated) series are denoted as $\boldsymbol{b}_t \in \mathbb{R}^{n_b}$. Thus, the hierarchical time series can be represented as:

$$
\boldsymbol{y}_t = \boldsymbol{S} \boldsymbol{b}_t,
$$

where $\boldsymbol{S} \in \mathbb{R}^{n \times n_b}$ is a summing matrix that aggregates the bottom-level to all-level series. The summing matrix $\boldsymbol{S}$ for the tree structure in @fig-hierarchy is:

$$
\boldsymbol{S} = 
\left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\boldsymbol{I_4}}
\end{array}
\right].
$$

The matrix $\boldsymbol{S}$ encodes the aggregation constraints presented in the structure. Hence, the columns of $\boldsymbol{S}$ span a linear subspace. Any observation $\boldsymbol{y}_t$ that lies inside this subspace is called *coherent*, while those outside are *incoherent*. We refer to the subspace spanned by $\boldsymbol{S}$ as the *coherent subspace* $\mathfrak{s} \in \mathbb{R}^{n_b}$.

![A 2-level grouped structure, which can be considered as the union of two hierarchical trees with common top and bottom level series](../figs/grouped_structure.png){#fig-grouped width="90%"}

This setting is not restricted to hierarchical (nested) structures. When there are attributes of interest that are crossed, such as the company sales at any aggregation level (company-wise, city-wise, or outlet-wise) is also considered by kinds of products, the structure is described as a *grouped structure*. As illustrated in @fig-grouped, the aggregation or disaggregation paths are not unique. These constraints formed by the grouped structure can also be represented using a summing matrix $\boldsymbol{S}$. For simplicity, we refer to both of these structures as hierarchical structure, we will distinguish between them if and when it is necessary. 

<!-- There are other forms of structures, such as temporal hierarchies or non-linear constraints, but they are beyond the scope of this paper.  -->

When we produce forecasts for each individual series, referred to as *base forecasts* $\hat{\boldsymbol{y}}_{t+h|t}$, they often do not respect the aggregation constraints, and thus are incoherent. Coherency can be achieved by linearly projecting the base forecasts onto the coherent subspace $\mathfrak{s}$ using a projection matrix $\boldsymbol{P}$: $\tilde{\boldsymbol{y}}_{t+h|t} = \boldsymbol{P} \hat{\boldsymbol{y}}_{t+h|t}$, where $\tilde{\boldsymbol{y}}_{t+h|t}$ are the *reconciled forecasts*. A schematic illustration of this projection is depicted in @fig-geometry.

![Geometry of probabilistic forecast reconciliation. The eliptical base forecast distribution is projected orthogonally onto the coherent subspace (purple line), resulting in the reconciled forecast distribution (red). The projection is defined by the projection matrix $\boldsymbol{P}$, and MinT allows oblique projections. Note that this figure is schematic since most applications are high-dimensional.](../figs/geometry_orthogonal.png){#fig-geometry width="60%"}

Many existing reconciliation methods including the OLS [@Hyndman2011-jv], WLS [@Hyndman2016-ic], and MinT [@Wickramasuriya2019-xq] express the projection matrix as $\boldsymbol{P} = \boldsymbol{S} \boldsymbol{G}$, for a suitable $n_b \times n$ mapping matrix $\boldsymbol{G}$. The idea is to map the base forecasts of all levels $\hat{\boldsymbol{y}}_{t+h|t}$ down into the bottom level, which is then aggregated to the higher levels by $\boldsymbol{S}$. Since the projection matrix $\boldsymbol{P}$ is idempotent and symmetric, $\boldsymbol{G}$ must satisfy the condition $\boldsymbol{S} \boldsymbol{G} \boldsymbol{S} = \boldsymbol{S}$. Thus, we generally have the mapping matrix $\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{M}^{-1} \boldsymbol{S})^{-1} \boldsymbol{S}' \boldsymbol{M}^{-1}$, for some positive definite matrix $\boldsymbol{M}$.

When setting $\boldsymbol{M} = \boldsymbol{I}_n$, the identity matrix, we obtain the OLS method, which also corresponds to an orthogonal projection onto the coherent subspace. Research have been done to introduce $\boldsymbol{M}$ that better utilise the inherent information from the observed data, resulting in oblique projections to deliver better-performing reconciled forecasts.


## The Minimum Trace (MinT) Reconciliation

- we link to MinT saying that when M is the covariance matrix of base forecast errors, minimise the trace of the reconciled forecast error covariance matrix 

@Wickramasuriya2019-xq showed that by setting $\boldsymbol{M} = \boldsymbol{W}_h = \mathbb{E}( \hat{\boldsymbol{e}}_{t+h|t} \; \hat{\boldsymbol{e}}_{t+h|t}' )$, the covariance matrix of the $h$-step-ahead base forecast errors $\hat{\boldsymbol{e}}_{t+h|t} = \boldsymbol{y}_{t+h} - \hat{\boldsymbol{y}}_{t+h|t}$, we essentially minimise the total variance of the reconciled forecast errors across all series. In which, it is minimising the trace of $\text{Var}[y_{t+h} - \tilde{y}_{t+h|t}] = \boldsymbol{S} \boldsymbol{G} \boldsymbol{W}_h \boldsymbol{G}' \boldsymbol{S}'$. This method is called Minimum Trace (MinT) reconciliation. The matrix $\boldsymbol{G}$ is thus given by:

$$
\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{W}_h^{-1} \boldsymbol{S})^{-1}
\boldsymbol{S}' \boldsymbol{W}_h^{-1} .
$$

<!-- @Wickramasuriya2019-xq framed the problem as minimising the variances of all reconciled forecast errors $\text{Var}[y_{t+h} - \tilde{y}_{t+h|t}] = \boldsymbol{S} \boldsymbol{G} \boldsymbol{W}_h \boldsymbol{G}' \boldsymbol{S}'$, where $\boldsymbol{W}_h = \mathbb{E}( \hat{\boldsymbol{e}}_{t+h|t} \; \hat{\boldsymbol{e}}_{t+h|t}' )$ is the positive definite covariance matrix of the $h$-step-ahead base forecast errors. They showed that this is equivalent to minimising the trace of the reconciled forecast error covariance matrix (sum of the diagonal elements - the variances). The Minimum Trace (MinT) solution is given by

$$
\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{W}_h^{-1} \boldsymbol{S})^{-1}
\boldsymbol{S}' \boldsymbol{W}_h^{-1} .
$$

@Wickramasuriya2019-xq also showed that MinT is an algebraic generalisation of the GLS, and the OLS and WLS methods are special cases of MinT when $\boldsymbol{W}_h$ is an identity matrix $I_{n_b}$ and a diagonal matrix $\text{diag}( \boldsymbol{W}_h )$, respectively. In this paper, we place our main focus on the MinT method. -->

The MinT solution hinges on a reliable, positive-definite estimate of $\boldsymbol{W}_h$, which is often not available and challenging to estimate in high-dimensional setting. The sample covariance matrix is unstable and non-positive-definite when the number of series $n$ is huge and larger than the time dimension $T$. To tackle this issue, the original paper @Wickramasuriya2019-xq assumed a proportionality relationship between $\hat{\boldsymbol{W}}^g_h = k_h g( \hat{\boldsymbol{W}}_1 )$, where $\hat{\boldsymbol{W}}_1$ is the covariance matrix of the in-sample 1-step-ahead base residuals (to approximate $\boldsymbol{W}_1$), $k_h$ is a positive scaling constant (which will be cancelled out in point-forecast reconciliation), and $g(.)$ is a covariance estimator that produces a positive-definite matrix. They suggested using the shrinkage estimator with diagonal target from @Schafer2005-yw, given by:

$$
\hat{\boldsymbol{W}}^{S}_{1} = \lambda_D \hat{\boldsymbol{W}}_{1, D} + (1 - \lambda_D) \hat{\boldsymbol{W}}_1 \, ,
$$

where $\hat{\boldsymbol{W}}_{1, D}$ is a diagonal matrix comprising the diagonal entries $\text{diag}( \hat{\boldsymbol{W}}_1 )$. We refer to any $\lambda \in [0,1]$ as the shrinkage intensity parameter, the subscript specifies which estimator it belongs to. This approach shrinks the covariance matrix $\hat{\boldsymbol{W}}_1$ towards its diagonal matrix, meaning the off-diagonal elements are shrunk towards zero while the diagonal ones remain unchanged.

@Schafer2005-yw also proposed an estimate of the optimal shrinkage intensity parameter $\lambda_D$:

$$
\hat{\lambda}_D = \frac{\sum_{i \neq j} \widehat{Var}(\hat{r}_{ij})} 
{\sum_{i \neq j} \hat{r}_{ij}^2} \, ,
$$

where $\hat{r}_{ij}$ is the $ij$th element of $\hat{\boldsymbol{R}}_1$, the 1-step-ahead sample correlation matrix (obtained from $\hat{\boldsymbol{W}}_1$). The optimal estimate is obtained by minimising $MSE(\hat{\boldsymbol{W}}_1) = Bias(\hat{\boldsymbol{W}}_1)^2 + Var(\hat{\boldsymbol{W}}_1)$. More specifically, we trade the unbiasedness of the sample covariance matrix for a lower variance.

Despite its relative simplicity and guaranteed positive definiteness, MinT with shrinkage estimator has three main shortcomings.

### Problem 1: Uniform shrinkage {.unnumbered}

The shrinkage estimator shrinks all off-diagonal elements towards zeros with equal weights $\lambda_D$. We might prefer to better preserve strong signals, and largely reduce the effects of small, noisy correlations. This is especially true in high-dimensional settings, where only a few series to be truly correlated; or due to the aggregation effects, bottom-level series will correlate more strongly with their parents. The shrinkage estimator lacks this flexibility.

### Problem 2: Latent factors {.unnumbered}

The hierarchical time series data often exhibit a prominent principal components structure, which is not fully taken advantage. Taking an example of the Australian domestic overnight trips data set [@tourism-au], where the national trips are disaggregated into states and territories, and further into regions. We then fit ETS models to all series, using the algorithm from Fabletools R package [@O-Hara-Wild2024-we], and compute the one-step-ahead in-sample base forecast residual covariance matrix $\hat{\boldsymbol{W}}_1$. The twenty largest eigenvalues of the covariance matrix are plotted in @fig-eigen. We can see that the point of inflexion occurs at the component with 5th largest eigenvalue, indicating a prominent principal components structure.

![Twenty largest eigenvalues of one-step-ahead in-sample base forecast error covariance, Australian domestic overnight trips](D:/Github/Recon_Honours_Thesis/figs/eigen_tourism.png){#fig-eigen width="70%"}

### Problem 3: Proportional scaling for h-step-ahead {.unnumbered}

The proportionality relationship $\hat{\boldsymbol{W}}^g_h = k_h g( \hat{\boldsymbol{W}}_1 )$ might not hold in practice. The covariance structure of h-step-ahead base forecast errors can be different from that of 1-step-ahead ones.

In the next sections, we will explore several options that tackle these issues one by one.

# Covariance Estimation Approaches {#sec-cov-est}

## NOVELIST Estimator

The NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance) estimator, proposed by @Huang2019-ua, introduces a way to control the target matrix's sparsity, retaining strong correlations while discarding weak, noisy effects. NOVELIST offers more flexibility than the shrinkage estimator, which is useful when we believe that only a few variables are truly correlated.

The method is based on the idea of soft-thresholding the sample covariance matrix, then performing shrinkage towards this thresholded version. This introduces an extra parameter, the threshold $\delta$, which is used to control the amount of soft-thresholding. The NOVELIST estimator is given by:

$$
\hat{\boldsymbol{W}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{W}}_{1, \delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{W}}_1,
$$ {#eq-novelist-cov}

where $\hat{\boldsymbol{W}}_{1, \delta}$ is the thresholded version of $\hat{\boldsymbol{W}}_{1}$. By convenient setting, we can rewrite it in terms of correlation:

$$
\hat{\boldsymbol{R}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{R}}_{1,\delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{R}}_1,
$$ {#eq-novelist-cor}

where, $\hat{\boldsymbol{R}}_{1,\delta}$ is the thresholded correlation matrix, where each element is regularised by:

$$
\hat{r}_{1,ij}^\delta = \text{sign}(\hat{r}_{1,ij}) \, 
\text{max}(|\hat{r}_{1,ij}| - \delta, \; 0),
$$ {#eq-soft-thr}

where $\delta \in [0,1]$ is the threshold parameter. For a given threshold $\delta$, @Huang2019-ua derived an analytical expression for the optimal shrinkage intensity parameter $\lambda(\delta)$ using Ledoit-Wolf's lemma \[@Ledoit2003-qv\], following similar logic to @Schafer2005-yw. It can be computed as:

<!-- - \textcolor{red}{@Huang2019-ua mentioned Ledoit-Wolf's lemma} -->

$$
\hat{\lambda}(\delta) = \frac{
  \sum_{i \neq j} \widehat{Var}(\hat{r}_{1,ij}) \; \boldsymbol{1}(|\hat{r}_{1,ij}| \leq \delta)
} {
  \sum_{i \neq j} (\hat{r}_{1,ij} - \hat{r}_{1,ij}^\delta)^2
},
$$ {#eq-lambda-thr}

where $\boldsymbol{1}(.)$ is the indicator function.

On the other hand, the optimal threshold $\hat \delta$ does not have a closed-form solution, and is typically obtained by executing a rolling-window cross-validation procedure. The idea is to find the threshold $\delta^*$, with the corresponding $\lambda^*$ and $\hat{\boldsymbol{R}}^{N}_{1}(\delta^*, \lambda^*)$, that minimises the average out-of-sample 1-step-ahead reconciled forecast mean squared error over all windows. The formal algorithm is given in @sec-novelist_cv.

We also tested out minimising 1- to h-step-ahead overall MSE in the cross-validation procedure. Surprisingly, it returns almost the same best threshold parameter is in the 1-step-ahead case above. Note that when $\delta \in \bigl[ \text{max}_{i \neq j}|\hat{r}_{1,ij}|, \; 1 \bigr]$, the NOVELIST estimator collapses to the shrinkage estimator, and when $\delta = 0$, it becomes the sample covariance matrix. An additional concern is that the estimator does not guarantee to be positive definite, but we can use @Higham2002-te algorithm to compute the nearest positive definite matrix if needed.

### NOVELIST cross-validation algorithm {#sec-novelist_cv}

It is only required to fit the base models once on the whole training data $\{\boldsymbol{y}_t\}_{t=1}^T$, and obtain the in-sample fitted values $\{\hat{\boldsymbol{y}}_t\}_{t=1}^T$.

```{=latex}
\begin{algorithm}[H]
\caption{Cross-validation procedure}
\begin{algorithmic}[1]

\State \textbf{Input:} Observations and fitted values $\boldsymbol{y}_t, \hat{\boldsymbol{y}}_t \in \mathbb{R}^n$ for $t = 1,\dots,T$, set of threshold candidates $\Delta$, window size $v$.

\State $\hat{\boldsymbol{e}}_t = \boldsymbol{y}_t - \hat{\boldsymbol{y}}_t$ for $t = 1,\dots,T$

\For{$i = v:T-1$}
    \State $j = i - v +1$
    \State $\hat{\boldsymbol{W}}_j = \frac{1}{v} \sum_{t=j}^{i} \hat{\boldsymbol{e}}_{t} \hat{\boldsymbol{e}}_{t}'$
    \State $\hat{\boldsymbol{D}}_j = \text{diag}(\hat{\boldsymbol{W}}_j)$
    \State $\hat{\boldsymbol{R}}_j = \hat{\boldsymbol{D}}_j^{-1/2} \hat{\boldsymbol{W}}_j \hat{\boldsymbol{D}}_j^{-1/2}$
    \For{$\delta \in \Delta$}
        \State Compute thresholded correlation $\hat{\boldsymbol{R}}_{j,\delta}$ using Equation 5
        \State Compute $\hat{\lambda}_{j,\delta}$ using Equation 6
        \State Compute $\hat{\boldsymbol{R}}^{N}_{j,\delta}$ using Equation 4
        \State $\hat{\boldsymbol{W}}^{N}_{j,\delta} = \hat{\boldsymbol{D}}_j^{1/2} \hat{\boldsymbol{R}}^{N}_{j,\delta} \hat{\boldsymbol{D}}_j^{1/2}$
        \State $\boldsymbol{G} = (\boldsymbol{S}' \hat{\boldsymbol{W}}^{N^{-1}}_{j,\delta} \boldsymbol{S})^{-1} \boldsymbol{S}' \hat{\boldsymbol{W}}^{N^{-1}}_{j,\delta}$
        \State Reconciled forecasts $\tilde{\boldsymbol{y}}_{i+1 | \delta} = \boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_{i+1}$
        \State $\tilde{\boldsymbol{e}}_{i+1 | \delta} = \boldsymbol{y}_{i+1} - \tilde{\boldsymbol{y}}_{i+1 | \delta}$
    \EndFor
\EndFor

\State $\text{MSE}_{\delta} = \frac{1}{T-v} \sum_{i=v}^{T-1} (\tilde{\boldsymbol{e}}_{i+1 | \delta})^2$ for each $\delta \in \Delta$
\State $\hat{\delta}^* = \arg\min_{\delta \in \Delta} \text{MSE}_{\delta}$
\State Compute $\hat{\lambda}^*$ on all training data using $\hat{\delta}^*$
\State Compute $\hat{\boldsymbol{R}}_1^*$ using $\hat{\delta}^*$ and $\hat{\lambda}^*$ on all training data, using Equation 3

\State \textbf{Output:} Estimate of optimal $\hat{\delta}^*$

\end{algorithmic}
\end{algorithm}
```

The cross-validation algorithm for NOVELIST is available in the ReconCov package [@ReconCov].

## PC-adjusted Estimator

To utilise the latent factors structure for better shrinkage, the PC-adjusted method takes the latent factors directly into its construction, and is appealing when there are common drivers in the time series within the hierarchy, as we saw in the Australian tourism example. It starts by decomposing the covariance matrix $\hat{\boldsymbol{W}}_1$ into a prominent principle components part (low-rank) and a orthogonal complement part $\hat{\boldsymbol{W}}^K_{1}$ (the correlation matrix after removing the first $K$ principal components). Then we can apply either shrinkage or NOVELIST estimator to $\hat{\boldsymbol{W}}^K_{1}$:

$$
\hat{\boldsymbol{W}}^{g, K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + g(\hat{\boldsymbol{W}}^K_{1})
$$

where $g(.)$ is either the shrinkage or NOVELIST estimator, $\hat{\gamma}_k$ and $\hat{\boldsymbol{\xi}}_k$ are the $k$-th largest eigenvalue and the corresponding eigenvector of the sample covariance matrix, respectively. Similar to the NOVELIST estimator, its PC-adjusted variant $\hat{\boldsymbol{W}}^{N, K}_{1}$ requires a cross-validation procedure and adjustment to obtain positive definiteness.
## Scaled Variance

To address the potential issue of the proportionality relationship $\hat{\boldsymbol{W}}^g_h = k_h g( \hat{\boldsymbol{W}}_1 )$ not holding in practice, we can relax the assumption by allowing the variances to scale differently. This is done by scaling the $1$-step-ahead correlation matrix back to $h$-step-ahead covariance matrix using the h-step-ahead standard deviations. The scaled variance estimator is given by:

$$
\hat{\boldsymbol{W}}^{g, sv}_{h} = \boldsymbol{D}^{1/2}_h g(\hat{\boldsymbol{R}}_1) \boldsymbol{D}^{1/2}_h,
$$

where $\boldsymbol{D}_h = \text{diag}(\hat{\sigma}^2_{1,h}, \ldots, \hat{\sigma}^2_{n,h})$, and $\hat{\sigma}^2_{i,h}$ is the variance of the $i$-th series' h-step-ahead base forecast errors. Similarly, $g(.)$ is either the shrinkage or NOVELIST estimator.

If NOVELIST is used to produce $h$-step-ahead reconciled forecasts, the cross-validation procedure is slightly modified to evaluate the out-of-sample reconciled forecast MSE using $\hat{\boldsymbol{W}}^{N, sv}_{h}$ instead of $\hat{\boldsymbol{W}}^{N}_{1}$.

## Constructing from h-step-ahead residuals

Another alternative is not to rely on the assumption of proportionality at all, and directly estimate the covariance matrix from the h-step-ahead base forecast errors:

$$
\hat{\boldsymbol{W}}^{g}_{h} = g(\hat{\boldsymbol{W}}_h)
$$

where $\hat{\boldsymbol{W}}_h$ is the covariance matrix of in-sample h-step-ahead base forecast residuals, and $g(.)$ is either the shrinkage or NOVELIST estimator. Similar to the scaled variance approach, if NOVELIST is used, the cross-validation procedure is modified to compute $\hat{\boldsymbol{W}}^{N}_{h}$ and evaluate the out-of-sample reconciled forecast MSE using it.

## Summary of MinT with Covariance Estimators {-}



| Covariance estimators used              | Abbreviation       | 
|-----------------------------------------|--------------------|
| Shrinkage                               | MinT-S             |
| NOVELIST                                | MinT-N             |
| PC-adjusted Shrinkage with *K* PCs      | MinT-S(PC*K*)      |
| PC-adjusted NOVELIST with *K* PCs       | MinT-N(PC*K*)      |
| Scaled Variance Shrinkage               | MinT-S(sv)         |
| Scaled Variance NOVELIST                | MinT-N(sv)         |
| Constructed from h-step-ahead Shrinkage | MinT-S(hcov)       |
| Constructed from h-step-ahead NOVELIST  | MinT-N(hcov)       |

# Probabilistic Scoring Rules {#sec-scores}

We use the mean squared error (MSE) to evaluate the accuracy of point forecasts. Meanwhile, to assess the quality of the probabilistic forecasts, it is common to use scoring rules. A scoring rule is a function $S(., .)$ taking a predictive distribution as its first argument and a realisation as its second argument, then returns a numerical score. We follow a convention that lower scores are better. A scoring rule is said to be *proper* if $\mathbb{E}_Q[S(Q, y)] \leq \mathbb{E}_Q[S(P, y)]$ for all $P$, where $P$ is a predictive distribution produced by forecasting model, $Q$ is the true distribution of the realisation $y$, and $\mathbb{E}_Q$ is the expectation with respect to $Q$. Hence, the expected score is minimised when the forecast distribution matches the true distribution.

We employ Winkler score and continuous ranked probability score as our univariate scoring rules, and energy score as the multivariate scoring rule. All three are proper scoring rules.

\textcolor{blue}{\textit{Winkler score (WS).}} If the 100$(1-\alpha)$\% prediction interval is $[l, u]$ (the $\alpha / 2$ and $1 - $\alpha / 2$ quantiles), then the Winkler score is defined as:

$$
WS_{\alpha}(l, u; y) = (u - l) + \frac{2}{\alpha} (l - y) \boldsymbol{1}(y < l) + \frac{2}{\alpha} (y - u) \boldsymbol{1}(y > u),
$$

where $y$ is the observed value, and $\boldsymbol{1}(.)$ is the indicator function. The Winkler score rewards narrow intervals that contain the observation, and penalises intervals that do not contain the observation.

\textcolor{blue}{\textit{Continuous ranked probability score (CRPS).}} The CRPS is defined as the squared difference between the predictive cumulative distribution function (CDF) $P$ and the empirical CDF of the observation $y$:

$$
CRPS(P, y) = \int_{-\infty}^{\infty} (P(x) - \boldsymbol{1}(x \geq y))^2 dx.
$$

When the predictive distribution is Gaussian with mean $\mu$ and standard deviation $\sigma$, the CRPS has a closed-form expression:

$$
CRPS(P, y) = \sigma \left[ z \left( 2 \Phi(z) -1 \right) + 2 \phi(z) - \frac{1}{\sqrt{\pi}} \right],
$$

where $z = \frac{y - \mu}{\sigma}$, and $\Phi(.)$ and $\phi(.)$ are the CDF and probability density function (PDF) of a standard normal distribution, respectively.

\textcolor{blue}{\textit{Energy score (ES).}} The energy score is a multivariate generalisation of the CRPS. It is defined as:

$$
ES(P, \boldsymbol{y}) = \mathbb{E}_P ||\boldsymbol{X} - \boldsymbol{y}||^{\beta} - \frac{1}{2} \mathbb{E}_P ||\boldsymbol{X} - \boldsymbol{X}'||^{\beta},
$$

where $\boldsymbol{X}$ and $\boldsymbol{X}'$ are independent random vectors with distribution $P$, $\boldsymbol{y}$ is the observed vector, $||.||$ is the Euclidean norm, and $\beta \in (0, 2]$. We set $\beta = 1$ following common convention.

Since the closed-form expression of the ES may not available, we approximate it using Monte Carlo samples $\{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_M\}$ drawn from $P$:

$$
\widehat{ES}(P, \boldsymbol{y}) = \frac{1}{M} \sum_{m=1}^M ||\boldsymbol{x}_m - \boldsymbol{y}|| - \frac{1}{2M(M-1)} \sum_{m=1}^M ||\boldsymbol{x}_m - \boldsymbol{x}^*_m||,
$$

where $\boldsymbol{x}^*_m$ is a randomly selected sample from $\{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_M\} \setminus \{\boldsymbol{x}_m\}$. In our experiments, we use $M=10000$ samples to approximate the ES.



# Simulation {#sec-simulation}

## General Design Framework {-}

The general design of data generating process for bottom-level series is a stationary VAR(1) process, with the following structure:

$$
\boldsymbol{b}_t = \boldsymbol{A} \boldsymbol{b}_{t-1} + \boldsymbol{\epsilon}_t,
$$

where $\boldsymbol{A}$ is a $n_b \times n_b$ block diagonal matrix of autoregressive coefficients $\boldsymbol{A} = diag(\boldsymbol{A}_1, \ldots, \boldsymbol{A}_m)$, with each $\boldsymbol{A}_i$ being a $n_{b,i} \times n_{b,i}$ matrix. The block diagonal structure ensures that the time series are grouped into $m$ groups, with each group having its own autoregressive coefficients. This aim to simulate the interdependencies between the time series within each group, where reconciliation will be expected to better performed than the usual base forecasts.

The model is added with a Gaussian innovation process $\boldsymbol{\epsilon}_t$, with covariance matrix $\boldsymbol{\Sigma}$. The covariance matrix $\boldsymbol{\Sigma}$ is generated specifically using the Algorithm 1 in @Hardin2013-wu:

1.  A compound symmetric correlation matrix is used for each block of size $n_{b,i}$ in $\boldsymbol{A}_i$, where the entries $\rho_i$ for each block $i$ are sampled from a uniform distribution between 0 and 1. They are baseline correlations within group.

2.  A constant correlation, which is smaller than $\text{min} \{\rho_1, \rho_2, \dots, \rho_m \}$, is imposed on the entries between different blocks. It serves as baseline correlations between group.

3.  The entry-wise random noise is added on top of the entire correlation matrix.

4.  The covariance matrix $\boldsymbol{\Sigma}$ is then constructed by uniform sampling of standard deviations, in a range of $[\sqrt{2}, \sqrt{6}]$, for all $n_b$ series.

We will randomly flip the signs of the covariance elements, which will create a more realistic structure in the innovation process. This can be done by pre- and post-multiplying $\boldsymbol{\Sigma}$ by a random diagonal matrix $\boldsymbol{V}$ with diagonal entries sampled from $\{-1, 1\}$, yielding $\boldsymbol{\Sigma}^* = \boldsymbol{V} \boldsymbol{\Sigma} \boldsymbol{V}$.

For all hierarchies in our experiments, we simulate two panel lengths, $T=54$ and $T=304$, reserving the final four observations as an out-of-sample test set. In each Monte Carlo replication ($M=500$), we fit univariate ARIMA models (base models) to the training observations using an automatic AICc minimization algorithm from @Hyndman2008-rf, implemented in the *fabletools* package [@O-Hara-Wild2024-we], generating incoherent 1–4-step base forecasts. We then reconcile these forecasts under three covariance estimators: the raw sample covariance (mint_sample), the shrinkage estimator (mint_shr), and the NOVELIST estimator (mint_n).

All data generation, covariance estimation, and reconciliation routines were implemented in the ReconCov R package and is available under an open‐source license on GitHub [@ReconCov].

```{r load results}
path_sim <- "D:/Github/Recon_Honours_Analysis/sim/thesis_sim/"

sim1_1 <- readRDS("./literature/lib/S4-2-1_T50_M500.rds")
sim1_2 <- readRDS("./literature/lib/S4-2-1_T300_M500.rds")

sim2_1 <- readRDS("./literature/lib/S36-6-1_T50_M500.rds")
sim2_2 <- readRDS("./literature/lib/S36-6-1_T300_M500.rds")

sim_dense_50 <- readRDS("./literature/lib/S100-10-3-1_T50_M200_dense.rds")
sim_dense_300 <- readRDS("./literature/lib/S100-10-3-1_T300_M200_dense.rds")

sim_sparse_50 <- readRDS("./literature/lib/S100-10-3-1_T50_M500_sparsebwgrp.rds")
sim_sparse_300 <- readRDS("./literature/lib/S100-10-3-1_T300_M200_sparsebwgrp.rds")

sim3_1 <- sim_dense_50 # using same with next section simulation
sim3_2 <- sim_dense_300
```


## Exploring Effects of Hierarchy's Size

In our first set of experiments, we examine how MinT combined with the different estimators perform as the hierarchy expands. We generate synthetic data from the same VAR(1) framework described earlier, but vary the number of bottom‐level series, $n_b$, across two structures: a small structure with six groups of six bottom series ($n_b = 6$x$6 = 36$), and a much larger configuration with two groups of fifty ($n_b = 2$x$50 = 100$).

In the 36‐series case, each block of six forms a level‐1 aggregate, and those six aggregates form the national total. The 100‐series design employs a deliberately intricate aggregation path to stress‐test reconciliation methods. We first sum the one hundred bottom series into ten intermediate series by grouping them in contiguous blocks of ten. These ten series are then organised into three level-2 aggregates—four, three, and four series, respectively—before finally summing to a single top node. This asymmetric hierarchy creates overlapping correlation patterns: some level‐2 series share bottom‐level groups, while others draw from both, emulating practical scenarios such as regional sales aggregations that span multiple product categories or overlapping territories. The aggregation paths for both structures are illustrated in @fig-sim-structures.

![Aggregation structures used in the simulation experiments: 6 groups of 6 (left) and 2 groups of 50 (right)](D:/Github/Recon_Honours_Thesis/figs/sim_structures.png){#fig-sim-structures}

The VAR(1) and correlation configurations for the 6 by 6 case and 2 by 50 case are illustrated in @fig-settings-6x6 and @fig-settings-2x50, respectively. The block diagonal structure of the VAR(1) coefficient matrices $\boldsymbol{A}$ reflects the grouping of series, and the correlation matrices $\boldsymbol{\Sigma}$ show higher correlations among series within the same group.

```{r generate plots for A and Sigma}
MSE1_1 <- transform_sim_MSE(sim1_1$MSE, F) |> 
  filter(.model != "ols")
MSE1_2 <- transform_sim_MSE(sim1_2$MSE, F) |> 
  filter(.model != "ols")

MSE2_1 <- transform_sim_MSE(sim2_1$MSE, F) |> 
  filter(! .model %in% c("mint_n_hstep", "ols"))
MSE2_2 <- transform_sim_MSE(sim2_2$MSE, F) |> 
  filter(! .model %in% c("mint_n_hstep", "ols"))

MSE3_1 <- transform_sim_MSE(sim3_1$MSE, F)
MSE3_2 <- transform_sim_MSE(sim3_2$MSE, F) |> 
  filter(.model != "ols")

p1_A <- plot_heatmap(sim1_1$A, T) + 
  ggtitle("2x2 VAR(1) Coefficients Matrix")

p1_Sigma <- plot_heatmap(cov2cor(sim1_1$Sigma), T) + 
  ggtitle("2x2 Correlation Matrix")

p2_A <- plot_heatmap(sim2_1$A) +
  ggtitle("VAR(1) Coefficients Matrix") + theme_void()

p2_Sigma <- plot_heatmap(cov2cor(sim2_1$Sigma), T) +
  ggtitle("Correlation Matrix") + theme_void()

p3_A <- plot_heatmap(sim3_1$A) +
  ggtitle("2x50 VAR(1) Coefficients Matrix")

p3_Sigma <- plot_heatmap(cov2cor(sim3_1$Sigma), T) +
  ggtitle("2x50 Correlation Matrix")
```

```{r settings 6x6}
#| label: fig-settings-6x6
#| fig-cap: "Heatmaps of the VAR(1) coefficient matrix and correlation matrix for the 6 by 6 structure."
#| fig.height: 4
#| fig.width: 6
#| out.width: "85%"
#| fig.align: "center"

p2_A + p2_Sigma + plot_layout(ncol = 2) & theme(legend.position = "bottom")
```

```{r settings 2x50}
#| label: fig-settings-2x50
#| fig-cap: "Heatmaps of the VAR(1) coefficient matrix and correlation matrix for the 2 by 50 structure."
#| fig.height: 12
#| fig.width: 8
#| out.width: "100%"
#| fig.align: "center"

p3_A + p3_Sigma + plot_layout(ncol = 1) & theme(legend.position = "right")
```


![Percentage relative improvement in MSE of reconciled forecasts over the base forecasts in the 6 by 6 case (top row) and the 2 by 50 case (bottom row), T=50 (left column) and T=300 (right column), for 1- to 4-step-ahead forecasts. The positive (negative) entries indicate a decrease (increase) in MSE relative to base.](D:/Github/Recon_Honours_Thesis/figs/sim_MSE.png){#fig-sim-results-1}

Figure @fig-sim-results-1 illustrates the relative improvements in mean squared error (MSE) of reconciled forecasts over the incoherent base forecasts, across two structures and time series lengths. The MinT with shrinkage (*MinT-S*) and its variants are colored in mint green, while MinT with NOVELIST (*MinT-N*) are in purple. The concrete lines represent the vanilla *MinT-S* and *MinT-N*; the dashed lines with dot points denote the PC-adjusted variants (e.g. *MinT-S(PC1)*, *MinT-N(PC2)*); and the dotted or dashed-dotted lines indicate the scaled variance and h-step-ahead residuals versions (e.g. *MinT-S(SV)*, *MinT-N(hcov)*).

The first key observation is that methods with shrinkage slightly outperform those with NOVELIST across all scenarios, despite the differences being small. Second, the PC-adjusted variants (using one and two principal components) do not yield improvements over the vanilla versions. This is expected since the synthetic data generating process does not simulate from strong latent factors. Third, the scaled variance and h-step-ahead residuals approaches do not enhance performance as the forecast horizon increases, suggesting that the proportionality assumption may not be severely violated in this VAR(1) setup. Lastly, as the hierarchy expands from 36 to 100 series, forecasts from NOVELIST methods become worse relative to base forecasts when we move to $2$-step-ahead and beyond. When looking into each MCMC replication, we find that there are instances where the NOVELIST estimator collapses to the shrinkage estimator due to a large optimal threshold $\hat{\delta}^*$ being selected in the cross-validation step. 

![Percentage relative improvement in Energy score in both hierarchies and time dimensions, for 1-step-ahead forecasts. The positive entries indicate a decrease in Energy score relative to base.](D:/Github/Recon_Honours_Thesis/figs/sim_energy.png){#fig-sim-results-2}

Moving on to probabilistic forecasts, we evaluate the performance of reconciliation methods using the energy score [@Gneiting2011-km], a proper scoring rule for multivariate distributions. Figure @fig-sim-results-2 presents the percentage relative improvement in energy score for 1-step-ahead forecasts. Across both hierarchies and time dimensions, the MinT methods consistently outperform the base forecasts. Meanwhile, the differences among *MinT-S* and *MinT-N* variants are insignificant, except for the 6 by 6 case with 50 observations, where shrinkage has a slight edge. The PC-adjusted variants again degrade performance as we add more principal components. The scaled variance and h-step-ahead residuals approaches are not available since we only evaluate 1-step-ahead covariance estimates.

## Exploring the sparsity of the DGP covariance matrix

\color{red}
to be revised for final draft
\color{black}

```{r}
#| label: fig-Sigma-2x50
#| fig-cap: "Heatmaps of the dense and sparse correlation matrix of the data generating process."
#| fig.height: 12
#| fig.width: 8
#| out.width: "100%"
#| fig.align: "center"
p_dense_Sigma <- sim_dense_50$Sigma |>
  cov2cor() |> 
  plot_heatmap(T) +
    labs(title = "Dense")

p_sparse_Sigma <- sim_sparse_50$Sigma |>
  cov2cor() |> 
  plot_heatmap(T) +
    labs(title = "Sparse")

p_dense_Sigma + p_sparse_Sigma + 
  plot_layout(ncol = 1, guides = "collect", axis_titles = "collect") & 
  theme(legend.position = "right")
```

```{r}
#| label: fig-sim-results-dsp
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the 2x50 hierarchical structure with dense and sparse DGP's correlation matrix, for 1 to 4 steps ahead forecasts, with 2 time series lengths (T = 54 and T = 304)."
#| fig.height: 7
#| fig.width: 8
#| out.width: "90%"
#| fig.align: "center"
p_dense_50 <- sim_dense_50$MSE |>
  transform_sim_MSE(F) |> 
  plot_relative_improvement() +
    labs(title = "Dense DGP Covariance Matrix", subtitle = "T = 54")
p_sparse_50 <- sim_sparse_50$MSE |>
  transform_sim_MSE(F) |> 
  plot_relative_improvement() +
    labs(title = "Dense DGP Covariance Matrix")

p_dense_300 <- sim_dense_300$MSE |>
  transform_sim_MSE(F) |> 
  filter(.model != "ols") |> 
  plot_relative_improvement() +
    labs(subtitle = "T = 304")
p_sparse_300 <- sim_sparse_300$MSE |>
  transform_sim_MSE(F) |> 
  plot_relative_improvement()

p_dense_50 + p_sparse_50 + p_dense_300 + p_sparse_300 +
  plot_layout(ncol = 2, guides = "collect", axis_titles = "collect") &
  theme(legend.position = "right")
```


In our second simulation study, we design a data‐generating process that contrasts “dense” and “sparse” correlation regimes among bottom‐level series, reflecting settings one might encounter in practice. We consider the same hierarchical structure of two large groups of 50 bottom series as above. Specifically, the two groups would have strong within-group dependencies throughout and either modest between‐group correlations (the dense scenario) or complete independence (the sparse scenario). These correlation matrices are depicted in Figure @fig-Sigma-2x50. Both scenarios share the same VAR(1) coefficient structure as in our previous simulations; only the innovation covariance changes. Such a setup mirrors real‐world contexts where, for example, sales within a product line may exhibit strong co‐movements, while those in a separate line operate nearly independently.

<!-- Rather than collapsing each group into a single aggregate as in previous practice, we impose a more intricate, multi‐level aggregation path to stress‐test reconciliation methods. We first form ten intermediate series by summing bottom‐level series in consecutive blocks of ten. These ten series are then assigned to three second‐level aggregates--groups of four, three, and four--and finally summed to produce a single top‐level series. This asymmetric hierarchy creates overlapping correlation patterns: some level‐2 series share bottom‐level groups, while others draw from both. -->

<!-- emulating practical scenarios such as regional sales aggregations that span multiple product categories or overlapping territories. -->

Figure @fig-sim-results-dsp presents out‐of‐sample mean squared error improvements over the base forecasts for each reconciliation strategy under both dense and sparse settings, with two panel lengths--54 and 304 observations--reserving the last four points for testing. In both short and long samples, MinT using either the shrinkage estimator (mint_shr) or the NOVELIST estimator (mint_n) delivers pronounced gains over incoherent ARIMA forecasts, particularly at the one‐step horizon where cross‐series correlations most directly inform the forecast adjustments. Although the two MinT variants perform almost indistinguishably overall, mint_n edges out mint_shr in the immediate horizon, whereas mint_shr slightly outperforms for longer horizons. By contrast, MinT with the raw sample covariance (mint_sample) suffers in small‐sample settings; as expected, its performance improves dramatically with 304 data points, since the sample covariance becomes more reliable with larger $n$. This highlights the practical necessity of regularized estimators in high‐dimensional, low‐sample contexts, a situation common in real applications where histories are short relative to the number of series.

Additional designs (varying block sizes, aggregation paths, correlation configurations) also failed to separate NOVELIST from Shrinkage. Their nearly identical performance under these synthetic scenarios suggests that our current simulation may not unveil the full advantages of the thresholding estimators. This finding motivates our turn to empirical data, where latent structural features and regime shifts, which we will discuss in the next section, may reveal performance differences.



# Forecasting Australian Domestics Tourism {#sec-empirical}

```{r}
tourism_plots <- readRDS("D:/Github/Recon_Honours_Thesis/literature/tourism_plots.rds")
```

Domestic tourism flows in Australia exhibit a natural hierarchical and grouped structure, driven both by geography and by purpose of travel. At the top of this hierarchy lies the national total, which splits into the seven states and territories. Each state is further sub-divided into tourism zones, which in turn break down into 77 regions. A complete illustration of this geographic hierarchy appears in Appendix @sec-tourism-supplement. Intersecting this geographic hierarchy is a second dimension--travel motive--partitioning tourism flows into four categories: holiday, business, visiting friends and relatives, and other. Altogether, this yields a grouped system of 560 series, from the most disaggregated regional-purpose cells up to the full national aggregate. @tbl-tourism-structure depicts this structure.

```{r}
#| tbl-cap: "Hierarchical and grouped structure of Australian domestic tourism flows"
#| label: tbl-tourism-structure

num_series_geo <- c(1, 7, 27, 77)
table_structure <- tibble(
  `Geographical division` = c("Australia", "States", "Zones", "Regions"),
  `Number of series per geographical division` = num_series_geo,
  `Number of series per purpose` = 4*num_series_geo,
  `Total number of series` = 5*num_series_geo
)
# add total per row
table_structure <- rbind(
  table_structure,
  c("Total", colSums(table_structure[,-1]))
)

# width margin smaller (90%)
table_structure |> 
  knitr::kable(
    align = "lccc",
    booktabs = TRUE
  )
```

We quantify tourism demand via "visitor nights", the total number of nights spent by Australians away from home. The data is collected via the National Visitor Survey, managed by Tourism Research Australia, using computer assisted telephone interviews from nearly 120,000 Australian residents aged 15 years and over [@tourism-au].

The data are monthly time series spanning from January 1998 to December 2016, resulting in 228 observations per series, producing a canonical "$n \ll p$" setting which is ideal for evaluating reconciliation approaches that rely on high-dimensional covariance estimation. The extreme dimensionality over sample size mirrors many contemporary business problems, for instance, Starbucks drink sales. Tourism demand is also economically vital yet highly volatile, with geographical and purpose‑specific patterns create a realistic stress‑test for reconciliation algorithms.

<!-- Consequently, this panel offers both a rich policy case study and a stringent statistical laboratory for comparing reconciliation strategies that exploit cross-series information to improve forecasts when historical data are scarce. -->

<!-- This dataset is particularly well‑suited to evaluating forecast‑reconciliation methods for three reasons. First, tourism demand is economically important and notoriously volatile; the heterogeneous regional and purpose‑specific patterns create a realistic stress‑test for reconciliation algorithms. Second, the clear structural shift towards stronger growth after 2016—visible in most aggregates—necessitates techniques capable of handling possible breaks or time‑varying error structures, making the exercise practically relevant. Third, the extreme dimensionality relative to sample size mirrors many contemporary business problems (e.g., SKU‑level retail sales), allowing insights to generalise beyond tourism. Using this panel therefore provides both a compelling policy context and a stringent statistical laboratory for comparing alternative reconciliation strategies, especially those that rely on regularised covariance estimation or exploit cross‑sectional information to stabilise forecasts when historical data are scant. -->

@Wickramasuriya2019-xq also argued that modelling spatial autocorrelations directly from the start would be challenging as in this case of a large collection of time series. Post-processing reconciliation approaches have the advantage to implicitly model this spatial autocorrelation structure, especially true for MinT.

![Rolling-window cross-validation scheme for evaluating forecasting performance in Australia tourism data](D:/Github/Recon_Honours_Thesis/figs/tourism_cv.png){#fig-tourism-cv width="70%"}

To assess forecasting performance between models, we adopt a rolling-window cross-validation scheme. Beginning with the first 120 monthly observations (January 1998-December 2005) as the initial training set, we obtain the best-fitted ARIMA model for each of the 560 series via the automatic algorithm by minimising AICc from @Hyndman2008-rf, implemented in the *fabletools* package [@O-Hara-Wild2024-we]. The 1- to 12-step-ahead base forecasts are then generated by these ARIMA models, and then reconciled using multiple approaches. To estimate the NOVELIST and its variants, we would have an extra cross-validation prodecure within this training window, as described in @sec-novelist_cv. We then roll the training window forward by one month and refit all models, rebuild reconciliations, and produce another batch of 1- to 12-step-ahead forecasts, repeating until the training set reaches December 2015. In total, this results in 97 out-of-sample windows. The entire procedure is illustrated in @fig-tourism-cv.


<!-- 
Should MinT-sample be included in the paper?

Note that the number of series is larger than the number of observations (560 compared to 96), hence the sample covariance matrix is not positive definite and will not be considered.
-->


```{r}
#| label: fig-tourism-MSE-line
#| fig.height: 6
#| fig.width: 8
#| fig-cap: "Percentage relative improvement in the mean squared error (MSE) of different reconciled forecasts over the base forecasts for the Australian domestic tourism data, for 1 to 12 steps ahead forecasts. The positive entries indicate an decrease in MSE."
tourism_plots$MSE_line + 
  labs(title = NULL, subtitle = NULL) + 
  theme(
    legend.position = "bottom", 
    legend.direction = "vertical", 
    legend.title.position = "left"
  ) + 
  scale_x_continuous(breaks = c(1:12), minor_breaks = NULL)
```

```{r}
#| label: fig-tourism-MSE-heat
#| fig.height: 5
#| fig.width: 7
#| out.width: "90%"
#| fig-cap: "Heatmap of relative improvement in the mean squared error (MSE) of different reconciled forecasts over the base forecasts for the Australian domestic tourism data, for 1 to 12 steps ahead forecasts. The values are scaled to the range of 0 to 100 for better visualisation, with darker colors indicating greater improvement and best performance is noted by a star."

tourism_plots$MSE_heat + 
  labs(title = NULL, subtitle = NULL) + 
  theme(legend.title = element_blank(), legend.position = "bottom") + 
  scale_x_continuous(breaks = c(1:12), minor_breaks = NULL)
```

<!-- @fig-tourism-MSE-line and @fig-tourism-MSE-heat show that reconciliation is beneficial for point forecasts: across horizons $h=1,2,\dots,12$, all reconciled methods improve MSE over incoherent base ARIMA on average. Among vanilla MinT variants, the shrinkage estimator (*MinT-S*) marginally outperforms NOVELIST (*MinT-N*) at most horizons, though the gap is small. Adjusting for a single dominant factor via PC decomposition tightens performance further: both *MinT-S(PC1)* and *MinT-N(PC1)* beat their unadjusted counterparts, with *MinT-N(PC1)* narrowly ahead. Adding more than one PC brings no additional benefit, likely due to injecting estimation noise from weaker components. Variants that modify the multi-step covariance, either via scaled-variance or direct h-step residual covariances, underperform standard MinT, suggesting that extra estimation at horizon $h>1$ is not rewarded in this setting. -->


@fig-tourism-MSE-line show a main difference compared to the one of simulation results. Adjusting for a single dominant factor via PC decomposition (dashed line with dot points) tightens performance further: both *MinT-S(PC1)* and *MinT-N(PC1)* beat their unadjusted counterparts. Other than that, similar patterns are observed: among vanilla MinT variants, the shrinkage estimator (*MinT-S*) slightly outperforms NOVELIST (*MinT-N*) at most horizons; adding more than one PC brings no additional benefit, likely due to injecting estimation noise from weaker components; variants that modify the multi-step covariance, either via scaled-variance or direct h-step residual covariances, underperform standard MinT, suggesting that extra estimation at horizon $h>1$ is not rewarded in this empirical analysis. 

@fig-tourism-MSE-heat provides a complementary heatmap view, scaling each method's MSE improvement to a 0-100 range for better visual discrimination. Here, *MinT-N(PC1)* and *MinT-S(PC1)* emerge as the top performers across most horizons. The heatmap underscores the consistent gains from PC adjustment and highlights the diminishing returns from more complex covariance treatments.


<!-- 
```{r}
#| label: fig-tourism-probscores
#| fig.height: 12
#| fig.width: 12
#| fig-cap: "Percentage relative improvement in the Winkler score at 80% and 95% nominal coverage, CRPS, and Energy score of multiple reconciled forecasts over the base forecasts for the Australian domestic tourism data, for 1-step-ahead forecasts. The positive entries indicate an decrease in the probabilistic scores."

# a 2x2 showing 4 plots showing the 1-step-ahead % improv in Winkler score 80, 95, CRPS, and Energy

tourism_plots$W80 + labs(title = "Winkler score 80%", subtitle = NULL) + 
  tourism_plots$W95 + labs(title = "Winkler score 95%", subtitle = NULL) +
  tourism_plots$CRPS + labs(title = "CRPS", subtitle = NULL) +
  tourism_plots$Energy + labs(title = "Energy score", subtitle = NULL) +
  plot_layout(ncol = 2, guides = "collect", axis_titles = "collect") & 
  theme(legend.position = "bottom", legend.title = element_blank())

```
-->

![Percentage relative improvement in the Winkler score at 80% and 95% nominal coverage, CRPS, and Energy score of multiple reconciled forecasts over the base forecasts for the Australian domestic tourism data, for 1-step-ahead forecasts. The positive (negative) entries indicate a decrease (increase) in the probabilistic scores relative to base.](D:/Github/Recon_Honours_Thesis/figs/tourism_probscores.png){#fig-tourism-probscores}

<!-- Turning to probabilistic forecasts (1-step-ahead forecasts), @fig-tourism-probscores shows that *MinT-N* consistently outperforms *MinT-S* across CRPS and both Winkler scores, and PC1-adjustment again yields the best improvements. Results are aligned across probabilistic scoring rules, but the 95% Winkler reveals a notable pattern, as all MinT variants lose to the base while *OLS* performs best. In the multivariate evaluation, the Energy score places *OLS* close to the PC1-adjusted MinT methods, indicating that simple coherent projection might be able to capture a substantial share of cross-series dependence benefits even without sophisticated covariance regularisation. -->

Turning to probabilistic forecasts (1-step-ahead forecasts), @fig-tourism-probscores shows that *MinT-N* (purple bars) consistently outperforms *MinT-S* (green bars) across univariate and multivariate scores. The PC1-adjusted variants again yield improvements over their vanilla counterparts, with *MinT-N(PC1)* leading overall. In the multivariate evaluation, the Energy score places *OLS* close to the PC1-adjusted MinT methods. One surprising finding is that all MinT variants underperform the base forecasts when moving to the 95% Winkler score, while *OLS* performs best.

![Percentage relative improvement in Winkler score at 95% nominal coverage by level (left), and empirical coverage of 95% prediction intervals by level (right)](D:/Github/Recon_Honours_Thesis/figs/tourism_coverage_95.png){#fig-tourism-coverage-95-level}

To dissect the 95% Winkler score results, @fig-tourism-coverage-95-level breaks down performance by hierarchical level, and examines empirical coverage of the 95% prediction intervals. The left panel shows that *MinT-S* underperforms the base at all levels, especially in higher aggregated levels. From our inspection, this is due to the overly shrunk variances from the shrinkage estimator, leading to narrow prediction intervals and thus high Winkler penalties when observations fall outside. The *MinT-N* method has relatively good coverage and improve the Winkler score at bottom levels, but still underperforms at higher levels. The PC-adjusted variants seem to strike a better balance, improving overall coverage and relative Winkler scores over the base.

```{r}
#| label: fig-tourism-radar
#| fig.height: 8
#| fig.width: 8
#| out.width: "90%"
#| fig-cap: "Radar plot of relative improvements in probabilistic scores (Winkler 80, Winkler 95, CRPS, Energy) over the base forecasts. The scores are scaled to a range of 0 to 100, with larger values indicating better performance. The outermost polygon represents the best possible score (100) and the innermost polygon represents the worst possible score (0). Only the top 5 models are shown."


# a radar plot of 5 top models based on probabilistic scores (Winkler 80, 95, CRPS, Energy) scaled to 0-100 
par(mfrow=c(1,1), mar=c(0,0,0,0))
tourism_plots$radar
```

The summary radar graph in @fig-tourism-radar consolidates these findings: among the selected MinT models by probabilistic criteria, *MinT-N(PC1)* (the yellow polygon) clearly leads across CRPS, W80, W95, and Energy, extending the improvements seen in the single-metric panels.

Taken together, the evidence supports the use of reconciliation for both point and probabilistic forecasts in this high-dimensional setting. For point forecasts, MinT with shrinkage is a solid default; for probabilistic forecasts, NOVELIST performs more reliably. PC adjustment with a single dominant factor consistently enhances performance in both point and probabilistic forecast and should be considered when a dominant latent factor is evident. More complex adjustments, such as multiple PCs or horizon-specific covariances, add estimation noise without clear benefits and can be omitted for parsimony.

<!-- # Supplementary -->

\pagebreak

# Appendix {#sec-appendix}

## Appendix: Australian Domestic Tourism Geographical Hierarchy {#sec-tourism-supplement}

```{r}
#| tbl-cap: "Geographical divisions of Australia."
#| label: tbl-tourism-geo
#| warning: false
#| message: false

source(paste0(path, "R/table_geo.R"))
table_geo_labeled <- create_table_geo()
table_geo_labeled[1,1] <- "Total"

df <- table_geo_labeled %>% 
  select(Series, Name, Label)

n  <- ceiling(nrow(df) / 2)
L  <- df[1:n, ]
R  <- df[(n+1):nrow(df), ]

# pad the shorter side
if (nrow(R) < nrow(L)) {
  R <- bind_rows(R, tibble(Series = rep(NA_integer_, n - nrow(R)),
                           Name = "", Label = ""))
}

wide <- bind_cols(
  setNames(L, c("Series","Name","Label")),
  setNames(R, c("Series","Name","Label"))
)

kable(
  wide,
  align     = c("r","l","l","r","l","l"),
  col.names = c("Series","Name","Label","Series","Name","Label")
) |> 
  kable_styling(latex_options = "hold_position", font_size = 8)
```

\pagebreak

# References
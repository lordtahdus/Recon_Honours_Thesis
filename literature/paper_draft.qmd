---
title: "Enhancing Forecast Reconciliation: A Study of Alternative Covariance Estimators"
author: 
  - Vincent Su
  - Shanika Wickramasuriya (supv.)
  - George Athanasopoulos (supv.)
format: 
  pdf:
    include-in-header:
      text: |
        <!-- \usepackage{amsmath} -->
        <!-- \usepackage{mathspec} -->
        \usepackage{algorithm}
        \usepackage{algpseudocode}
        \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
        \usepackage{array}
    mathspec: true
    number-sections: true
# crossref:
#   custom:
#     - kind: float
#       key: alg
#       reference-prefix: Algorithm
#       caption-prefix: Algorithm
#       # for PDF output only, tell LaTeX to use the `algorithm` env:
#       latex-env: algorithm
execute:
  cache: false
  echo: false
  warning: false
bibliography: D:/Github/Recon_Honours_Thesis/references.bib
# bibliographystyle: apa
csl: D:/Github/Recon_Honours_Thesis/apa.csl
editor_options: 
  chunk_output_type: console
---

<!-- # Notation {-} -->

<!-- -   Scalar $y_t$ -->

<!-- -   Vector $\boldsymbol{y}_t$ -->

<!-- -   Matrix $\boldsymbol{S}$ -->

<!-- -   Covariance matrix of in-sample 1-step-ahead base forecast errors $\hat{\boldsymbol{W}}_1$ -->

<!-- -   Its shrinkage estimator with diagonal target $\hat{\boldsymbol{W}}^{shr}_{1, D}$ -->

<!-- -   Its NOVELIST estimator $\hat{\boldsymbol{W}}^{shr}_{1, thr}$ -->

```{r library}
library(ggplot2)
library(patchwork)
library(ReconCov)
library(dplyr)
library(tidyr)
library(fable)
library(fabletools)
library(feasts)
library(tsibble)
library(lubridate)

path <- "D:/Github/Recon_Honours_Thesis"
knitr::opts_knit$set(root.dir = path)
# setwd(path)
```


# Abstract {-}

<!-- [This is pasted from the Project Description]{style="color:red;"} -->

A collection of time series connected via a set of linear constraints is known as hierarchical time series. Forecasting these series without respecting the hierarchical nature of the data can lead to incoherent forecasts across aggregation levels and lower accuracy. To mitigate this issue, various forecast reconciliation approaches have been proposed in the literature, where the individual forecasts are adjusted to satisfy the aggregation constraints. Among these, **MinT** (Minimum Trace) is widely used, however, it requires a good estimate of the covariance matrix of the base forecast errors. The current practice is to use the shrinkage estimator (often shrinking toward a diagonal matrix), but it lacks flexibility and might not fully utilise the prominent latent structure presented. In this project, we aim to assess the forecasting performance of MinT when different covariance estimators are used, namely NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance), POET (Principal Orthogonal complEment Thresholding), and others.

# Introduction

<!-- ############  Intro  ############ -->

<!--
Para 1
- Introduce hierarchical time series and its importance in various applications.
-->

In time series forecasting, aggregation occurs in a variety of settings. While a formal definition of hierarchical time series can be found in @sec-hierarchy, we can think of Starbucks sales data as an illustrative example. Starbucks operates in many countries, and each country has multiple cities where they have outlets. The sales data is *structured hierarchically*: the top level is the total sales across all countries, followed by national sales for each country, and then individual sales for each outlet in a city. As a result, there are over 40,000 individual outlet sales to forecast, plus additional series at higher levels of aggregation such as city and country. The hierarchy can be even more complex if we consider the sales of different kinds of drinks (e.g., coffees, teas, refreshers) at each aggregation level.

Forecasting data from such hierarchical structures also arises in many other decision-making contexts, from supply chains [@Seaman2022-bb; @Angam2025-od] and energy planning [@Di-Modica2021-ad], to macroeconomics [@El-Gemayel2022-hx; @Li2019-vv] and tourism analysis [@Athanasopoulos2009-lp]. Stakeholders in these settings need forecasts at several aggregation levels to allocate resources and manage risk. The impact of methods for forecasting hierarchical time series has not been limited to academia, with industry also showing a strong interest. Many companies and organisations have adopted these methods in practice, including Amazon, the International Monetary Fund, IBM, SAP, and more [@Athanasopoulos2024-as].


<!-- ##########  Literature Review  ########## -->

<!-- - Talk about the history and evolution of forecasting hierarchical time series, starting from the early heuristic methods to the modern statistical approaches.

  - Single level methods
  - Optimal combination methods (OLS, WLS)
  - MinT
  - Bayesian, Machine learning
  - Probabilistic methods -->

In practice, when forecasts are produced for all series (often called *base forecasts*), they typically violate the aggregation constraints observed in the data; such forecasts are *incoherent*. This can undermine downstream decisions that require internal consistency.

Traditionally, forecasting these hierarchical time series has been done using single-level methods, such as bottom-up, top-down, and middle-out approaches. Bottom-up methods involve generating forecasts for the bottom-level series and aggregating them to higher levels. Top-down methods start with forecasts for the only top-level series and disaggregate them down. Middle-out methods combine both approaches by forecasting middle-level series and then aggregating or disaggregating as needed. Despite their simplicity, these methods only anchor forecasts to a single level, implying a large loss of information on the hierarchy's inherent correlation structure. Additionally, the most disaggregated series often are very noisy or even intermittent, and the higher-level data might be smoother due to the aggregation. Furthermore, as we saw from the Starbucks example considering the sales of different kinds of drinks at each aggregation level -- formally defined as grouped structure in @sec-hierarchy -- the disaggregation becomes more complex since the disaggregation paths are not unique. Consequently, these single-level methods tend to give poor results across other levels of the hierarchy.

To overcome these issues, forecast reconciliation was introduced by @Hyndman2011-jv, and later developed by @Van_Erven2015-nx, @Hyndman2016-ic, @Ben-Taieb2019-yx, @Wickramasuriya2019-xq, and others to achieve coherency in point forecasts and enhance accuracy. Forecast reconciliation projects a collection of independent base forecasts into a set of coherent forecasts that respect the linear constraints defining a hierarchical or grouped time-series system.

Among the modern reconciliation strategies, the Min Trace (MinT) approach developed by @Wickramasuriya2019-xq is widely used and perform significantly well under right conditions. Despite its properties, MinT relies on a good high-dimensional covariance estimator, which is positive-definite. However, not many researchers have explored this issue in the current literature, except @Carrara2025-rz, who introduced a new estimator for MinT--Double Shrinkage estimator. The current stage of this research piece is fairly early, leaving many potential gaps still not yet to explore. As a result, a study of alternative high-dimensional covariance estimators for MinT is timely and desirable. 

The remainder of the paper is organised as follows. @sec-pre-theory provides the basic theoretical framework for hierarchical time series and forecasting, and Min Trace approach, introducing notations, terminologies, and motivations for alternative estimators. @sec-cov-est walks through the main covariance estimators this paper explores, and argues their strengths and weaknesses. @sec-simulation covers the simulation design and currently explores the performance of NOVELIST on MinT. @sec-empirical shows a real-world application of MinT with NOVELIST, which produces results that did not occured in the simulation settings, suggesting further inspection and analysis.


<!-- Include more details on the gaps for each of these methods, such as bias, high variance? -->
  

  
<!-- - Discuss the interests in MinT and how it has become a standard method for forecast reconciliation.

- Discuss the MinT's reliance on a good estimate of the covariance matrix of base forecast errors and other gaps

  - Empirical evidence of MinT under perform
  - Comparison with other methods

- Explain why this paper focus on exploring alternative covariance estimators for MinT. And is there any paper talk about this.

  - Is better estimate of W_h really lead to better performance?

- Talk about the paper outline and what will be covered in the following sections. -->


<!--
-   **Problem Statement:**

    The sample covariance matrix, although natural, suffers in high-dimensional settings. Especially when the number of series $p$ is huge and larger than the time dimension $T$, the sample covariance matrix is non-positive definite (rank T if p\>T).

    The shrinkage estimators come in to tackle this issue. The shrinkage estimator with diagonal target (often shrinking toward a diagonal matrix) is proven to produce a guaranteed PD matrix (Schäfer & Strimmer, 2005). However, as it shrinks the covariance matrix toward a diagonal one, it does not have flexibility and might neglect the prominent structure presented in the covariance matrix.

    An alternative approach is to perform shrinkage of the sample covariance towards its thresholded version, instead of a diagonal matrix. This is the NOVELIST (NOVEL Integration of the Sample and Thresholded covariance estimators) method proposed by Huang & Fryzlewicz (2019). They introduced thresholding functions applied only to off-diagonal elements, allowing for more flexibility in the estimation.

    [... can include more estimators ...]{style="color:red;"}

-   **Research Aim:**

    This paper assesses the reconciled forecasting performance of MinT approach using various covariance estimators, with a focus on the NOVELIST estimator.

-   **Paper Outline:**

    The paper is structured as follows:

    -   A literature review of forecast reconciliation and covariance estimation.
    -   A description of the methodology, including the NOVELIST estimator and its principal-component-adjusted variant.
    -   An experimental design using both synthetic and real hierarchical time series.
    -   Empirical results and discussion.
    -   Conclusions and suggestions for future work.
-->


<!--
# Literature Review

## Forecast Reconciliation in Hierarchical and Grouped Time Series

Forecast reconciliation converts a collection of independent base forecasts into a set of coherent forecasts that respect the linear constraints defining a hierarchical or grouped time-series system. Early work focused on heuristic single-level strategies, including bottom-up, top-down, and middle-out (...), each of which exploits only part of the information in the hierarchy and can induce bias or high variance.

-   Cite the single level approach
-   Athanasopoulos et al., 2024

Hyndman et al. (2011) first showed that all single-level methods can be written as $\tilde{y} = SG\hat{y}$, where $S$ is the summing matrix and $G$ is a matrix that maps base forecasts $\hat{y}$ to into the bottom level. Treating reconciliation as a GLS regression problem, Hyndman et al. (2011) found that it yields a solution for $G$, but the required covariance matrix of reconcilation error is not identifiable in practice (Wickramasuriya et al., 2019).

-   (Talk more about how others transform it to OLS, WLS,..)
-   Di Fonzo and Marini (2011)
-   Athanasopoulos et al. (2009)

Wickramasuriya, Athanasopoulos & Hyndman (2019) reframed the problem by taking an optimisation approach rather than the regression. They formulated the problem as minimising the variances of all reconciled forecasts, which happens to be equivalent to minimising the trace of the covariance matrix (sum of the diagonal elements). This is known as the Minimum Trace (MinT) reconciliation method. The MinT solution is given by $G_h = (S'W_h^{-1}S)^{-1}S'W_h^{-1}$, and $W_h$ is the covariance matrix of the h-step-ahead base forecast errors.

The MinT approach is an algebraical generalisation of the GLS, and the OLS and WLS methods are special cases of MinT when $W_h$ is a diagonal or identity matrix, respectively. However, the MinT solution hinges on a reliable estimate of the h-step-ahead base forecast error covariance $W_h$. In high-dimensional setting, the usual sample covariance matrix is unstable, thus we need alternative covariance estimators.

-   Structural Scaling, based only on the struc- ture of the hierarchy (Athanasopoulos et al., 2017)
-   Shrinkage estimators (Schäfer & Strimmer, 2005; Ledoit & Wolf, 2004)

Empirical evaluations have demonstrated that MinT with an appropriate covariance estimate often outperforms earlier methods in both simulation and real data studies.

Wickramasuriya et al. (2019) adopt the diagonal-target Ledoit–Wolf approach and show analytically that any scalar multiple $k_h$ from $W_h = k_hW_1$ suffices for point-forecast reconciliation.  However, simulation and empirical evidence suggest that ignoring cross-series error correlation can leave accuracy untapped, especially in highly-coupled systems (energy, retail SKUs).  Their subsequent work imposes non-negativity constraints via quadratic programming citeturn0file2 and studies bias-robust variants (MinT-U).

Empirical findings to date

Athanasopoulos et al. (2024) synthesise the accumulated evidence: MinT dominates single-level baselines in most studies; WLS or diagonal-shrinkage MinT is the current industry default; but gains plateau when off-diagonals are weak citeturn0file9.  Only a handful of papers explore richer covariance structures (e.g., block or factor shrinkage), and none benchmark them systematically against NOVELIST-type estimators.

## Covariance Estimation in High Dimensions

-   Limitations of the sample covariance matrix.
-   Estimators used by Wickramasuriya et al. (2019).
-   Shrinkage estimators:
    -   Diagonal shrinkage (e.g., Schäfer & Strimmer, Ledoit & Wolf).
    -   NOVELIST estimator and its Cross-validation & PC-adjusted variant.
    -   ...

## Relevance to Forecast Reconciliation

-   Discuss how covariance estimation affects MinT performance.
-   Identify research gaps.
-->


# Theoretical Framework {#sec-pre-theory}

## Hierarchical tructure {#sec-hierarchy}

The *hierarchical structure* can be represented as a tree, as shown in @fig-hierarchy. The top level of the tree represents the total value of all series, while the lower levels represent the series at different levels of disaggregation. These hierarchical structures naturally form *aggregation constraints* (child levels must sum up to parents). When there are attributes of interest that are crossed, such as the Starbucks drinks sales at any aggregation level (brand-wise, national, or outlet) is also considered by kinds of drinks (e.g., coffees, refreshers), the structure is described as a grouped time series. As illustrated in @fig-grouped, the aggregation or disaggregation paths are not unique.

![A 2-level hierarchical tree structure](D:/Github/Recon_Honours_Thesis/research_proposal/figs/hierarchical_structure.png){#fig-hierarchy width="40%"}

![A 2-level grouped structure, which can be considered as the union of two hierarchical trees with common top and bottom level series](D:/Github/Recon_Honours_Thesis/research_proposal/figs/grouped_structure.png){#fig-grouped width="90%"}

For simplicity, we refer to both of these structures as hierarchical structure, we will distinguish between them if and when it is necessary. Taken together, a collection of time series organised in a hierarchy and subject to aggregation constraints is refered as *hierarchical time series*. All hierarchical time series can be represented using matrix algebra:

$$
\boldsymbol{y}_t = \boldsymbol{S} \boldsymbol{b}_t,
$$

where $\boldsymbol{S}$ is a summing matrix of order $n \times n_b$ which aggregates the bottom-level series $\boldsymbol{b}_t \in \mathbb{R}^{n_b}$ to the series at aggregation levels above. The vector $\boldsymbol{y}_t \in \mathbb{R}^n$ contains all observations at time $t$. The summing matrix $\boldsymbol{S}$ for the tree structure in @fig-hierarchy is:

$$
\boldsymbol{S} = 
\left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\boldsymbol{I_4}}
\end{array}
\right].
$$

Assume we produce $h$-step-ahead base forecasts $\hat{\boldsymbol{b}}_{t+h|t}$ for the bottom-level series, obtained by any prediction methods, pre-multiplying them by $\boldsymbol{S}$ we get: 

$$
\tilde{\boldsymbol{y}}_{t+h|t} = \boldsymbol{S} \hat{\boldsymbol{b}}_{t+h|t} \, .
$$ {#eq-sb}

We refer to $\tilde{\boldsymbol{y}}_{t+h|t}$ as *coherent forecasts*, as they respect the aggregation constraints. We also refer to this way of obtaining coherent forecasts by summing the bottom-level forecasts as the bottom-up approach. However, generating forecasts this way is anchored only to prediction models at a single level, and will not be utilising the inherent information from other levels. This drawback applies to the top-down and middle-out approaches. For example, the bottom-level data can be very noisy or even intermittent, and the higher-level data might be smoother due to the aggregation.

Another issue with expressing reconciled methods as in @eq-sb is that it restricts the reconciliation to only single-level approaches. Thus, @Hyndman2011-jv suggested a generalised expression for all existing methods, which also provides a framework for new methods to be developed:

$$
\tilde{\boldsymbol{y}}_{t+h|t} = \boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_{t+h|t} \; ,
$$ {#eq-sgy}

for a suitable $n_b \times n$ matrix $\boldsymbol{G}$. $\boldsymbol{G}$ maps the base forecasts of all levels $\hat{\boldsymbol{y}}_{t+h|t}$ down into the bottom level, which is then aggregated to the higher levels by $\boldsymbol{S}$. The choice of $\boldsymbol{G}$ determines the composition of reconciled forecasts $\tilde{\boldsymbol{y}}_{t+h|t}$, and modern reconciliation methods are developed to estimate $\boldsymbol{G}$. 

<!-- including the OLS and WLS developed by @Hyndman2011-jv and @Hyndman2016-ic, respectively. A more detailed explanation of these methods can be found in the @sec-ols-wls Appendix. -->




## The Minimum Trace (MinT) Reconciliation

@Wickramasuriya2019-xq framed the problem as minimising the variances of all reconciled forecast errors $\text{Var}[y_{t+h} - \tilde{y}_{t+h|t}] = \boldsymbol{S} \boldsymbol{G} \boldsymbol{W}_h \boldsymbol{G}' \boldsymbol{S}'$, where $\boldsymbol{W}_h = \mathbb{E}( \hat{\boldsymbol{e}}_{t+h|t} \; \hat{\boldsymbol{e}}_{t+h|t}' )$ is the positive definite covariance matrix of the $h$-step-ahead base forecast errors. They showed that this is equivalent to minimising the trace of the reconciled forecast error covariance matrix (sum of the diagonal elements - the variances). The Minimum Trace (MinT) solution is given by

$$
\boldsymbol{G} = (\boldsymbol{S}' \boldsymbol{W}_h^{-1} \boldsymbol{S})^{-1}
\boldsymbol{S}' \boldsymbol{W}_h^{-1} .
$$

@Wickramasuriya2019-xq also showed that MinT is an algebraic generalisation of the GLS, and the OLS and WLS methods are special cases of MinT when $\boldsymbol{W}_h$ is an identity matrix $I_{n_b}$ and a diagonal matrix $\text{diag}( \boldsymbol{W}_h )$, respectively. In this paper, we place our main focus on the MinT method.

The MinT solution hinges on a reliable, positive-definite estimate of $\boldsymbol{W}_h$, which is challenging to estimate in high-dimensional setting. The sample covariance matrix is unstable and non-positive-definite when the number of series $n$ is huge and larger than the time dimension $T$. To tackle this issue, the original paper @Wickramasuriya2019-xq adopted the diagonal-target shrinkage estimator from @Schafer2005-yw, given by

$$
\hat{\boldsymbol{W}}^{shr}_{1} = \lambda_D \hat{\boldsymbol{W}}_{1, D} + (1 - \lambda_D) \hat{\boldsymbol{W}}_1 \, ,
$$

where $\hat{\boldsymbol{W}}_{1, D}$ is a diagonal matrix comprising the diagonal entries  $\text{diag}( \hat{\boldsymbol{W}}_1 )$. We refer to any $\lambda \in [0,1]$ as the shrinkage intensity parameter, the subscript specifies which estimator it belongs to. This approach shrinks the covariance matrix $\hat{\boldsymbol{W}}_1$ towards its diagonal matrix, meaning the off-diagonal elements are shrunk towards zero while the diagonal ones remain unchanged.

@Schafer2005-yw also proposed an estimate of the optimal shrinkage intensity parameter $\lambda_D$:

$$
\hat{\lambda}_D = \frac{\sum_{i \neq j} \widehat{Var}(\hat{r}_{ij})} 
{\sum_{i \neq j} \hat{r}_{ij}^2} \, ,
$$

where $\hat{r}_{ij}$ is the $ij$th element of $\hat{\boldsymbol{R}}_1$, the 1-step-ahead sample correlation matrix (obtained from $\hat{\boldsymbol{W}}_1$). The optimal estimate is obtained by minimising $MSE(\hat{\boldsymbol{W}}_1) = Bias(\hat{\boldsymbol{W}}_1)^2 + Var(\hat{\boldsymbol{W}}_1)$. More specifically, we trade the unbiasedness of the sample covariance matrix for a lower variance. 

However, the hierarchical time series data often exhibit a prominent principal components structure, which is not fully taken advantage. Taking an example of the Australian domestic overnight trips data set [@tourism-au], where the national trips are disaggregated into states and territories, and further into regions. We then fit ETS models to all series, using the algorithm from Fabletools R package [@O-Hara-Wild2024-we], and compute the one-step-ahead in-sample base forecast error covariance matrix $\hat{\boldsymbol{W}}_1$. The twenty largest eigenvalues of the covariance matrix are plotted in @fig-eigen. We can see that the point of inflexion occurs at the component with 5th largest eigenvalue, indicating a prominent principal components structure.

![Twenty largest eigenvalues of one-step-ahead in-sample base forecast error covariance, Australian domestic overnight trips](D:/Github/Recon_Honours_Thesis/research_proposal/figs/eigen_tourism.png){#fig-eigen width="70%"}

Additionally, the shrinkage estimator shrinks all off-diagonal elements towards zeros with equal weights $\lambda_D$. We might prefer to better preserve strong signals, and largely reduce the effects of small, noisy correlations. In the next sections, we will explore several options that take these two issues into account.



# Covariance Estimation Approaches {#sec-cov-est}

## NOVELIST Estimator

The NOVELIST (NOVEL Integration of the Sample and Thresholded Covariance) estimator, proposed by @Huang2019-ua, introduces a way to control the target matrix's sparsity, retaining strong correlations while discarding weak, noisy effects. NOVELIST offers more flexibility than the shrinkage estimator, which is useful when we believe that only a few variables are truly correlated. However, it does not guarantee to be positive definite.

The method is based on the idea of soft-thresholding the sample covariance matrix, then performing shrinkage towards this thresholded version. This introduces an extra parameter, the threshold $\delta$, which is used to control the amount of soft-thresholding. The NOVELIST estimator is given by:

$$
\hat{\boldsymbol{W}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{W}}_{1, \delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{W}}_1,
$$ {#eq-novelist-cov}

where $\hat{\boldsymbol{W}}_{1, \delta}$ is the thresholded version of $\hat{\boldsymbol{W}}_{1}$. By convenient setting, we can rewrite it in terms of correlation:

$$
\hat{\boldsymbol{R}}^{N}_{1} = \lambda_{\delta} \hat{\boldsymbol{R}}_{1,\delta} + (1 - \lambda_{\delta}) \hat{\boldsymbol{R}}_1,
$$ {#eq-novelist-cor}

In this setting, $\hat{\boldsymbol{R}}_{1,\delta}$ is the thresholded correlation matrix, where each element is regularised by:

$$
\hat{r}_{1,ij}^\delta = \text{sign}(\hat{r}_{1,ij}) \, 
\text{max}(|\hat{r}_{1,ij}| - \delta, \; 0),
$$ {#eq-soft-thr}

where $\delta \in [0,1]$ is the threshold parameter. For a given threshold $\delta$, @Huang2019-ua derived an analytical expression for the optimal shrinkage intensity parameter $\lambda(\delta)$ using Ledoit-Wolf's lemma [@Ledoit2003-qv], following similar logic to @Schafer2005-yw. It can be computed as:

<!-- - \textcolor{red}{@Huang2019-ua mentioned Ledoit-Wolf's lemma} -->

$$
\hat{\lambda}(\delta) = \frac{
  \sum_{i \neq j} \widehat{Var}(\hat{r}_{1,ij}) \; \boldsymbol{1}(|\hat{r}_{1,ij}| \leq \delta)
} {
  \sum_{i \neq j} (\hat{r}_{1,ij} - \hat{r}_{1,ij}^\delta)^2
},
$$ {#eq-lambda-thr}

where $\boldsymbol{1}(.)$ is the indicator function.

On the other hand, the optimal threshold $\delta^*$ does not have a closed-form solution, and is typically obtained by executing a rolling-window cross-validation procedure. The idea is to find the threshold $\hat{\delta^*}$, with the corresponding $\hat{\lambda}^*$ and $\hat{\boldsymbol{R}}^{N}_{1}(\hat{\delta^*}, \hat{\lambda}^*)$, that minimises the average out-of-sample 1-step-ahead reconciled forecast mean squared error over all windows. The formal algorithm is given in the @sec-novelist_cv Appendix. Although it is not required to fit forecasting models multiple times, the cross-validation procedure is still computationally expensive as it computes the NOVELIST estimator and perform reconciliation for each threshold value.

We also tested out minimising 1- to h-step-ahead MSE in the cross-validation procedure. Surprisingly, it returns almost the same best threshold parameter is in the 1-step-ahead case above. Note that when $\delta \in \bigl[ \text{max}_{i \neq j}|\hat{r}_{1,ij}|, \; 1 \bigr]$, the NOVELIST estimator collapses to the shrinkage estimator, and when $\delta = 0$, it becomes the sample covariance matrix.

<!-- ## POET

The POET (Principal Orthogonal complEment Thresholding) estimator, proposed by @Fan2013-jz, is another "sparse" + "non-sparse" covariance estimator. It takes the latent factors directly into its construction, and is appealing when there are common drivers in the time series within the hierarchy, as we saw in the Australian tourism example.

The POET method starts by decomposing the correlation matrix $\hat{\boldsymbol{R}}_1$ into a prominent principle components part (low-rank) and a orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$ (the correlation matrix after removing the first $K$ principal components). Then it applies thresholding to $\hat{\boldsymbol{R}}_{1, K}$. The POET estimator is given by:

$$
\hat{\boldsymbol{R}}^{K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + T(\hat{\boldsymbol{R}}_{1, K})
$$

where $\hat{\gamma}_k$ and $\hat{\boldsymbol{\xi}}_k$ are the $k$th largest eigenvalue and the corresponding eigenvector of the sample covariance matrix, respectively, and $T(.)$ is the thresholding function, which can be either soft-thresholding, hard-thresholding, or others. -->


<!-- ## PC-adjusted NOVELIST

This approach is best of both worlds, leveraging the strengths of both NOVELIST and POET. The PC-adjusted (Principal-Component-adjusted) NOVELIST overcomes the shortcomings of the current shrinkage estimator, taking prominent PCs into account while also offers extra flexibility. The idea is to apply the NOVELIST estimator to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$, and then add the principal components part back. The PC-adjusted NOVELIST estimator is formulated as:

$$
\hat{\boldsymbol{R}}^{N, K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + \hat{\boldsymbol{R}}^{N}_{1, K}
\; ,
$$

where $\hat{\boldsymbol{R}}^{N}_{1, K}$ is the NOVELIST estimator applied to the orthogonal complement part $\hat{\boldsymbol{R}}_{1, K}$. Similar to the NOVELIST estimator, $\hat{\boldsymbol{R}}^{N, K}_{1}$ is not guaranteed to be positive definite.

Methods to ensure positive definiteness of the NOVELIST estimator (and its PC-adjusted variant) will be explored and studied in the project. @Huang2019-ua proposed to diagonalise the NOVELIST estimator and replace any eigenvalues that fall under a certain small positive threshold by the value of that threshold. Alternatively, we can implement the algorithm of @Higham2002-te that computes the nearest positive definite matrix to a given matrix. -->

## PC-adjusted Estimator

This method takes the latent factors directly into its construction, and is appealing when there are common drivers in the time series within the hierarchy, as we saw in the Australian tourism example. It starts by decomposing the correlation matrix $\hat{\boldsymbol{R}}_1$ into a prominent principle components part (low-rank) and a orthogonal complement part $\hat{\boldsymbol{C}}^K_{1}$ (the correlation matrix after removing the first $K$ principal components). Then we can apply either shrinkage or NOVELIST estimator to $\hat{\boldsymbol{R}}_{1, K}$:

$$
\hat{\boldsymbol{R}}^{g, K}_{1} = \sum_{k = 1}^K \hat{\gamma}_k \hat{\boldsymbol{\xi}}_k \hat{\boldsymbol{\xi}}_k' + g(\hat{\boldsymbol{C}}^K_{1})
$$

where $g(.)$ is either the shrinkage or NOVELIST estimator, $\hat{\gamma}_k$ and $\hat{\boldsymbol{\xi}}_k$ are the $k$-th largest eigenvalue and the corresponding eigenvector of the sample covariance matrix, respectively. Similar to the NOVELIST estimator, $\hat{\boldsymbol{R}}^{*, K}_{1}$ is not guaranteed to be positive definite.


## Scaled Variance

Apply shrinkage or NOVELIST to the correlation matrix of 1-step-ahead base forecast errors, then scale it back to covariance matrix of h-step-ahead base forecast errors using the h-step-ahead standard deviations:

$$
\hat{\boldsymbol{W}}^{sv}_{h} = \boldsymbol{D}^{1/2}_h g(\hat{\boldsymbol{R}}_1) \boldsymbol{D}^{1/2}_h,
$$

where $\boldsymbol{D}_h = \text{diag}(\hat{\sigma}_{1,h}, \hat{\sigma}_{2,h}, \ldots, \hat{\sigma}_{n_b,h})$, and $\hat{\sigma}_{i,h}$ is the standard deviation of the $i$-th series' h-step-ahead base forecast errors. The asterisk $*$ indicates either shrinkage or NOVELIST estimator.


## Constructing from h-step-ahead residuals

Construct the sample covariance matrix directly from h-step-ahead base forecast errors, then apply either shrinkage or NOVELIST estimator:

$$
\hat{\boldsymbol{W}}^{g}_{h} = g(\hat{\boldsymbol{W}}_h)
$$

where $\hat{\boldsymbol{R}}_h$ is the sample correlation matrix of h-step-ahead base forecast errors, and $\circ$ denotes the Hadamard product (element-wise product). The asterisk $*$ indicates either shrinkage or NOVELIST estimator.




# Simulation Design {#sec-simulation}

<!-- -   Talk about description of the hierarchical time series data set (e.g., economic, financial, or synthetic data). -->
<!-- -   Characteristics such as dimensionality, frequency, and hierarchical structure. -->

<!-- **Experimental Design** -->

<!-- -   Design different cases of simulation studies and real-data experiments. -->
<!-- -   Metrics: Forecast accuracy (e.g., RMSE, MAE), reconciliation error reduction, and matrix stability... -->
<!-- -   Visualisation... -->


The general design of data generating process for bottom-level series is a stationary VAR(1) process, with the following structure:

$$
\boldsymbol{b}_t = \boldsymbol{A} \boldsymbol{b}_{t-1} + \boldsymbol{\epsilon}_t,
$$

where $\boldsymbol{A}$ is a $n_b \times n_b$ block diagonal matrix of autoregressive coefficients $\boldsymbol{A} = diag(\boldsymbol{A}_1, \ldots, \boldsymbol{A}_m)$, with each $\boldsymbol{A}_i$ being a $n_{b,i} \times n_{b,i}$ matrix. The block diagonal structure ensures that the time series are grouped into $m$ groups, with each group having its own autoregressive coefficients. This aim to simulate the interdependencies between the time series within each group, where reconciliation will be expected to better performed than the usual base forecasts.

The model is added with a Gaussian innovation process $\boldsymbol{\epsilon}_t$, with covariance matrix $\boldsymbol{\Sigma}$. The covariance matrix $\boldsymbol{\Sigma}$ is generated specifically using the Algorithm 1 in @Hardin2013-wu:

1. A compound symmetric correlation matrix is used for each block of size $n_{b,i}$ in $\boldsymbol{A}_i$, where the entries $\rho_i$ for each block $i$ are sampled from a uniform distribution between 0 and 1. They are baseline correlations within group.

2. A constant correlation, which is smaller than $\text{min} \{\rho_1, \rho_2, \dots, \rho_m \}$, is imposed on the entries between different blocks. It serves as baseline correlations between group.

3. The entry-wise random noise is added on top of the entire correlation matrix.

4. The covariance matrix $\boldsymbol{\Sigma}$ is then constructed by uniform sampling of standard deviations, in a range of $[\sqrt{2}, \sqrt{6}]$, for all $n_b$ series.

We will randomly flip the signs of the covariance elements, which will create a more realistic structure in the innovation process. This can be done by pre- and post-multiplying $\boldsymbol{\Sigma}$ by a random diagonal matrix $\boldsymbol{V}$ with diagonal entries sampled from $\{-1, 1\}$, yielding $\boldsymbol{\Sigma}^* = \boldsymbol{V} \boldsymbol{\Sigma} \boldsymbol{V}$.

For all hierarchies in our experiments, we simulate two panel lengths, $T=54$ and $T=304$, reserving the final four observations as an out-of-sample test set. In each Monte Carlo replication ($M=500$), we fit univariate ARIMA models (base models) to the training observations using an automatic AICc minimization algorithm from the fabletools package [@O-Hara-Wild2024-we], generating incoherent 1–4-step base forecasts. We then reconcile these forecasts under three covariance estimators: the raw sample covariance (mint_sample), the shrinkage estimator (mint_shr), and the NOVELIST estimator (mint_n).

All data generation, covariance estimation, and reconciliation routines were implemented in the ReconCov R package, a package developed solely for this research, and is available under an open‐source license on GitHub [@ReconCov].

```{r load results}
sim1_1 <- readRDS("./literature/lib/S4-2-1_T50_M500.rds")
sim1_2 <- readRDS("./literature/lib/S4-2-1_T300_M500.rds")

sim2_1 <- readRDS("./literature/lib/S36-6-1_T50_M500.rds")
sim2_2 <- readRDS("./literature/lib/S36-6-1_T300_M500.rds")

# sim3_1 <- readRDS("./literature/lib/S100-2-1_T50_M200_run1.rds")
# sim3_2 <- readRDS("./literature/lib/S100-2-1_T100_M200_run1.rds")

sim_dense_50 <- readRDS("./literature/lib/S100-10-3-1_T50_M200_dense.rds")
sim_dense_300 <- readRDS("./literature/lib/S100-10-3-1_T300_M200_dense.rds")

sim_sparse_50 <- readRDS("./literature/lib/S100-10-3-1_T50_M500_sparsebwgrp.rds")
sim_sparse_300 <- readRDS("./literature/lib/S100-10-3-1_T300_M200_sparsebwgrp.rds")

sim3_1 <- sim_dense_50 # using same with next section simulation
sim3_2 <- sim_dense_300
```

## Exploring Effects of Hierarchy's Size

In our first set of experiments, we examine how MinT combined with the NOVELIST estimator scales as the hierarchy expands. We generate synthetic data from the same VAR(1) framework described earlier, but vary the number of bottom‐level series, $n_b$, across three different structures: the smallest with two groups of two ($n_b = 4$), an intermediate case with six groups of six ($n_b = 36$), and a much larger configuration with two clusters of fifty ($n_b = 100$). In the 4‐series hierarchy, each pair collapses into a single level‐1 series, which then sums to the top. In the 36‐series case, each block of six forms a level‐1 aggregate, and those six aggregates form the national total.

The 100‐series design employs a deliberately intricate aggregation path to stress‐test reconciliation methods. We first sum the one hundred bottom series into ten intermediate series by grouping them in contiguous blocks of ten. These ten series are then organised into three level-2 aggregates—four, three, and four series, respectively—before finally summing to a single top node. This asymmetric hierarchy creates overlapping correlation patterns: some level‐2 series share bottom‐level groups, while others draw from both, emulating practical scenarios such as regional sales aggregations that span multiple product categories or overlapping territories.

To save space, we illustrate only the six-by-six's VAR(1) and correlation configuration in @fig-settings-6x6; the two-by-two and the one-hundred-series structures appear in Appendix @sec-sim-supplement. 

```{r generate plots for A and Sigma}
MSE1_1 <- transform_sim_MSE(sim1_1$MSE, F) |> 
  filter(.model != "ols")
MSE1_2 <- transform_sim_MSE(sim1_2$MSE, F) |> 
  filter(.model != "ols")

MSE2_1 <- transform_sim_MSE(sim2_1$MSE, F) |> 
  filter(! .model %in% c("mint_n_hstep", "ols"))
MSE2_2 <- transform_sim_MSE(sim2_2$MSE, F) |> 
  filter(! .model %in% c("mint_n_hstep", "ols"))

MSE3_1 <- transform_sim_MSE(sim3_1$MSE, F)
MSE3_2 <- transform_sim_MSE(sim3_2$MSE, F) |> 
  filter(.model != "ols")

p1_A <- plot_heatmap(sim1_1$A, T) + 
  ggtitle("2x2 VAR(1) Coefficients Matrix")

p1_Sigma <- plot_heatmap(cov2cor(sim1_1$Sigma), T) + 
  ggtitle("2x2 Correlation Matrix")

p2_A <- plot_heatmap(sim2_1$A) +
  ggtitle("VAR(1) Coefficients Matrix") + theme_void()

p2_Sigma <- plot_heatmap(cov2cor(sim2_1$Sigma), T) +
  ggtitle("Correlation Matrix") + theme_void()

p3_A <- plot_heatmap(sim3_1$A) +
  ggtitle("2x50 VAR(1) Coefficients Matrix")

p3_Sigma <- plot_heatmap(cov2cor(sim3_1$Sigma), T) +
  ggtitle("2x50 Correlation Matrix")
```

```{r fig settings 6x6}
#| label: fig-settings-6x6
#| fig-cap: "Heatmaps of the VAR(1) coefficient matrix and correlation matrix for the 2x50 structure."
#| fig.height: 4
#| fig.width: 6
#| out.width: "85%"
#| fig.align: "center"

p2_A + p2_Sigma + plot_layout(ncol = 2) & theme(legend.position = "bottom")
```

```{r}
plot_relative_improvement <- function(MSE_data) {
  MSE_data |>
    filter(
      .model != "mint_true",
      h <= 4
    ) |>
    group_by(series, h) |>
    mutate(
      base_MSE = MSE[.model == "base"]
    ) |>
    ungroup() |>
    group_by(.model, h) |>
    summarise(
      MSE = mean(MSE),
      base_MSE = mean(base_MSE),
      pct_change = (MSE - base_MSE) / base_MSE * 100
    ) |>
    ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(
      x = "Forecast Horizon (h)",
      y = "Percentage Change (%)"
    ) +
    # labs(x = element_blank(), y = element_blank()) +
    theme_bw()
}
```

```{r fig-sim-results-1}
#| label: fig-sim-results-1
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the 2x2, 6x6, and 2x50 hierarchical structures, for 1 to 4 steps ahead forecasts, with 2 time series lengths (T = 54 and T = 304)."
#| fig.width: 8
#| fig.height: 6
#| out.width: "100%"
#| fig.align: "center"

p_MSE1_1 <- MSE1_1 |> 
  plot_relative_improvement() +
    labs(title = "2x2 Hierarchy", subtitle = "T = 54")
p_MSE2_1 <- MSE2_1 |> 
  plot_relative_improvement() +
    labs(title = "6x6 Hierarchy")
p_MSE3_1 <- MSE3_1 |> 
  plot_relative_improvement() +
    labs(title = "2x50 Hierarchy")

p_MSE1_2 <- MSE1_2 |> 
  plot_relative_improvement() +
    labs(subtitle = "T = 304")
p_MSE2_2 <- MSE2_2 |> 
  plot_relative_improvement()
p_MSE3_2 <- MSE3_2 |> 
  plot_relative_improvement()

p_MSE1_1 + p_MSE2_1 + p_MSE3_1 + p_MSE1_2 + p_MSE2_2 + p_MSE3_2 +
  plot_layout(ncol = 3, guides = "collect", axis_titles = "collect") &
  theme(legend.position = "right")
```

Figure @fig-sim-results-1 illustrates that as hierarchy size and complexity increase, the relative improvements in MSE over the base forecasts from reconciliation amplify, across all 1- to 4-step horizons. While extending the training window (from 50 to 300 in‐sample observations) uniformly improves the accuracy of base ARIMA forecasts, it does not alter the relative improvements of mint_shr or mint_n. This is evidence that both high-dimensional estimators robustly handle the "large $p \gg T$” regime. By contrast, mint_sample achieves significant accuracy when more data make the sample covariance more reliable. 

Despite their theoretical differences, mint_shr and mint_n exhibit nearly identical performance in these canonical settings. Yet NOVELIST’s thresholding ability to minimise effects of weak correlations and retain strong ones suggests it may excel in contexts where correlation matrix has small, noisy entries. This motivates our subsequent exploration of sparse covariance structures, in which many off-diagonals are truly zero.
<!--
## Exploring Effects of Hierarchy's Aggregation

- 2x50 with usual path and weird path

- Finding: Complex aggregation MinT_n performs well
-->

## Exploring the sparsity of the DGP covariance matrix

In our second simulation study, we design a data‐generating process that contrasts “dense” and “sparse” correlation regimes among bottom‐level series, reflecting settings one might encounter in practice. We consider the same hierarchical structure of two large groups of 50 bottom series as above. Specifically, the two groups would have strong within-group dependencies throughout and either modest between‐group correlations (the dense scenario) or complete independence (the sparse scenario). These correlation matrices are depicted in Figure @fig-Sigma-2x50. Both scenarios share the same VAR(1) coefficient structure as in our previous simulations; only the innovation covariance changes. Such a setup mirrors real‐world contexts where, for example, sales within a product line may exhibit strong co‐movements, while those in a separate line operate nearly independently.

<!-- Rather than collapsing each group into a single aggregate as in previous practice, we impose a more intricate, multi‐level aggregation path to stress‐test reconciliation methods. We first form ten intermediate series by summing bottom‐level series in consecutive blocks of ten. These ten series are then assigned to three second‐level aggregates--groups of four, three, and four--and finally summed to produce a single top‐level series. This asymmetric hierarchy creates overlapping correlation patterns: some level‐2 series share bottom‐level groups, while others draw from both. -->
<!-- emulating practical scenarios such as regional sales aggregations that span multiple product categories or overlapping territories. -->

```{r}
#| label: fig-Sigma-2x50
#| fig-cap: "Heatmaps of the dense and sparse correlation matrix of the data generating process."
#| fig.height: 12
#| fig.width: 8
#| out.width: "100%"
#| fig.align: "center"
p_dense_Sigma <- sim_dense_50$Sigma |>
  cov2cor() |> 
  plot_heatmap(T) +
    labs(title = "Dense")

p_sparse_Sigma <- sim_sparse_50$Sigma |>
  cov2cor() |> 
  plot_heatmap(T) +
    labs(title = "Sparse")

p_dense_Sigma + p_sparse_Sigma + 
  plot_layout(ncol = 1, guides = "collect", axis_titles = "collect") & 
  theme(legend.position = "right")
```

```{r}
#| label: fig-sim-results-dsp
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the 2x50 hierarchical structure with dense and sparse DGP's correlation matrix, for 1 to 4 steps ahead forecasts, with 2 time series lengths (T = 54 and T = 304)."
#| fig.height: 7
#| fig.width: 8
#| out.width: "90%"
#| fig.align: "center"
p_dense_50 <- sim_dense_50$MSE |>
  transform_sim_MSE(F) |> 
  plot_relative_improvement() +
    labs(title = "Dense DGP Covariance Matrix", subtitle = "T = 54")
p_sparse_50 <- sim_sparse_50$MSE |>
  transform_sim_MSE(F) |> 
  plot_relative_improvement() +
    labs(title = "Dense DGP Covariance Matrix")

p_dense_300 <- sim_dense_300$MSE |>
  transform_sim_MSE(F) |> 
  filter(.model != "ols") |> 
  plot_relative_improvement() +
    labs(subtitle = "T = 304")
p_sparse_300 <- sim_sparse_300$MSE |>
  transform_sim_MSE(F) |> 
  plot_relative_improvement()

p_dense_50 + p_sparse_50 + p_dense_300 + p_sparse_300 +
  plot_layout(ncol = 2, guides = "collect", axis_titles = "collect") &
  theme(legend.position = "right")
```

Figure @fig-sim-results-dsp presents out‐of‐sample mean squared error improvements over the base forecasts for each reconciliation strategy under both dense and sparse settings, with two panel lengths--54 and 304 observations--reserving the last four points for testing. In both short and long samples, MinT using either the shrinkage estimator (mint_shr) or the NOVELIST estimator (mint_n) delivers pronounced gains over incoherent ARIMA forecasts, particularly at the one‐step horizon where cross‐series correlations most directly inform the forecast adjustments. Although the two MinT variants perform almost indistinguishably overall, mint_n edges out mint_shr in the immediate horizon, whereas mint_shr slightly outperforms for longer horizons. By contrast, MinT with the raw sample covariance (mint_sample) suffers in small‐sample settings; as expected, its performance improves dramatically with 304 data points, since the sample covariance becomes more reliable with larger $n$. This highlights the practical necessity of regularized estimators in high‐dimensional, low‐sample contexts, a situation common in real applications where histories are short relative to the number of series.

Additional designs (varying block sizes, aggregation paths, correlation configurations) also failed to separate NOVELIST from Shrinkage. Their nearly identical performance under these synthetic scenarios suggests that our current simulation may not unveil the full advantages of the thresholding estimators. This finding motivates our turn to empirical data, where latent structural features and regime shifts, which we will discuss in the next section, may reveal performance differences.

<!--
## Exploring Grouped Structure

- small vs big 

- Finding: NOVELIST performs well in both cases
-->

# Forecasting Australian Domestics Tourism {#sec-empirical}

Domestic tourism flows in Australia exhibit a natural hierarchical and grouped structure, driven both by geography and by purpose of travel. At the top of this hierarchy lies the national total, which splits into the seven states and territories. Each state is further sub-divided into tourism zones, which in turn break down into 77 regions. A complete illustration of this geographic hierarchy appears in Appendix @sec-tourism-supplement. Intersecting this geographic hierarchy is a second dimension--travel motive--partitioning tourism flows into four categories: holiday, business, visiting friends and relatives, and other. Altogether, this yields a grouped system of 560 series, from the most disaggregated regional-purpose cells up to the full national aggregate. @tbl-tourism-structure depicts this structure.

<!-- - A table/figure to illustrate the hierarchy -->

We quantify tourism demand via "visitor nights", the total number of nights spent by Australians away from home. The data is collected via the National Visitor Survey, managed by Tourism Research Australia, using computer assisted telephone interviews from nearly 120,000 Australian residents aged 15 years and over [@tourism-au]. 

```{r}
library(stringr)

desired_order <- c(
  "Australia", "Australia_by_Purpose",
  "State",     "State_by_Purpose",
  "Zone",      "Zone_by_Purpose",
  "Region",    "Region_by_Purpose"
)
```


```{r fig tourism results}
#| label: fig-tourism-results
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows"
#| fig.width: 8
#| fig.height: 10
#| out.width: "100%"

tourism_MSE <- readRDS("./literature/lib/tourism_MSE.rds")

# % improv plot by level
tourism_MSE |>
  group_by(series, h) |>
  mutate(
    base_MSE = MSE[.model == "base"]
  ) |>
  ungroup() |>
  group_by(.model, level, h) |>
  summarise(
    MSE = mean(MSE),
    base_MSE = mean(base_MSE),
    pct_change = (MSE - base_MSE) / base_MSE * 100
  ) |>
  filter(.model != "mint_sample") %>%
  # Reorder the levels according to the desired order
  mutate(
    level = factor(
      level,
      levels = desired_order,
      labels = str_replace_all(desired_order, "_", " ")  # replace "_" with " "
    )
  ) %>% 
  ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(x = "Horizon", y = "% improvements") +
    facet_wrap(~ level, ncol=2, scales = "free_y") +
    theme_bw() +
    scale_x_continuous(limits = c(1, 12), expand = c(0, 0)) +
    theme(legend.position = "bottom", legend.title = element_blank())
```

The data are monthly time series spanning from January 1998 to December 2019, resulting in 264 observations per series, producing a canonical "$n \ll p$" setting which is ideal for evaluating reconciliation approaches that rely on high-dimensional covariance estimation. The extreme dimensionality over sample size mirrors many contemporary business problems, for instance, Starbucks drink sales. Tourism demand is also economically vital yet highly volatile, with geographical and purpose‑specific patterns create a realistic stress‑test for reconciliation algorithms. Consequently, this panel offers both a rich policy case study and a stringent statistical laboratory for comparing reconciliation strategies that exploit cross-series information to improve forecasts when historical data are scarce.

<!-- This dataset is particularly well‑suited to evaluating forecast‑reconciliation methods for three reasons. First, tourism demand is economically important and notoriously volatile; the heterogeneous regional and purpose‑specific patterns create a realistic stress‑test for reconciliation algorithms. Second, the clear structural shift towards stronger growth after 2016—visible in most aggregates—necessitates techniques capable of handling possible breaks or time‑varying error structures, making the exercise practically relevant. Third, the extreme dimensionality relative to sample size mirrors many contemporary business problems (e.g., SKU‑level retail sales), allowing insights to generalise beyond tourism. Using this panel therefore provides both a compelling policy context and a stringent statistical laboratory for comparing alternative reconciliation strategies, especially those that rely on regularised covariance estimation or exploit cross‑sectional information to stabilise forecasts when historical data are scant. -->

@Wickramasuriya2019-xq also argued that modelling spatial autocorrelations would be challenging as in the case of a large collection of time series. Reconciliation approaches have the advantage to implicitly model this spatial autocorreltion structure, especially when the MinT method is used.

To assess forecasting performance, we adopt a rolling-window cross-validation scheme. Beginning with the first 96 monthly observations (January 1998-December 2005) as the initial training set, we obtain the best-fitted ARIMA model for each of the 560 series via the automatic algorithm in the fabletools package, by minimising AICc [@O-Hara-Wild2024-we]. The 1- to 12-step-ahead base forecasts are then generated by these ARIMA models. These incoherent base forecasts are reconciled via OLS, MinT with shrinkage (mint_shr), and MinT with the NOVELIST estimator (mint_n). We then roll the training window forward by one month and refit all models, rebuild reconciliations, and produce another batch of 1- to 12-step-ahead forecasts, repeating until December 2018. In total, this results in 156 out-of-sample windows and an equal number of forecast sets for each series.

Note that the number of series is larger than the number of observations (560 compared to 96), hence the sample covariance matrix is not positive definite and will not be considered.

The results are presented in @fig-tourism-results, which shows the relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows. The results are grouped by levels of aggregation. MinT variants show improvement over the base ARIMA forecasts for middle to lower aggregation levels. We hardly differentiate the point accuracy performance between mint_shr and mint_n in these level, except at the most disaggregated, where mint_shr slightly edges out mint_n.

```{r fig STL tourism}
#| label: fig-STL-tourism
#| fig-cap: "Monthly domestic overnight tourism nights in Australia by state, with STL decomposition to extract trend component"
#| fig.width: 8
#| fig.height: 6
#| out.width: "100%"
#| fig.align: "center"

visnights_full <- readRDS("literature/lib/visnights_full.rds")

visnights_full %>% 
  filter(
    is_aggregated(Purpose) & is_aggregated(Zone) & is_aggregated(Region),
    !is_aggregated(State)
  ) %>% 
  model(
    STL(Nights)
  ) %>% 
  components() %>% 
  pivot_longer(
    cols = c(Nights, trend, season_year, remainder, season_adjust),
    names_to = "component",
    values_to = "value"
  ) %>%
  filter(
    component == c("Nights", "trend"),
    year(Month) >= 2000
  ) %>%
  as_tibble() %>%
  mutate(component = recode(component,                     # give them nicer names
                            Nights = "Observed Nights",
                            trend  = "STL Decomposed Trend")) %>% 
  ggplot(aes(Month, value, color = as.character(State))) +
    geom_line() +
    facet_wrap(~ component, ncol=1, scales = "free_y") +
    labs(
      x = "Month",
      y = "Visitor Nights (in thousands)",
      color = "State"
    ) +
    theme_bw()
```

Surprisingly, however, MinT underperforms OLS at higher aggregation level and even the base forecasts at country-wise. This contradicts with the canonical results from @Wickramasuriya2019-xq, prompting further investigation. Applying STL decomposition by @Bandara2022-og to state-aggregated observations reveals a structural change around 2016: four large states (New South Wales, Victoria, Queensland, Western Australia) develop a modest upward trend in visitor nights, as shown in @fig-STL-tourism. The trend is likely due to the increasing popularity of tourism in these places, as well as the impact of various factors such as economic growth, population growth, and changes in consumer preferences. 

```{r}
#| label: fig-tourism-results-ex2016
#| fig-cap: "Relative improvement of the MSE of reconciled forecasts over the base forecasts for the Australian domestic tourism flows, excluding the period after 2016"
#| fig.width: 8
#| fig.height: 10
#| out.width: "100%"

tourism_MSE_ex2016 <- readRDS("literature/lib/tourism_MSE_ex2016.rds")

# % improv plot by level
tourism_MSE_ex2016 |>
  group_by(series, h) |>
  mutate(
    base_MSE = MSE[.model == "base"]
  ) |>
  ungroup() |>
  group_by(.model, level, h) |>
  summarise(
    MSE = mean(MSE),
    base_MSE = mean(base_MSE),
    pct_change = (MSE - base_MSE) / base_MSE * 100
  ) |>
  filter(.model != "mint_sample") %>%
  # Reorder the levels according to the desired order
  mutate(
    level = factor(
      level,
      levels = desired_order,
      labels = str_replace_all(desired_order, "_", " ")  # replace "_" with " "
    )
  ) %>%
  ggplot(aes(x = h, y = pct_change, color = .model)) +
    geom_line() +
    labs(x = "Horizon", y = "% improvements") +
    facet_wrap(~ level, ncol=2, scales = "free_y") +
    theme_bw() +
    scale_x_continuous(limits = c(1, 12), expand = c(0, 0)) +
    theme(legend.position = "bottom", legend.title = element_blank())
```

Neither ARIMA nor reconciliation strategies capture this shift, degrading accuracy at the most aggregated level. From the results in @fig-tourism-results-ex2016, excluding the post-2016 period (i.e., restricting the final training window to end on December 2015) restores reconciliation’s performance: both MinT methods now clearly outperform OLS and the base forecasts at all levels. Additionally, we notice that mint_n increasingly outperforms mint_shr at higher levels (Australia, State, and Australia by Purposes levels). This is an advantage of NOVELIST estimator. At higher levels most series are combinations of many bottom cells, their cross-series base forecast error correlations are strong and genuinely useful. NOVELIST keeps these strong links at those higher levels, while its thresholding feature minimises the effects of weak, noisy ones.

The empirical analysis on real, hierarchical Australian domestic tourism shreds lights on the strengths of the NOVELIST estimator--advantages that were not fully apparent in previous simulation studies. Going forward, we plan to embed this structure into our data-generating processes and to pinpoint contexts where NOVELIST’s thresholding yields gains beyond those of Shrinkage. Moreover, our findings underscore the need to accommodate structural changes--the post-2016 Australian domestic tourism demand shifts we encountered.

<!-- perhaps via robust trend adjustment or state-space extensions that jointly model coherence and temporal dynamics. -->

<!-- # Supplementary -->


\pagebreak

# Timeline & Milestones {#sec-timeline}

```{r timeline}
#| echo: false
#| message: false
#| warning: false
library(tibble)
library(knitr)
library(kableExtra)

tibble(
  Period = c(
    "March",
    "April", 
    "May", 
    "June",
    "July",
    "August", 
    "September",
    "October"
  ),
  Task = c(
    "Initial setup and understanding the research scope. Preliminary knowledge and Literature review",

    "Develop ReconCov, an R package implementing the Min Trace approach with the shrinkage and NOVELIST estimators, and rolling-window cross-validation process for choosing NOVELIST threshold parameter. Exploring simulation framework.",

    "Code functions for producing simulation settings in ReconCov package. Initial simulation for MinT in hierarchical time series. Writing research proposal and presenting research project.",

    "Intensive simulation on multiple different settings, in order to assess the performance and behaviours of NOVELIST under different conditions, and to explore cases where NOVELIST performs different than Shrinkage.",

    "Empirical analysis on forecasting Australian Domestic Tourism data. Explore the real-world data generating process which can be used to refine the current simulation design",

    "Implement the Tourism DGP configuration into the current simulation settings. Implementing POET and PC-adjusted NOVELIST estimators. Assessing whether prominent principal components structure can be exploited to improve the out-of-sample forecast performance.",

    "Exploring other high-dimensional covariance estimators. More simulations on all explored covariance estimators. Final,  full-fledged assessment on all implemented estimators",

    "Final writing, presenation, submission"
  ),
  Deliverable = c(
    "Initial literature review + paper draft",
    "ReconCov package: Main codes for estimators + simulation framework + cross-validation",
    "NOVELIST initial simulation results + research proposal + presentation",
    "More simulation results on NOVELIST",
    "Empirical results on forecasting Tourism data",
    "New simulation design, POET & PC-adjusted NOVELIST simulation results",
    "Other estimators simulation results and full-fledged covariance estimators assessment",
    "Presentation and thesis manuscript"
  )
) %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    longtable = TRUE,
    caption = "Project Timeline and Key Milestones",
    escape = TRUE
  ) %>%
  column_spec(1, width = "1.9cm") %>%
  column_spec(2, width = "8cm") %>%
  column_spec(3, width = "4.8cm") %>%
  kable_styling(latex_options = "hold_position", font_size = 10)
```

\pagebreak

# Appendix {#sec-appendix} 

## Simulation Settings: Supplementary Figures {#sec-sim-supplement}

```{r fig settings 2x2}
#| label: fig-settings-2x2
#| fig-cap: "Heatmaps of the VAR(1) coefficient matrix and correlation matrix for the 2x2 structure."
#| fig.height: 4
#| fig.width: 6
#| out.width: "55%"
#| fig.align: "center"

p1_A + p1_Sigma + plot_layout(ncol = 2) & theme(legend.position = "bottom")
```

```{r fig settings 2x50}
#| label: fig-settings-2x50
#| fig-cap: "Heatmaps of the VAR(1) coefficient matrix and correlation matrix for the 6×6 structure."
#| fig.height: 12
#| fig.width: 8
#| out.width: "85%"
#| fig.align: "center"

p3_A + p3_Sigma + plot_layout(ncol = 1) & theme(legend.position = "right")
```

\pagebreak

## Algorithm: NOVELIST cross-validation for optimal threshold $\delta^*$ {#sec-novelist_cv}

The cross-validation algorithm for NOVELIST is available in the ReconCov package [@ReconCov].

```{=latex}
\begin{algorithm}
\caption{Cross-validation procedure}
\begin{algorithmic}[1]

\State \textbf{Input:} Observations and fitted values $\boldsymbol{y}_t, \hat{\boldsymbol{y}}_t \in \mathbb{R}^n$ for $t = 1,\dots,T$, set of threshold candidates $\Delta$, window size $v$.

\State $\hat{\boldsymbol{e}}_t = \boldsymbol{y}_t - \hat{\boldsymbol{y}}_t$ for $t = 1,\dots,T$

\For{$i = v:T-1$}
    \State $j = i - v +1$
    \State $\hat{\boldsymbol{W}}_j = \frac{1}{v} \sum_{t=j}^{i} \hat{\boldsymbol{e}}_{t} \hat{\boldsymbol{e}}_{t}'$
    \State $\hat{\boldsymbol{D}}_j = \text{diag}(\hat{\boldsymbol{W}}_j)$
    \State $\hat{\boldsymbol{R}}_j = \hat{\boldsymbol{D}}_j^{-1/2} \hat{\boldsymbol{W}}_j \hat{\boldsymbol{D}}_j^{-1/2}$
    \For{$\delta \in \Delta$}
        \State Compute thresholded correlation $\hat{\boldsymbol{R}}_{j,\delta}$ using Equation 5
        \State Compute $\hat{\lambda}_{j,\delta}$ using Equation 6
        \State Compute $\hat{\boldsymbol{R}}^{N}_{j,\delta}$ using Equation 4
        \State $\hat{\boldsymbol{W}}^{N}_{j,\delta} = \hat{\boldsymbol{D}}_j^{1/2} \hat{\boldsymbol{R}}^{N}_{j,\delta} \hat{\boldsymbol{D}}_j^{1/2}$
        \State $\boldsymbol{G} = (\boldsymbol{S}' \hat{\boldsymbol{W}}^{N^{-1}}_{j,\delta} \boldsymbol{S})^{-1} \boldsymbol{S}' \hat{\boldsymbol{W}}^{N^{-1}}_{j,\delta}$
        \State Reconciled forecasts $\tilde{\boldsymbol{y}}_{i+1 | \delta} = \boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_{i+1}$
        \State $\tilde{\boldsymbol{e}}_{i+1 | \delta} = \boldsymbol{y}_{i+1} - \tilde{\boldsymbol{y}}_{i+1 | \delta}$
    \EndFor
\EndFor

\State $\text{MSE}_{\delta} = \frac{1}{T-v} \sum_{i=v}^{T-1} (\tilde{\boldsymbol{e}}_{i+1 | \delta})^2$ for each $\delta \in \Delta$
\State $\hat{\delta}^* = \arg\min_{\delta \in \Delta} \text{MSE}_{\delta}$
\State Compute $\hat{\lambda}^*$ on all training data using $\hat{\delta}^*$
\State Compute $\hat{\boldsymbol{R}}_1^*$ using $\hat{\delta}^*$ and $\hat{\lambda}^*$ on all training data, using Equation 3

\State \textbf{Output:} Estimate of optimal $\hat{\delta}^*$

\end{algorithmic}
\end{algorithm}
```

\pagebreak

# References

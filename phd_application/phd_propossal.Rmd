---
title: "\\LARGE Decision-Optimal Probabilistic Forecast Reconciliation for Expected Shortfall in Hierarchical Time Series"
author: 
  - \Large Vincent Su
format: 
  pdf:
    include-in-header:
      text: |
        <!-- \usepackage{amsmath} -->
        <!-- \usepackage{mathspec} -->
        \usepackage{algorithm}
        \usepackage{algpseudocode}
        \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
        \usepackage{array}
        
        % adjust spacing
        \usepackage{titling}
        \pretitle{\begin{center}\vspace{-5em}}
        \posttitle{\end{center}\vspace{-1.5em}} 
        \preauthor{\begin{center}}
        \postauthor{\end{center}\vspace{-4em}} 
    mathspec: true
    number-sections: true
link-citations: true
execute:
  cache: true
  echo: false
  warning: false
bibliography: D:/Github/Recon_Honours_Thesis/references.bib
# bibliographystyle: apa
csl: D:/Github/Recon_Honours_Thesis/apa.csl
editor_options: 
  chunk_output_type: console
---

<!--
========== Tas Advice ==========
- Start with ES
  - Talk about VaR and then ES
  - Take portfolio as example
  - Walk to hierarchical time series
  - Motivation is bringing ES and HTS together

- Quantiles are point forecasts



-->

# Background

<!-- 
========== Old version ==========

- Data is hierarchical
- Decision making requires forecasts at different levels of hierarchy
- Base forecasts are incoherent and accuracy suffers
- Reconciliation methods are used to adjust forecasts to be coherent
- Point forecasts methods are developed
- Probabilistic forecasts are sometimes the quantity of interest, especially in risk management

Many operational datasets are naturally hierarchical. For Starbucks, their total sales is the sum of all countries they are operating in, and each national sales is the sum for all cities, down to many outlets, forming an aggregation constraint. Forecasting such hierarchical time series arises in many decision-making contexts ranging from supply chains [@Seaman2022-bb; @Angam2025-od] and energy planning [@Di-Modica2021-ad], to macroeconomic [@El-Gemayel2022-hx; @Li2019-vv] and tourism analysis [@Athanasopoulos2009-lp], where stakeholders need forecasts at several aggregation levels to allocate resources and manage risk.

In practice, when base forecasts are produced independently for all series, they typically fail to satisfy the aggregation constraints. Such forecasts are *incoherent*, which can undermine downstream decisions that require internal consistency. Forecast reconciliation addresses this by adjusting the base forecasts so the final set is *coherent* with the hierarchy. Approaches such as OLS [@Hyndman2011-jv], WLS [@Hyndman2016-ic], and MinT [@Wickramasuriya2019-xq] implement this as linear mappings (often projections) so that the adjusted forecasts lie in the coherent subspace; see @Athanasopoulos2024-as for a comprehensive review. 
-->

<!-- 
- Talk about VaR and then ES
- Take portfolio as example
- Link to hierarchical time series with previous example
- Mention HTS arises in many contexts
- Brief about coherence and reconciliation
-->

<!-- 
While VaR is conceptually straightforward, it only measures the percentiles of loss distributions and ignores the severity of losses beyond that threshold [@Yamai2005-me]. ES, on the other hand, captures the average loss in the tail beyond the VaR threshold. This makes ES a more reliable metric for assessing and managing "tail risks", and it has been adopted as the preferred regulatory metric since the Basel III framework [@Basel-iii].
-->

Risk managers and regulators use quantitative risk measures to quantify and manage potential losses in uncertain environments. Two familiar choices are Value-at-Risk (VaR)--a loss threshold exceeded with some probability--and Expected Shortfall (ES; also known as Conditional VaR)--the average loss beyond that threshold. 

These tail risks are, however, not managed in isolation. Consider a hedge fund's investment portfolio returns, which compose of returns of different asset classes, and each asset class further contain returns of various individual securities. This is an example of a hierarchical structure, with multiple *aggregation levels*. Risk managers need to assess and forecast the returns or risk quantities at several levels, from the overall portfolio, down to individual securities. Ignoring this naturally-formed hierarchical structure can lead to misaligned forecasts, for example, the predicted returns of individual securities are low while the overall portfolio returns are high. This results in misallocation of capital and suboptimal risk management strategies.

Forecasting such hierarchical structure also arises in many other decision-making contexts, from supply chains [@Seaman2022-bb; @Angam2025-od] and energy planning [@Di-Modica2021-ad], to macroeconomic [@El-Gemayel2022-hx; @Li2019-vv] and tourism analysis [@Athanasopoulos2009-lp]. Stakeholders in these settings need forecasts at several aggregation levels to allocate resources and manage risk. These hierarchical structures naturally form *aggregation constraints* (child levels must sum up to parents). Taken together, when we have a collection of time series organised in a hierarchy subject to the aggregation constraints, we refer to this as *hierarchical time series*.

In practice, when forecasts are produced for all series (often called *base forecasts*), they typically violate the aggregation constraints observed in the data; such forecasts are *incoherent*. This can undermine downstream decisions that require internal consistency. Forecast reconciliation, a post-processing step, addresses this by adjusting the base forecasts so the final adjusted set of forecasts is *coherent* with the aggregation structure. Approaches such as OLS [@Hyndman2011-jv], WLS [@Hyndman2016-ic], MinT [@Wickramasuriya2019-xq], and more are developed and commonly used in practice. An additional incentive for the rapid adoption of reconciliation methods is that they improve forecast accuracy [@Athanasopoulos2024-as]. For instance, the International Monetary Fund reported improvements in their liquidity forecasts, generating economic benefits via more efficient monetary policy operations [@El-Gemayel2022-hx].

\color{red}
Importantly, ES behaves well under aggregation (satisfying subadditivity), making it a natural target when risks are rolled up through a hierarchy. This context motivates methods that deliver coherent, multi-level forecasts whose uncertainty aligns with the tail-risk metrics managers actually use.

(should i remove this?)
\color{black}




# Motivation

<!--
========== Old version ==========

- Recent works mainly focus on point forecasts
- Existing point forecast reconciliation can be extended to probabilistic forecasts
- Point forecast reconciliation methods are optimised for scoring rules such as MSE
- Question: How can probabilistic reconciliation be optimised with respect to different loss functions targeted at the
downstream decision? 
- Value-at-risk (VaR) is common risk measure, however, it ignores the tail risk and not coherent risk measure
- Expected Shortfall (ES) is coherent risk measure and accounts for tail risk, and has recently superseded the Value at Risk as the preferred risk measure in banking regulation.
- This work investigates the optimal probabilistic forecast reconciliation for Expected Shortfall.


In the current literature, most reconciliation methodology has been developed for point forecasts, and its benefits for accuracy and coordination across organisation's departments or "silos" are well documented. However, in many applications the quantity of interest is probabilistic forecasts, not just a specific value. Decision makers care about the full distribution of future outcomes to quantify uncertainty, especially in risk management, where tail risks matter (e.g., extreme demand in newsvendor settings or energy generation shortfalls). This motivates moving from point reconciliation to probabilistic reconciliation, so that coherent forecasts also carry coherent uncertainty.

Existing reconciliation methods for point forecasts has recently been generalised to reconcile probabilistic forecasts. However, these approaches originally focus on point forecasts and are optimised for scoring rules such as mean squared error (MSE). There is a gap in the literature on how *optimal* this approach is with respect to the downstream decision. The current expansion of point reconciliation methods no longer guarantee optimality for probabilistic forecasts, such as quantiles or expected shortfall (ES). This leaves the question: *"How can probabilistic reconciliation be optimised with respect to different loss functions targeted at the downstream decision?"*

Although @Panagiotelis2023-fs introduced an score optimisation for reconciliation weights using the energy score or variogram score, they are scoring rules for full multivariate predictive distributions and do not directly address the tail functionals. 

Among the risk measures, quantiles, also known as Value-at-Risk (VaR), is a common choice, but it has limitations. VaR ignores the severity of losses beyond that threshold and is not a coherent risk measure, since it does not satisfy the subadditivity property [@Yamai2005-me]. This can lead to inconsistencies in risk assessments across different levels of the hierarchy.

Meanwhile, Expected Shortfall (ES) is a coherent risk measure that accounts for the severity of losses in the tail of the distribution [@Yamai2005-me]. It has recently been adopted as the preferred risk measure in banking regulation since the Basel III regulatory framework [@Basel-iii]. It has ability to capture tail risks more effectively than VaR. ES is also applicable beyond finance, such as quantifying expected economic loss when electricity demand exceeds available capacity. Therefore, we aim to investigate and develop the optimal probabilistic forecast reconciliation under the Expected Shortfall criterion.
-->

<!--
=========== 1st version =========
In the current literature, most reconciliation methodology has been developed for point forecasts, and its benefits for accuracy and coordination across organisation's departments or "silos" are well documented. However, modern risk management and operations need more than a typical mean or median value--they need quantities from the full predictive distribution that capture uncertainty and tail behaviour, because many decisions are tail-sensitive (e.g., extreme demand in newsvendor settings or energy generation shortfalls).

Existing reconciliation methods for point forecasts has recently been generalised to reconcile probabilistic forecasts with downstream decisions based on these. However, these approaches originally focus on point forecasts and are optimised for scoring functions such as mean squared error (MSE). There is a gap in the literature on how *optimal* this approach when considering alternative quantities of interest, such as Value-at-Risk or Expected Shortfall. This leaves the question: *"How can we construct probabilistic forecast reconciliation that are explicitly optimised for tail-risk measures?"*

Although @Panagiotelis2023-fs introduced an score optimisation for reconciliation weights using proper scoring rules (energy score and variogram score) for full multivariate predictive distributions, they do not directly address the tail functionals. Other than that, there is a lack of research on optimising reconciliation for specific risk measures such as VaR and ES.

Both VaR and ES are popular risk measures, but they have different properties. While VaR is conceptually straightforward, it ignores the severity of losses beyond the VaR threshold and is not a coherent risk measure, since it does not satisfy the subadditivity property [@Yamai2005-me]. VaR can mislead risk assessments across different levels of the hierarchy, and may underestimate the true risk exposure under market stress. ES, on the other hand, is a coherent metric that captures the average loss in the tail beyond the VaR threshold. This makes ES a more reliable metric for assessing and managing "tail risks", and it has been adopted as the preferred regulatory metric since the Basel III framework [@Basel-iii].

Even with these advantages, ES has its own challenges. Notably, ES is not elicitable on its own, meaning there is no scoring rule or loss function that can uniquely identify the ES as the optimal optimiser. However, works from @Fissler2015-uu and @Rockafellar2000-el show that this is not impossible when ES is considered jointly with VaR. This joint elicitability opens the door to optimising reconciliation methods specifically for ES. As a result, the biggest objective of this project is to bring these recent theoretical advances into the reconciliation framework, to develop methods that produce coherent probabilistic forecasts optimised for ES.
-->

Modern risk management and operations need more than a typical mean or median value--they need quantities from the full predictive distribution that capture uncertainty and tail behaviour, because many decisions are tail-sensitive (e.g., extreme demand in retail settings or energy generation shortfalls). While VaR is conceptually straightforward, it ignores the severity of losses beyond the VaR threshold and does not satisfy the subadditivity property\footnote{\textbf{Subadditivity.} A risk measure $\rho$ on loss variables is subadditive if $\rho(X+Y)\le \rho(X)+\rho(Y)$ for any losses $X,Y$. Expected Shortfall satisfies subadditivity for all $\alpha\in(0,1)$, whereas VaR can violate it. Thus, the portfolio ES cannot exceed the sums of individual security ESs.} [@Yamai2005-me]. Violating subadditivity is a critical problem, because it means that the risk measure of a combined portfolio can exceed the sum of stand-alone risks, which contradicts the principle of diversification. VaR therefore can mislead risk assessments across different levels of the hierarchy, and may underestimate the true risk exposure under market stress. ES, on the other hand, is a consistent metric that captures the average loss in the tail beyond the VaR threshold. This makes ES a more reliable measure for assessing and managing "tail risks", and it has been adopted as the preferred regulatory metric since the Basel III framework [@Basel-iii]. If ES is the metric that risk managers actually use, reconciliation methods should guarantee the coherence and optimality with respect to ES.

Most reconciliation methodology has been developed for point forecasts and are optimised for point scoring functions such as mean squared error (MSE). Generalising these methods to reconcile probabilistic forecasts does not guarantee optimality for tail-risk measures such as VaR or ES. Although @Panagiotelis2023-fs introduced score optimisation for reconciliation weights using proper scoring rules (energy score and variogram score) for full multivariate predictive distributions, they do not directly address the tail functionals. This leaves the question: *"How can we construct probabilistic forecast reconciliation that are explicitly optimised for tail-risk measures such as ES?"*

<!-- \color{red}
**AP: I have thought a bit about how we discuss the point v probabilistic issue. Ultimately, a VaR or ES forecast is a point forecast. However, a) any point forecast can be derived from a distributional forecast and b) scoring rules give us a way to obtain the optimal VaR or ES if we know the underlying probabilistic distribution. These two facts taken together mean we can naturally adapt the score optimisation framework of Panagiotelis et al (2023) to obtain optimal ES forecasts while also guaranteeing that these forecast come from an underlying probabilitistic forecast that is coherent.**
\color{black} -->

A practical challenge is that ES is not elicitable on its own, meaning there is no scoring rule or loss function that can uniquely identify ES as the optimiser. However, ES is jointly elicitable with VaR [@Fissler2015-uu], and is amenable to convex optimisation via an auxiliary VaR variable [@Rockafellar2000-el]. This opens the door to train reconciliation specifically for ES.

As a result, the objective of my thesis is to bring these theoretical advances into the reconciliation framework and develop methods that produce coherent probabilistic forecasts optimised for ES. The goal is a reliable, consistent, and decision-aligned risk-forecasting framework that helps national and international organisations improve ES backtesting, stabilise capital, and allocate resources more effectively across hierarchical levels.

<!-- footnote for subadditivity property? -->

# Methodology

### Preliminaries {-}
<!-- 
- Hierarchical time series (HTS) is a collection of time series that are organised in a hierarchy.
- Coherent subspace and reconciliation definition
- Reconciliation to probabilistic forecasts\
-->

Let $\boldsymbol{y}_t \in \mathbb{R}^K$ denote a vector of $K$ time series variables from the hierarchy, at time $t$. For simplicity, we suppress the subscript $t$ and denote it as $\boldsymbol{y}$. These variables are subject to linear aggregation constraints represented by a matrix $\boldsymbol C$, such that $\boldsymbol C \boldsymbol{y} = 0$. For example, in a retail setting, the total sales at a warehouse ($y_1$) and two stores ($y_2$ and $y_3$) might satisfy the constraint $y_1 = y_2 + y_3$, leading to $\boldsymbol C = (1, -1, -1)$.

<!-- 
\tilde{\boldsymbol{y}}
\hat{\boldsymbol{y}}
-->

Base forecasts $\hat{\boldsymbol{y}}$, which we generate independently for each series, are typically *incoherent* (i.e. $\boldsymbol C \hat{\boldsymbol{y}} \neq 0$), while reconciled forecasts $\tilde{\boldsymbol{y}}$ are coherent by construction (i.e. $\boldsymbol C \hat{\boldsymbol{y}} = 0$). We obtain coherence via a linear mapping $\tilde{\boldsymbol{y}} = \boldsymbol P \hat{\boldsymbol{y}}$, where $\boldsymbol P$ maps the full *incoherent* space $\mathbb{R}^K$ to the linear *coherent* subspace $\mathbb{S} \subset \mathbb{R}^K$, where aggregation constraints hold. 

<!-- 
This mapping $\boldsymbol P$ can be a projection operator $\boldsymbol S (\boldsymbol S' \boldsymbol S)^{-1} \boldsymbol S'$ [@Hyndman2011-jv], where $\boldsymbol S$ is a matrix designed such that $\boldsymbol S \boldsymbol C = 0$. The mapping $\boldsymbol P$ is not unique, and selecting an optimal $\boldsymbol P$ is the core design problem. In this project, we will focus solely on linear reconciliation.

For probabilistic forecasts, let $\hat F$ be a multivariate base probabilistic forecast on $\mathbb{R}^K$. Define the reconciled probabilistic forecast $\tilde F$ on $\mathbb{S}$ so that probabilities are preserved under the reconciliation map (intuitively, events in the coherent subspace under $\tilde F$ correspond to pre-images under $\hat F$). This extends point reconciliation to the distributional setting while respecting the aggregation constraints.

\color{red}
Talk about the probabilistic reconciliation and how it is extended from point reconciliation.
\color{black}
-->




### Optimisation Framework {-}
<!-- 
- ES definition
- ES is not elicitable, but is jointly elicitable with the quantile function.
- the quantile and ES jointly minimise in expectation the Fissler-Ziegel class of loss functions
- utilise the Rockafellar-Uryasev joint characterisation of the quantile and ES
- thus jointly reconcile the quantile and ES using the following bilevel optimisation problem
- ...
-->

For a random variable $Y$, the Expected Shortfall (ES) at level $\alpha \in (0, 1)$ is defined as $\mathrm{ES}_\alpha(Y) = E[Y \mid Y < \mathrm{VaR}_\alpha(Y)]$, where $\mathrm{VaR}_\alpha(Y)$ is the quantile $q$ at level $\alpha$. The ES alone is not elicitable, meaning there is no scoring rule or loss function that can uniquely identify the ES as the optimal optimiser. However, @Fissler2015-uu showed that the quantile function and ES are jointly elicitable, minimising the Fissler-Ziegel class of loss functions in expectation. One of which takes the form:

$$
L_\alpha^\mathrm{FZ}(y, q, e) = (\mathbb{I}(y \leq q) - \alpha)(q-y) 
+ \frac{1}{\alpha} \mathrm{exp}(e) \mathbb{I}(y \leq q)(q-y) + \mathrm{exp}(e)(e-q-1),
$$

where $y$ is the realisation of $Y$, $q$ is the quantile, and $e$ is the ES. This loss function is non-convex, thus greatly complicating the optimisation problem. Alternatively, we propose to use the Rockafellar-Uryasev joint characterisation of the quantile and ES [@Rockafellar2000-el]:
$\widetilde{\mathrm{ES}}_k(\boldsymbol{P})=\max_{q}\, E_{\tilde{\boldsymbol{y}}}\!\left[ H_{\alpha}\!\big(\tilde y_k(\boldsymbol{P}), q\big) \right]$ 
and 
$\tilde q_k(\boldsymbol{P})=\arg\max_{q}\, E_{\tilde{\boldsymbol{y}}}\!\left[ H_{\alpha}\!\big(\tilde y_k(\boldsymbol{P}), q\big) \right]$,
where
$H_{\alpha}(y, q) = q - \frac{1}{\alpha}\max\{0, y - q\}$.
Thus, we jointly reconcile the quantile and ES using the following bilevel optimisation problem:

\vspace{-0.8cm}

\begin{align}
\min_{\boldsymbol{P}}\;&
\sum_{k=1}^{K} E_{\boldsymbol y}\!\left[
  L^{\mathrm{FZ}}_{\alpha}\!\big(y_k,\, \tilde q_k(\boldsymbol{P}),\, \widetilde{\mathrm{ES}}_k(\boldsymbol{P})\big)
\right]
\\
\text{s.t. }&
\tilde q_k(\boldsymbol{P})=\arg\max_{q}\, E_{\tilde{\boldsymbol{y}}}\!\left[ H_{\alpha}\!\big(\tilde y_k(\boldsymbol{P}), q\big) \right],
\notag\\
&
\widetilde{\mathrm{ES}}_k(\boldsymbol{P})=\max_{q}\, E_{\tilde{\boldsymbol{y}}}\!\left[ H_{\alpha}\!\big(\tilde y_k(\boldsymbol{P}), q\big) \right],
\qquad \forall\,k=1,\ldots,K.
\notag
\end{align}

\vspace{-0.4cm}

This ensures that the lower-level problem is still convex, however the upper-level objective function with the Fissler-Ziegler loss is non-convex. Irrespective of this non-convexity, smoothing techniques combined with the implicit function and envelope theorems can be used to devise gradient-based methods for locally minimising.

# Applications 
<!-- 
- Finance: ES is prefered risk measure, hierarchical structure from composite indices and etfs, also accounting for varying weights. Data is publicly available.
- Energy: ES can be a measure of economic loss accrued if insufficient generation capacity, hierarchical structure from geographical aggregation of service zones, states, and national level. Electricity load data are made available by the Australian Energy Market Operator
-->

# Contributions
<!-- 
- Methodological: cutting-edge techniques from optimisation, econometrics and statistics will be combined to develop new reconciliation methods, new methods will be applied to important problems in energy, retail, and finance thus demonstrating that the research can inform decisions that lower real-world economic costs.

- Economic, environmental and social benefits: the research will improve the accuracy of probabilistic forecasts for decision making in energy, retail, and finance. This will lead to more efficient allocation of resources, reduced economic costs, and improved risk management.
-->


\pagebreak
# References {-}

---
title: "\\LARGE Decision-Optimal Probabilistic Forecast Reconciliation for Expected Shortfall in Hierarchical Time Series"
author: 
  - Vincent Su
format: 
  pdf:
    include-in-header:
      text: |
        <!-- \usepackage{amsmath} -->
        <!-- \usepackage{mathspec} -->
        \usepackage{algorithm}
        \usepackage{algpseudocode}
        \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
        \usepackage{array}
    mathspec: true
    number-sections: true
link-citations: true
execute:
  cache: false
  echo: false
  warning: false
bibliography: D:/Github/Recon_Honours_Thesis/references.bib
# bibliographystyle: apa
csl: D:/Github/Recon_Honours_Thesis/apa.csl
editor_options: 
  chunk_output_type: console
---

<!-- # Abstract {-} -->

# Background

<!--
- Data is hierarchical
- Decision making requires forecasts at different levels of hierarchy
- Base forecasts are incoherent and accuracy suffers
- Reconciliation methods are used to adjust forecasts to be coherent
- Point forecasts methods are developed
- Probabilistic forecasts are sometimes the quantity of interest, especially in risk management
-->

Many operational datasets are naturally hierarchical. For Starbucks, their total sales is the sum of all countries they are operating in, and each national sales is the sum for all cities, down to many outlets, forming an aggregation constraint. Forecasting such hierarchical time series arises in many decision-making contexts ranging from supply chains [@Seaman2022-bb; @Angam2025-od] and energy planning [@Di-Modica2021-ad], to macroeconomic [@El-Gemayel2022-hx; @Li2019-vv] and tourism analysis [@Athanasopoulos2009-lp], where stakeholders need forecasts at several aggregation levels to allocate resources and manage risk.

In practice, when base forecasts are produced independently for all series, they typically fail to satisfy the aggregation constraints. Such forecasts are *incoherent*, which can undermine downstream decisions that require internal consistency. Forecast reconciliation addresses this by adjusting the base forecasts so the final set is *coherent* with the hierarchy. Approaches such as OLS [@Hyndman2011-jv], WLS [@Hyndman2016-ic], and MinT [@Wickramasuriya2019-xq] implement this as linear mappings (often projections) so that the adjusted forecasts lie in the coherent subspace; see @Athanasopoulos2024-as for a comprehensive review.

# Motivation

<!--
- Recent works mainly focus on point forecasts
- Existing point forecast reconciliation can be extended to probabilistic forecasts
- Point forecast reconciliation methods are optimised for scoring rules such as MSE
- Question: How can probabilistic reconciliation be optimised with respect to different loss functions targeted at the
downstream decision? 
- Value-at-risk (VaR) is common risk measure, however, it ignores the tail risk and not coherent risk measure
- Expected Shortfall (ES) is coherent risk measure and accounts for tail risk, and has recently superseded the Value at Risk as the preferred risk measure in banking regulation.
- This work investigates the optimal probabilistic forecast reconciliation for Expected Shortfall.
-->

In the current literature, most reconciliation methodology has been developed for point forecasts, and its benefits for accuracy and coordination across organisation's departments or "silos" are well documented. However, in many applications the quantity of interest is probabilistic forecasts, not just a specific value. Decision makers care about the full distribution of future outcomes to quantify uncertainty, especially in risk management, where tail risks matter (e.g., extreme demand in newsvendor settings or energy generation shortfalls). This motivates moving from point reconciliation to probabilistic reconciliation, so that coherent forecasts also carry coherent uncertainty.

Existing reconciliation methods for point forecasts has recently been generalised to reconcile probabilistic forecasts. However, these approaches originally focus on point forecasts and are optimised for scoring rules such as mean squared error (MSE). There is a gap in the literature on how *optimal* this approach is with respect to the downstream decision. The current expansion of point reconciliation methods no longer guarantee optimality for probabilistic forecasts, such as quantiles or expected shortfall (ES). This leaves the question: *"How can probabilistic reconciliation be optimised with respect to different loss functions targeted at the downstream decision?"*

Although @Panagiotelis2023-fs introduced an score optimisation for reconciliation weights using the energy score or variogram score, they are scoring rules for full multivariate predictive distributions and do not directly address the tail functionals. Among the risk measures, quantiles, also known as Value-at-Risk (VaR), is a common choice, but it has limitations. VaR ignores the severity of losses beyond that threshold and is not a coherent risk measure, since it does not satisfy the subadditivity property [@Yamai2005-me]. This can lead to inconsistencies in risk assessments across different levels of the hierarchy.

Meanwhile, Expected Shortfall (ES) is a coherent risk measure that accounts for the severity of losses in the tail of the distribution [@Yamai2005-me]. It has recently been adopted as the preferred risk measure in banking regulation since the Basel III regulatory framework [@Basel-iii]. It has ability to capture tail risks more effectively than VaR. ES is also applicable beyond finance, such as quantifying expected economic loss when electricity demand exceeds available capacity. Therefore, we aim to investigate and develop the optimal probabilistic forecast reconciliation under the Expected Shortfall criterion.

<!-- footnote for subadditivity property? -->

# Methodology

### Preliminaries {-}
<!-- 
- Hierarchical time series (HTS) is a collection of time series that are organised in a hierarchy.
- Coherent subspace and reconciliation definition
- Reconciliation to probabilistic forecasts\
-->

Let $\boldsymbol{y}_t \in \mathbb{R}^K$ denote a vector of $K$ time series variables from the hierarchy, at time $t$. For simplicity, we suppress the subscript $t$ and denote it as $\boldsymbol{y}$. These variables are subject to linear aggregation constraints represented by a matrix $\boldsymbol C$, such that $\boldsymbol C \boldsymbol{y} = 0$. For example, in a retail setting, the total sales at a warehouse ($y_1$) and two stores ($y_2$ and $y_3$) might satisfy the constraint $y_1 = y_2 + y_3$, leading to $\boldsymbol C = (1, -1, -1)$.

<!-- 
\tilde{\boldsymbol{y}}
\hat{\boldsymbol{y}}
-->

Base forecasts $\hat{\boldsymbol{y}}$, which we generate independently for each series, are typically *incoherent* (i.e. $\boldsymbol C \hat{\boldsymbol{y}} \neq 0$), while reconciled forecasts $\tilde{\boldsymbol{y}}$ are coherent by construction (i.e. $\boldsymbol C \hat{\boldsymbol{y}} = 0$). We obtain coherence via a linear mapping $\tilde{\boldsymbol{y}} = \boldsymbol P \hat{\boldsymbol{y}}$, where $\boldsymbol P$ maps the full *incoherent* space $\mathbb{R}^K$ to the linear *coherent* subspace $\mathbb{S} \subset \mathbb{R}^K$, where aggregation constraints hold. This mapping $\boldsymbol P$ can be a projection operator $\boldsymbol S (\boldsymbol S' \boldsymbol S)^{-1} \boldsymbol S'$ [@Hyndman2011-jv], where $\boldsymbol S$ is a matrix designed such that $\boldsymbol S \boldsymbol C = 0$. The mapping $\boldsymbol P$ is not unique, and selecting an optimal $\boldsymbol P$ is the core design problem. In this project, we will focus solely on linear reconciliation.

<!-- 
For probabilistic forecasts, let $\hat F$ be a multivariate base probabilistic forecast on $\mathbb{R}^K$. Define the reconciled probabilistic forecast $\tilde F$ on $\mathbb{S}$ so that probabilities are preserved under the reconciliation map (intuitively, events in the coherent subspace under $\tilde F$ correspond to pre-images under $\hat F$). This extends point reconciliation to the distributional setting while respecting the aggregation constraints.
-->

\color{red}
Talk about the probabilistic reconciliation and how it is extended from point reconciliation.
\color{black}




### Optimisation Framework {-}
<!-- 
- ES definition
- ES is not elicitable, but is jointly elicitable with the quantile function.
- the quantile and ES jointly minimise in expectation the Fissler-Ziegel class of loss functions
- utilise the Rockafellar-Uryasev joint characterisation of the quantile and ES
- thus jointly reconcile the quantile and ES using the following bilevel optimisation problem
- ...
-->

For a random variable $Y$, the Expected Shortfall (ES) at level $\alpha \in (0, 1)$ is defined as $\mathrm{ES}_\alpha(Y) = E[Y \mid Y < \mathrm{VaR}_\alpha(Y)]$, where $\mathrm{VaR}_\alpha(Y)$ is the quantile $q$ at level $\alpha$. The ES alone is not elicitable, meaning there is no scoring rule or loss function that can uniquely identify the ES as the optimal optimiser. However, @Fissler2015-uu showed that the quantile function and ES are jointly elicitable, minimising the Fissler-Ziegel class of loss functions in expectation. One of which takes the form:

$$
L_\alpha^\mathrm{FZ}(y, q, e) = (\mathbb{I}(y \leq q) - \alpha)(q-y) 
+ \frac{1}{\alpha} \mathrm{exp}(e) \mathbb{I}(y \leq q)(q-y) + \mathrm{exp}(e)(e-q-1),
$$

where $y$ is the realisation of $Y$, $q$ is the quantile, and $e$ is the ES. This loss function is non-convex, thus greatly complicating the optimisation problem. Alternatively, we propose to use the Rockafellar-Uryasev joint characterisation of the quantile and ES [@Rockafellar2000-el]:
$\widetilde{\mathrm{ES}}_k(\boldsymbol{P})=\max_{q}\, E_{\tilde{\boldsymbol{y}}}\!\left[ H_{\alpha}\!\big(\tilde y_k(\boldsymbol{P}), q\big) \right]$ 
and 
$\tilde q_k(\boldsymbol{P})=\arg\max_{q}\, E_{\tilde{\boldsymbol{y}}}\!\left[ H_{\alpha}\!\big(\tilde y_k(\boldsymbol{P}), q\big) \right]$,
where
$H_{\alpha}(y, q) = q - \frac{1}{\alpha}\max\{0, y - q\}$.
Thus, we jointly reconcile the quantile and ES using the following bilevel optimisation problem:

\vspace{-0.8cm}

\begin{align}
\min_{\boldsymbol{P}}\;&
\sum_{k=1}^{K} E_{\boldsymbol y}\!\left[
  L^{\mathrm{FZ}}_{\alpha}\!\big(y_k,\, \tilde q_k(\boldsymbol{P}),\, \widetilde{\mathrm{ES}}_k(\boldsymbol{P})\big)
\right]
\\
\text{s.t. }&
\tilde q_k(\boldsymbol{P})=\arg\max_{q}\, E_{\tilde{\boldsymbol{y}}}\!\left[ H_{\alpha}\!\big(\tilde y_k(\boldsymbol{P}), q\big) \right],
\notag\\
&
\widetilde{\mathrm{ES}}_k(\boldsymbol{P})=\max_{q}\, E_{\tilde{\boldsymbol{y}}}\!\left[ H_{\alpha}\!\big(\tilde y_k(\boldsymbol{P}), q\big) \right],
\qquad \forall\,k=1,\ldots,K.
\notag
\end{align}

\vspace{-0.4cm}

This ensures that the lower-level problem is still convex, however the upper-level objective function with the Fissler-Ziegler loss is non-convex. Irrespective of this non-convexity, smoothing techniques combined with the implicit function and envelope theorems can be used to devise gradient-based methods for locally minimising.

# Applications 
<!-- 
- Finance: ES is prefered risk measure, hierarchical structure from composite indices and etfs, also accounting for varying weights. Data is publicly available.
- Energy: ES can be a measure of economic loss accrued if insufficient generation capacity, hierarchical structure from geographical aggregation of service zones, states, and national level. Electricity load data are made available by the Australian Energy Market Operator
-->

# Contributions
<!-- 
- Methodological: cutting-edge techniques from optimisation, econometrics and statistics will be combined to develop new reconciliation methods, new methods will be applied to important problems in energy, retail, and finance thus demonstrating that the research can inform decisions that lower real-world economic costs.

- Economic, environmental and social benefits: the research will improve the accuracy of probabilistic forecasts for decision making in energy, retail, and finance. This will lead to more efficient allocation of resources, reduced economic costs, and improved risk management.
-->


\pagebreak
# References {-}
